Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Outward issue link (Required),Inward issue link (Supercedes),Outward issue link (Supercedes),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Parse Exception : character '@' not supported while granting privileges to user in a Secure Cluster through hive client.,HIVE-4413,12644338,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Not A Problem,bolke,navmad,navmad,24/Apr/13 11:43,18/Feb/16 14:18,28/Nov/24 15:58,18/Feb/16 14:18,0.10.0,0.14.0,1.2.0,1.2.1,2.0.0,,,,,CLI,,,0,cli,hive,"While running through hive CLI , hive grant command  throws a parseException '@' not supported. But in a secure cluster ( Kerberos ) the username is appended with the realmname seperated by the character '@'.Without giving the full username the permissions are not granted to the intended user.

""grant all on table tablename to user user@REALM""",,,,,,,,,,,,,,HIVE-3807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,324705,,,,Thu Feb 18 14:18:40 UTC 2016,,,,,,,,,,"0|i1k0qn:",325050,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/14 13:56;qwertymaniac;HIVE-3807 should resolve this (the specific need of @ in secure clusters);;;","18/Feb/16 12:34;bolke;HADOOP-12751 is aiming to remove the limitation on characters within a username, eg. '@' will be allowed in a username. This is important for interoperability across different trusted domains which create usernames with their own realm included. 

The proper fix for this issue should be to allow these characters in a username, possibly by escaping them or quoted.

grant all on table tablename to user bolke\@ad.domain
grant all on table tablename to user ""bolke@ad.domain""

;;;","18/Feb/16 14:18;bolke;Apologies, quoting works fine with `;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive on tez: memory manager for grace hash join,HIVE-10233,12818760,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,hagleitn,vikram.dixit,vikram.dixit,07/Apr/15 00:14,08/Jul/15 00:32,28/Nov/24 15:58,08/Jul/15 00:32,2.0.0,llap,,,,,1.3.0,,,Tez,,,0,TODOC1.3,,We need a memory manager in llap/tez to manage the usage of memory across threads. ,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/15 19:38;vikram.dixit;HIVE-10233-WIP-2.patch;https://issues.apache.org/jira/secure/attachment/12725658/HIVE-10233-WIP-2.patch","17/Apr/15 18:43;vikram.dixit;HIVE-10233-WIP-3.patch;https://issues.apache.org/jira/secure/attachment/12726230/HIVE-10233-WIP-3.patch","21/Apr/15 19:38;vikram.dixit;HIVE-10233-WIP-4.patch;https://issues.apache.org/jira/secure/attachment/12726972/HIVE-10233-WIP-4.patch","23/Apr/15 01:10;vikram.dixit;HIVE-10233-WIP-5.patch;https://issues.apache.org/jira/secure/attachment/12727494/HIVE-10233-WIP-5.patch","13/May/15 23:48;vikram.dixit;HIVE-10233-WIP-6.patch;https://issues.apache.org/jira/secure/attachment/12732711/HIVE-10233-WIP-6.patch","07/Jun/15 02:12;vikram.dixit;HIVE-10233-WIP-7.patch;https://issues.apache.org/jira/secure/attachment/12738219/HIVE-10233-WIP-7.patch","15/Jun/15 18:05;wzheng;HIVE-10233-WIP-8.patch;https://issues.apache.org/jira/secure/attachment/12739657/HIVE-10233-WIP-8.patch","18/Jun/15 17:29;wzheng;HIVE-10233.08.patch;https://issues.apache.org/jira/secure/attachment/12740448/HIVE-10233.08.patch","18/Jun/15 23:22;wzheng;HIVE-10233.09.patch;https://issues.apache.org/jira/secure/attachment/12740513/HIVE-10233.09.patch","20/Jun/15 01:14;vikram.dixit;HIVE-10233.10.patch;https://issues.apache.org/jira/secure/attachment/12740783/HIVE-10233.10.patch","23/Jun/15 03:31;hagleitn;HIVE-10233.11.patch;https://issues.apache.org/jira/secure/attachment/12741210/HIVE-10233.11.patch","23/Jun/15 20:13;hagleitn;HIVE-10233.12.patch;https://issues.apache.org/jira/secure/attachment/12741360/HIVE-10233.12.patch","23/Jun/15 23:50;vikram.dixit;HIVE-10233.13.patch;https://issues.apache.org/jira/secure/attachment/12741403/HIVE-10233.13.patch","24/Jun/15 01:09;hagleitn;HIVE-10233.14.patch;https://issues.apache.org/jira/secure/attachment/12741424/HIVE-10233.14.patch","26/Jun/15 03:05;hagleitn;HIVE-10233.15.patch;https://issues.apache.org/jira/secure/attachment/12742027/HIVE-10233.15.patch","26/Jun/15 03:22;hagleitn;HIVE-10233.16.patch;https://issues.apache.org/jira/secure/attachment/12742033/HIVE-10233.16.patch","26/Jun/15 17:41;hagleitn;HIVE-10233.17.patch;https://issues.apache.org/jira/secure/attachment/12742168/HIVE-10233.17.patch","26/Jun/15 17:58;hagleitn;HIVE-10233.18.patch;https://issues.apache.org/jira/secure/attachment/12742174/HIVE-10233.18.patch","26/Jun/15 19:40;hagleitn;HIVE-10233.19.patch;https://issues.apache.org/jira/secure/attachment/12742207/HIVE-10233.19.patch","26/Jun/15 21:30;hagleitn;HIVE-10233.20.patch;https://issues.apache.org/jira/secure/attachment/12742234/HIVE-10233.20.patch","26/Jun/15 21:44;hagleitn;HIVE-10233.21.patch;https://issues.apache.org/jira/secure/attachment/12742236/HIVE-10233.21.patch","26/Jun/15 23:47;hagleitn;HIVE-10233.22.patch;https://issues.apache.org/jira/secure/attachment/12742264/HIVE-10233.22.patch","27/Jun/15 00:27;hagleitn;HIVE-10233.23.patch;https://issues.apache.org/jira/secure/attachment/12742271/HIVE-10233.23.patch","27/Jun/15 04:51;hagleitn;HIVE-10233.24.patch;https://issues.apache.org/jira/secure/attachment/12742296/HIVE-10233.24.patch",,24.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 08 00:32:29 UTC 2015,,,,,,,,,,"0|i2cvp3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/15 21:22;sseth;Looked at just the Tez Configuration changes.
- Since Hive will be setting the memory explicitly, disabling the Tez scaling makes sense. That's done by setting
tez.task.scale.memory.enabled = false (TezConfiguration.TEZ_TASK_SCALE_MEMORY_ENABLED).
This needs to be set before creating the AM, and applies to all DAGs running in the AM.

- TezRuntimeConfiguration.TEZ_RUNTIME_IO_SORT_MB, TezRuntimeConfiguration.TEZ_RUNTIME_UNORDERED_OUTPUT_BUFFER_SIZE_MB - need to convert the memory from bytes to MB before setting these properties
- edgeProp.getInputMemoryNeededPercent - this needs to be a fraction (0-1) (rather than an actual percentage (0-100)). Not sure what the method gives back right now.
- Missed mentioning this in the offline discussions about the properties involved, one more needs to be set for the Ordered case. (TEZ_RUNTIME_INPUT_POST_MERGE_BUFFER_PERCENT). This is a measure of how much memory will be used after the merge is complete to avoid spilling to disk. This defaults to 0, but is typically a lower value than the MergeMemory.
Given that this memory is always reserved for the Input, it can just be set to the Input merge memory.

There's explicit APIs which can be used to configure these properties.
{code}
.setValueSerializationClass(TezBytesWritableSerialization.class.getName(), null)
.configureOutput().setSortBufferSize([OUT_SIZE]).done()
.configureInput().setShuffleBufferFraction(IN_FRACTION).setPostMergeBufferFraction(IN_FRACTION).done()
{code}

Similarly for the UnorderedCase.



;;;","17/Apr/15 18:42;vikram.dixit;Address Sid's comments.;;;","23/Apr/15 01:10;vikram.dixit;Fix runtime issues.;;;","28/Apr/15 21:45;hagleitn;I'm still reviewing, but there are some changes in this that I think is unnecessary. I think you've renamed the llap memory manager to MemoryManagerInterface to make room for another MemoryManager (ql/exec/MemoryManager). But that one isn't used. You really use the ExecMemoryManager. 

So - you could roll back the changes to the llap cache, remove the old memory manager and just use the exec one. That simplifies the patch.

I also think you don't need a memory manager class at all. All it does is remember a field per operator. It seems cleaner to add memInfo to the operator base class with some facilities to track memory. (or introduce a class between operator and gby/join/rs).;;;","11/May/15 19:43;vikram.dixit;Mostly there. Facing issues with map joins in reduce side runs. Checking against trunk.;;;","15/May/15 22:23;hagleitn;[~vikram.dixit] could you create a rb entry for this?;;;","15/May/15 22:31;hagleitn;In group by: I think the max size you compute via memory decider defines the max hash table size also. the max memory in the operator is reserved for memory of the jvm/container. so when memory manager is on you should store the container memory size in max memory of the operator, but compute the max hashtable size to be ""max memory"" of the group by desc.;;;","15/May/15 22:34;hagleitn;I think the same is true for top n hashes. Also - getMemoryNeeded() is a misnomer. That's the max right? getMaxMemory? or getAllocatedMemory?;;;","15/May/15 22:39;hagleitn;The if/else block in DagUtils is 99% copied code. There has to be a better way.;;;","07/Jun/15 02:12;vikram.dixit;Join only patch.;;;","15/Jun/15 18:05;wzheng;Upload WIP-8 patch for join only MM.;;;","17/Jun/15 17:08;wzheng;Trigger testing patch 8;;;","18/Jun/15 17:29;wzheng;Change the filename to trigger the test;;;","18/Jun/15 22:52;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12740448/HIVE-10233.08.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4312/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4312/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4312/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-4312/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at b98a30b HIVE-10746: Hive 1.2.0+Tez produces 1-byte FileSplits from mapred.TextInputFormat (Gopal V via Gunther H)
+ git clean -f -d
Removing ql/src/test/queries/clientpositive/tez_self_join.q
Removing ql/src/test/results/clientpositive/tez/tez_self_join.q.out
+ git checkout master
Already on 'master'
+ git reset --hard origin/master
HEAD is now at b98a30b HIVE-10746: Hive 1.2.0+Tez produces 1-byte FileSplits from mapred.TextInputFormat (Gopal V via Gunther H)
+ git merge --ff-only origin/master
Already up-to-date.
+ git gc
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12740448 - PreCommit-HIVE-TRUNK-Build;;;","18/Jun/15 23:22;wzheng;Rebase and upload patch 09;;;","19/Jun/15 01:56;hagleitn;Partial review: 
  * There's some unnecessary commented out code in HashTableLoader (NOCOND..)
   * getOutputMemoryNeeded isn't referenced anywhere. I think this can be dropped together with setOutputMemoryNeeded + references.
   * IMO a definition of ONE_MB makes no sense might as well use the number in the code
   * Same for getInputMemoryNeededFraction
   * memoryInUse in AbstractOperatorDesc isn't used anywhere
   *    || (conf.getBoolVar(HiveConf.ConfVars.HIVEUSEHYBRIDGRACEHASHJOIN))) { did you mean to say &&? Are you trying to run the mem manager only if tez and hybrid?
   * You set a work's memory usage to the data size of it's terminal operator. How come?;;;","19/Jun/15 01:58;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12740513/HIVE-10233.09.patch

{color:red}ERROR:{color} -1 due to 93 failed/errored test(s), 9010 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_parquet_types
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join21
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join29
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join30
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join_filters
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_10
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_gby
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_gby_empty
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_limit
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_semijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_simple_select
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_stats
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_subq_exists
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_subq_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_subq_not_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_views
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cross_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cross_product_check_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_hybridgrace_hashjoin_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mergejoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadataonly1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf_streaming
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_env_var1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_env_var2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_selectDistinctStar
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_hash
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_tests
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_joins_explain
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_multi_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_smb_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_smb_main
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_decimal
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_dynamic_partition
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_group_by
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_multiinsert
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union6
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_unionDistinct_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_unionDistinct_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_6
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_join30
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_join_filters
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_join_nulls
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_leftsemi_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_mr_diff_schema_alias
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_multi_insert
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_null_projection
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_nullsafe_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_bucketmapjoin1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_nested_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_shufflejoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join28
org.apache.hadoop.hive.ql.parse.TestGenTezWork.testCreateReduce
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4314/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4314/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4314/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 93 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12740513 - PreCommit-HIVE-TRUNK-Build;;;","23/Jun/15 03:33;hagleitn;.11 is a simplified version. I threw out all the mem computation wrt edges and basically just kept the adjustment for graceful joins. [~vikram.dixit]/[~wzheng] could you take a look?;;;","23/Jun/15 03:45;mmokhtar;[~hagleitn] [~vikram.dixit] [~wzheng]

It would make sense to annotate the explain plan with memory assigned to each Hash table, as in 
{code}
      DagName: jenkins_20150622122318_f770d9ab-0ddd-43cf-b950-32f38e2f17e1:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: store_sales
                  filterExpr: (ss_item_sk is not null and ss_sold_date_sk BETWEEN 2450816 AND 2451500) (type: boolean)
                  Statistics: Num rows: 28878719387 Data size: 2405805439460 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ss_item_sk is not null (type: boolean)
                    Statistics: Num rows: 28878719387 Data size: 231029755096 Basic stats: COMPLETE Column stats: COMPLETE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 ss_item_sk (type: int)
                        1 i_item_sk (type: int)
                      outputColumnNames: _col1, _col22, _col26
                      input vertices:
                        1 Map 3
                      Statistics: Num rows: 28878719387 Data size: 346544632644 Basic stats: COMPLETE Column stats: COMPLETE
                      HybridGraceHashJoin: true Hash table memory : 1848000 Bytes
                      Filter Operator
                        predicate: ((_col26 = _col1) and _col22 BETWEEN 2450816 AND 2451500) (type: boolean)
                        Statistics: Num rows: 7219679846 Data size: 86636158152 Basic stats: COMPLETE Column stats: COMPLETE
                        Select Operator
                          Statistics: Num rows: 7219679846 Data size: 86636158152 Basic stats: COMPLETE Column stats: COMPLETE
                          Group By Operator
                            aggregations: count()
                            mode: hash
                            outputColumnNames: _col0
                            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              sort order: 
                              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                              value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: i_item_sk is not null (type: boolean)
                  Statistics: Num rows: 462000 Data size: 663560457 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: i_item_sk is not null (type: boolean)
                    Statistics: Num rows: 462000 Data size: 1848000 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: i_item_sk (type: int)
                      sort order: +
                      Map-reduce partition columns: i_item_sk (type: int)
                      Statistics: Num rows: 462000 Data size: 1848000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
{code};;;","23/Jun/15 04:18;mmokhtar;[~gunther]
Should totalAvailableMemory be HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD?
resourceAvailable.getMemory() would basically return container size, this will result in a lot of GC if all memory gets used up. 

Change Log.Debug to Log.INFO and print the input size. 
{code}
 for (MapJoinOperator mj : mapJoins) {
	        mj.getConf().setMemoryNeeded(minMemory);
		  LOG.info(""Setting "" + minMemory + "" bytes needed for "" + mj);
	      }
{code}

Also I am not following the logic here, shouldn't the memory needed per operator be something like ""(estimate size) / (Total input sizes) x memoryAvailable"" ?
{code}
	      int numJoins = mapJoins.size();
	      long minMemory = totalAvailableMemory / ((numJoins > 0) ? numJoins : 1);
	      minMemory = Math.min(minMemory, onePercentMemory);
	
	      for (MapJoinOperator mj : mapJoins) {
	        mj.getConf().setMemoryNeeded(minMemory);
		if (LOG.isDebugEnabled()) {
		  LOG.debug(""Setting "" + minMemory + "" bytes needed for "" + mj);
		}
	      }
{code};;;","23/Jun/15 04:20;mmokhtar;[~hagleitn]
Please check comment above. ;;;","23/Jun/15 12:23;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12741210/HIVE-10233.11.patch

{color:red}ERROR:{color} -1 due to 43 failed/errored test(s), 9013 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_10
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_gby_empty
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_hybridgrace_hashjoin_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mergejoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_env_var1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_script_env_var2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_selectDistinctStar
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_hash
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_multi_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_smb_main
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_decimal
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_dynamic_partition
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_group_by
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_multiinsert
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union6
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_unionDistinct_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_unionDistinct_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_6
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_leftsemi_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_multi_insert
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_null_projection
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_nested_mapjoin
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4348/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4348/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4348/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 43 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12741210 - PreCommit-HIVE-TRUNK-Build;;;","23/Jun/15 18:12;wzheng;[~hagleitn] [~vikram.dixit]
1. Same question as [~mmokhtar] mentioned, why do we only allocate less than 1% memory to the mapjoin?
2. What is the use for pctx.getConf(); in the beginning of MemoryDecider.resolve()?
3. For these three method calls, the param work is not used, so can be removed. Also can consider removing the param work in evaluateWork(TezWork work, BaseWork w).
evaluateMapWork(work, (MapWork) w);
evaluateReduceWork(work, (ReduceWork) w);
evaluateMergeWork(work, (MergeJoinWork) w);
4. In evaluateOperators(BaseWork w, PhysicalContext pctx), pctx is not used
5. Indentation for evaluateOperators needs to be adjusted;;;","23/Jun/15 20:13;hagleitn;.12 addresses review comments;;;","23/Jun/15 23:50;vikram.dixit;Fix for unions.;;;","24/Jun/15 01:09;hagleitn;fix indent in .14;;;","24/Jun/15 02:32;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12741403/HIVE-10233.13.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 9016 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_delete
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_delete_own_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_update_own_table
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join4
org.apache.hive.hcatalog.pig.TestHCatStorer.testEmptyStore[3]
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4358/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4358/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4358/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12741403 - PreCommit-HIVE-TRUNK-Build;;;","24/Jun/15 07:21;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12741424/HIVE-10233.14.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 8972 tests executed
*Failed tests:*
{noformat}
TestMiniSparkOnYarnCliDriver - did not produce a TEST-*.xml file
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.testSparkQuery
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4361/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4361/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4361/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12741424 - PreCommit-HIVE-TRUNK-Build;;;","24/Jun/15 17:30;hagleitn;Test failures are unrelated.;;;","24/Jun/15 22:54;vikram.dixit;Change looks good from the planning side. Wei can you take a look from the execution side (grace hash join) please.

Thanks
Vikram.;;;","24/Jun/15 23:03;mmokhtar;[~hagleitn] [~vikram.dixit]
On the latest build I am hitting OOM for several queries 
{code}
hive> explain select count(*) from store_sales, customer c1, customer_address ca1, customer_demographics cd1 , customer c2, customer_address ca2, customer_demographics cd2 where ss_customer_sk = c1.c_customer_sk and ss_addr_sk = ca1.ca_address_sk and ss_cdemo_sk = cd1.cd_demo_sk;
{code}

Exception 
{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
	at org.apache.commons.codec.binary.Base64.resizeBuffer(Base64.java:376)
	at org.apache.commons.codec.binary.Base64.encode(Base64.java:461)
	at org.apache.commons.codec.binary.Base64.encode(Base64.java:937)
	at org.apache.commons.codec.binary.Base64.encodeBase64(Base64.java:818)
	at org.apache.commons.codec.binary.Base64.encodeBase64(Base64.java:785)
	at org.apache.commons.codec.binary.Base64.encodeBase64(Base64.java:767)
	at org.apache.commons.codec.binary.Base64.encodeBase64(Base64.java:642)
	at org.apache.hadoop.hive.ql.exec.Utilities.serializeExpression(Utilities.java:799)
	at org.apache.hadoop.hive.ql.plan.TableScanDesc.setFilterExpr(TableScanDesc.java:153)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateTableScanProc.process(ConstantPropagateProcFactory.java:1208)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate$ConstantPropagateWalker.walk(ConstantPropagate.java:150)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.transform(ConstantPropagate.java:120)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:196)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10169)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9993)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1124)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1172)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1061)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1051)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
{code};;;","24/Jun/15 23:49;vikram.dixit;Those don't look like the memory manager's errors. I think this is a different issue.;;;","25/Jun/15 00:00;mmokhtar;[~vikram.dixit]
Yes, issue unrelated. ;;;","25/Jun/15 01:06;wzheng;[~vikram.dixit] Patch 14 looks good.;;;","25/Jun/15 23:25;vikram.dixit;+1;;;","26/Jun/15 03:07;hagleitn;Addressed some offline comments from [~mmokhtar]. Patch .15 orders the mapjoins by hashtable size. Then it tries to fit the smallest ones into mem w/o the need for spill. After half the memory is used up that way it will scale the rest to fit into place.;;;","26/Jun/15 19:40;hagleitn;.19 has more reported fixes (fallback in case of all joins are small, actually making fallback work...);;;","27/Jun/15 00:10;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12742236/HIVE-10233.21.patch

{color:green}SUCCESS:{color} +1 9027 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4401/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4401/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4401/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12742236 - PreCommit-HIVE-TRUNK-Build;;;","27/Jun/15 00:41;vikram.dixit;+1 LGTM.;;;","27/Jun/15 02:09;prasanth_j;Looks good to me too. ;;;","27/Jun/15 04:55;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12742271/HIVE-10233.23.patch

{color:green}SUCCESS:{color} +1 9030 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4404/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4404/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4404/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12742271 - PreCommit-HIVE-TRUNK-Build;;;","27/Jun/15 15:58;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12742296/HIVE-10233.24.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 9030 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables_compact
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4411/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4411/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4411/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12742296 - PreCommit-HIVE-TRUNK-Build;;;","27/Jun/15 17:37;vikram.dixit;+1 for the latest iteration.;;;","28/Jun/15 18:49;leftyl;Doc note:  This adds two configuration parameters (*hive.tez.enable.memory.manager* & *hive.hash.table.inflation.factor*) which need to be documented in the wiki in Configuration Properties for release 1.3.0.

* *hive.tez.enable.memory.manager* belongs in [Configuration Properties -- Tez | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-Tez]
* *hive.hash.table.inflation.factor* belongs in [Configuration Properties -- Query and DDL Execution | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-QueryandDDLExecution]

Is any general documentation needed for the memory manager?  Perhaps in the design docs?

* [Hive on Tez | https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez]
* [Hybrid Hybrid Grace Hash Join, v1.0 | https://cwiki.apache.org/confluence/display/Hive/Hybrid+Hybrid+Grace+Hash+Join%2C+v1.0]

Also, this jira needs updates for Status, Resolution, and Fix Version.;;;","08/Jul/15 00:32;hagleitn;Committed to master, branch-1;;;"
ORC concatenation of old files can fail while merging column statistics,HIVE-11031,12838341,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,prasanth_j,prasanth_j,prasanth_j,17/Jun/15 00:14,26/Jun/15 19:09,28/Nov/24 15:58,18/Jun/15 14:39,0.13.0,0.14.0,1.0.0,1.1.0,1.2.0,2.0.0,1.2.1,,,,,,0,,,"Column statistics in ORC are optional protobuf fields. Old ORC files might not have statistics for newly added types like decimal, date, timestamp etc. But column statistics merging assumes column statistics exists for these types and invokes merge. For example, merging of TimestampColumnStatistics directly casts the received ColumnStatistics object without doing instanceof check. If the ORC file contains time stamp column statistics then this will work else it will throw ClassCastException.

Also, the file merge operator swallows the exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/15 17:10;prasanth_j;HIVE-11031-branch-1.0.patch;https://issues.apache.org/jira/secure/attachment/12740439/HIVE-11031-branch-1.0.patch","18/Jun/15 02:37;prasanth_j;HIVE-11031.2.patch;https://issues.apache.org/jira/secure/attachment/12740275/HIVE-11031.2.patch","18/Jun/15 07:13;prasanth_j;HIVE-11031.3.patch;https://issues.apache.org/jira/secure/attachment/12740320/HIVE-11031.3.patch","18/Jun/15 07:52;prasanth_j;HIVE-11031.4.patch;https://issues.apache.org/jira/secure/attachment/12740329/HIVE-11031.4.patch","17/Jun/15 05:58;prasanth_j;HIVE-11031.patch;https://issues.apache.org/jira/secure/attachment/12740042/HIVE-11031.patch",,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 26 19:09:44 UTC 2015,,,,,,,,,,"0|i2g4sn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/15 06:09;prasanth_j;[~gopalv] fyi..;;;","17/Jun/15 19:59;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12740042/HIVE-11031.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 9009 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join28
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4288/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4288/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4288/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12740042 - PreCommit-HIVE-TRUNK-Build;;;","18/Jun/15 02:39;prasanth_j;[~gopalv] Added some changes to throw when incompatible statistics gets merged. Also the orc files which does not have stripe statistics will be added to incompatible file set (ignored from merging).;;;","18/Jun/15 05:55;gopalv;General LGTM - +1.

Minor stylistic comments - use isStatsPresent() with (count > 0 || hasNull == true).

I had to read that twice to confirm that's what it was doing.;;;","18/Jun/15 07:13;prasanth_j;Addressed minor nit.;;;","18/Jun/15 11:45;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12740329/HIVE-11031.4.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 9010 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join28
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4301/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4301/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4301/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12740329 - PreCommit-HIVE-TRUNK-Build;;;","18/Jun/15 14:39;prasanth_j;Committed to branch-1.2 and master.;;;","18/Jun/15 17:23;prasanth_j;Note for backport: The branch-1.0 patch will apply cleanly, but if we run orc_merge_incompat1.q it can fail on some platforms. To make it more consistent we need HIVE-8801 patch which makes the orc_merge_incompat1.q test more consistent across platforms.;;;","26/Jun/15 09:24;sztanko;Hello [~prasanth_j], my MR jobs are getting this error when concatenating ORC files:

{code}
java.io.IOException: java.io.IOException: java.lang.IndexOutOfBoundsException: Index: 0
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:226)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:136)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:230)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:210)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.io.IOException: java.lang.IndexOutOfBoundsException: Index: 0
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:105)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:41)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:224)
	... 11 more
Caused by: java.lang.IndexOutOfBoundsException: Index: 0
	at java.util.Collections$EmptyList.get(Collections.java:3212)
	at org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.nextStripe(OrcFileStripeMergeRecordReader.java:82)
	at org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.next(OrcFileStripeMergeRecordReader.java:71)
	at org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.next(OrcFileStripeMergeRecordReader.java:31)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
	... 15 more
2015-06-26 08:24:19,248 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task
{code}

Is this failure a result of the bug described in this ticket or that can be a different problem?;;;","26/Jun/15 19:09;prasanth_j;It could be because of this or HIVE-10685. Can you try with branch-1.2 and see if it works for your query? Alternatively you can provide me a small repro. I can verify and confirm.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unicode table comments do not work,HIVE-11332,12846713,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,,sershe,sershe,21/Jul/15 23:06,16/Sep/15 18:38,28/Nov/24 15:58,16/Sep/15 18:38,0.13.1,1.1.0,2.0.0,,,,,,,,,,0,,,"Noticed by accident.
{noformat}
select '😾 ', count(*) from moo;
Query ID = sershe_20150721190413_979e1b6f-86d6-436f-b8e6-d6785b9d3b83
Total jobs = 1
Launching Job 1 out of 1

[snip]
OK
😾 	0
Time taken: 13.347 seconds, Fetched: 1 row(s)
hive> ALTER TABLE moo SET TBLPROPERTIES ('comment' = '😾 ');
OK
Time taken: 0.292 seconds
hive> desc extended moo;
OK
i                   	int                 	                    
	 	 
Detailed Table Information	Table(tableName:moo, dbName:default, owner:sershe, createTime:1437519787, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:i, type:int, comment:null)], location:hdfs://cn108-10.l42scl.hortonworks.com:8020/apps/hive/warehouse/moo, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{last_modified_time=1437519883, totalSize=0, numRows=-1, rawDataSize=-1, COLUMN_STATS_ACCURATE=false, numFiles=0, transient_lastDdlTime=1437519883, comment=?? , last_modified_by=sershe}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
Time taken: 0.347 seconds, Fetched: 3 row(s)
{noformat}",,,,,,,,,,,,,,HIVE-11837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 24 01:24:26 UTC 2015,,,,,,,,,,"0|i2hiwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/15 01:24;sershe;If we had Unicode 8.0 support, you could even use this, which would improve some code vastly: 🦄;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PPD: Orc Split elimination fails because filterColumns=[-1],HIVE-11035,12838507,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,gopalv,gopalv,17/Jun/15 15:56,19/Jun/15 23:30,28/Nov/24 15:58,18/Jun/15 06:56,0.14.0,1.0.0,1.1.0,1.2.0,1.3.0,2.0.0,1.2.1,,,,,,0,,,"{code}
create temporary table xx (x int) stored as orc ;
insert into xx values (20),(200);
set hive.fetch.task.conversion=none;
select * from xx where x is null;
{code}

This should generate zero tasks after optional split elimination in the app master, instead of generating the 1 task which for sure hits the row-index filters and removes all rows anyway.

Right now, this runs 1 task for the stripe containing (min=20, max=200, has_null=false), which is broken.

Instead, it returns YES_NO_NULL from the following default case

https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java#L976",,,,,,,,,,,,,,,,HIVE-7052,,,,,,,,,,,"18/Jun/15 06:53;prasanth_j;HIVE-11035-branch-1.0.patch;https://issues.apache.org/jira/secure/attachment/12740318/HIVE-11035-branch-1.0.patch","17/Jun/15 22:15;prasanth_j;HIVE-11035.patch;https://issues.apache.org/jira/secure/attachment/12740226/HIVE-11035.patch",,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 18 06:56:02 UTC 2015,,,,,,,,,,"0|i2g5t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/15 22:18;prasanth_j;This is a regression introduced by HIVE-7052. As a result of this regression, the first column id is not pushed down properly but the column name exists. For the example in the description, the value for hive.io.file.readcolumn.ids in conf is empty and the value for hive.io.file.readcolumn.names is "",x"" (comma is prepended). Making the CSV joiner more reliable fixes the issue and pushes the column ids properly which is then used by SARG for populating filterColumns array.;;;","18/Jun/15 05:27;hiveqa;

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12740226/HIVE-11035.patch

{color:green}SUCCESS:{color} +1 9008 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4296/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4296/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4296/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12740226 - PreCommit-HIVE-TRUNK-Build;;;","18/Jun/15 05:42;gopalv;Needs ETL strategy to skip entire splits.

Patch LGTM - +1.;;;","18/Jun/15 06:56;prasanth_j;Committed to all relevant branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StatsOptimizer should return no rows on empty table with group by,HIVE-13452,12956722,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,pxiong,ashutoshc,ashutoshc,07/Apr/16 02:08,14/Dec/17 16:25,28/Nov/24 15:58,13/Dec/16 06:27,1.0.0,1.1.0,1.2.0,2.0.0,2.1.0,,2.3.0,,,Logical Optimizer,,,0,,,"{code}
create table t1 (a int);
analyze table t1 compute statistics;
analyze table t1 compute statistics for columns;
select count(1) from t1 group by 1;
set hive.compute.query.using.stats=true;
select count(1) from t1 group by 1;
{code}

In both cases result set should be empty. However, with statsoptimizer on Hive returns one row with value 0.",,,,,,,,,,,,,,,,,,HIVE-18279,,,,,,,,,"13/Dec/16 00:52;pxiong;HIVE-13452.01.patch;https://issues.apache.org/jira/secure/attachment/12842907/HIVE-13452.01.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 06:27:18 UTC 2016,,,,,,,,,,"0|i2vrdb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/16 02:09;ashutoshc;[~pxiong] Can you take a look?;;;","07/Apr/16 21:03;pxiong;mysql
{code}
Database changed
mysql> create table t1 (a int);
Query OK, 0 rows affected (0.02 sec)

mysql> select count(1) from t1 group by 1;
ERROR 1056 (42000): Can't group on 'count(1)'
mysql> select count(1) from t1;
+----------+
| count(1) |
+----------+
|        0 |
+----------+
1 row in set (0.00 sec)
{code};;;","07/Apr/16 21:07;pxiong;postgres
{code}
dbtmp=# create table t1 (a int);
CREATE TABLE
dbtmp=# select count(1) from t1 group by 1;
ERROR:  aggregates not allowed in GROUP BY clause
LINE 1: select count(1) from t1 group by 1;
               ^
dbtmp=# select count(1) from t1;
 count
-------
     0
(1 row)
{code};;;","07/Apr/16 22:11;ashutoshc;yeah.. difference is MySQL/Postgres treats constant in expressions for group by as positional reference in select list and in which case it doesnt make sense. In Hive, you can get either behavior by {{hive.groupby.orderby.position.alias}} config. However, important point here is even for queries like {{select count(*) from t1 group by c1;}} should return no resultset for empty table. group by 1 essentially mean.. treat all rows as one grouping, so in case for empty table group by 1 should return no rows and just select count(*) from t1 should return row with value 0.;;;","08/Nov/16 22:45;vgarg;Another example

{code} select (1=1) from t2 group by (1=1); {code}

This returns 1 row irrespective of statsoptimizer.;;;","18/Nov/16 19:18;vgarg;With postgres 
{code}
select count(*) from tempty group by (1=1) ; --tempty is an empty table
 count 
-------
(0 rows)
{code}

So as [~ashutoshc] suggested for count(*) with group by 1 on an empty table should not return any row;;;","02/Dec/16 22:44;ashutoshc;+1 pending test;;;","13/Dec/16 05:17;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12842907/HIVE-13452.01.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10782 tests executed
*Failed tests:*
{noformat}
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=144)
	[vectorized_rcfile_columnar.q,vector_elt.q,explainuser_1.q,multi_insert.q,tez_dml.q,vector_bround.q,schema_evol_orc_acid_table.q,vector_when_case_null.q,orc_ppd_schema_evol_1b.q,vector_join30.q,vectorization_11.q,cte_3.q,update_tmp_table.q,vector_decimal_cast.q,groupby_grouping_id2.q,vector_decimal_round.q,tez_smb_empty.q,orc_merge6.q,vector_decimal_trailing.q,cte_5.q,tez_union.q,cbo_rp_subq_not_in.q,vector_decimal_2.q,columnStatsUpdateForStatsOptimizer_1.q,vector_outer_join3.q,schema_evol_text_vec_part_all_complex.q,tez_dynpart_hashjoin_2.q,auto_sortmerge_join_12.q,offset_limit.q,tez_union_multiinsert.q]
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2554/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2554/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2554/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12842907 - PreCommit-HIVE-Build;;;","13/Dec/16 06:27;pxiong;pushed to master. Thanks [~ashutoshc] for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive Metastore is incompatible with MariaDB 10.x,HIVE-17126,13088458,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Not A Problem,,eyang,eyang,19/Jul/17 16:23,09/Aug/17 20:59,28/Nov/24 15:58,09/Aug/17 20:59,1.1.0,1.2.0,2.0.0,,,,,,,Metastore,,,0,,,"MariaDB 10.x is commonly used for cheap RDBMS high availability.  Hive usage of Datanucleus is currently preventing Hive Metastore to use MariaDB 10.x as highly available metastore. Datanucleus generate SQL statements that are not parsable by MariaDB 10.x when dropping Hive table or database schema.  Without MariaDB HA setup, the SQL statement problem also exists for metastore interaction with MariaDB 10.x.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 09 20:59:02 UTC 2017,,,,,,,,,,"0|i3hqy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/17 20:05;sershe;Sounds like a DataNucleus issue to me (assuming it supports Maria DB), or at least the lack of functionality (assuming it doesn't). You might want to find out which one :)

Also, there's an ORM-free, SQL-based implementation of the entire metastore in the works in HIVE-14870. That is Oracle-specific, but some (or at least I ;) ) believe it should be database agnostic. It might also be of use, if there isn't an easy solution with DataNucleus.
 cc [~cdrome];;;","05/Aug/17 20:45;eyang;This seems to affect RHEL/CentOS 6.8 with MariaDB 10.x release.  It has been reported that CentOS 7.x works fine.;;;","05/Aug/17 20:46;eyang;Stack trace for the failure:

{code}
2017-08-05 13:42:25,984 ERROR bonecp.BoneCP (BoneCP.java:destroyConnection(221)) - Error in attempting to close connection
java.sql.SQLException: Unknown system variable 'OPTION'
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2619)
        at com.mysql.jdbc.ConnectionImpl.unsetMaxRows(ConnectionImpl.java:5421)
        at com.mysql.jdbc.StatementImpl.realClose(StatementImpl.java:2441)
        at com.mysql.jdbc.PreparedStatement.realClose(PreparedStatement.java:3079)
        at com.mysql.jdbc.ConnectionImpl.closeAllOpenStatements(ConnectionImpl.java:1585)
        at com.mysql.jdbc.ConnectionImpl.realClose(ConnectionImpl.java:4361)
        at com.mysql.jdbc.ConnectionImpl.close(ConnectionImpl.java:1557)
        at com.jolbox.bonecp.ConnectionHandle.internalClose(ConnectionHandle.java:549)
        at com.jolbox.bonecp.BoneCP.destroyConnection(BoneCP.java:219)
        at com.jolbox.bonecp.ConnectionHandle.markPossiblyBroken(ConnectionHandle.java:390)
        at com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:183)
        at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)
        at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)
        at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:280)
        at org.datanucleus.store.query.Query.executeQuery(Query.java:1786)
        at org.datanucleus.store.query.AbstractSQLQuery.executeWithArray(AbstractSQLQuery.java:339)
        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:312)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1660)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:483)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:403)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1735)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1731)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2391)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1742)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1725)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
        at com.sun.proxy.$Proxy2.getPartitions(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.dropPartitionsAndGetLocations(HiveMetaStore.java:1693)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1532)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1737)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy4.drop_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9256)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9240)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2017-08-05 13:42:25,992 WARN  metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:executeWithArray(1670)) - Failed to execute [select ""PARTITIONS"".""PART_ID"" from ""PARTITIONS""  inner join ""TBLS"" on ""PARTITIONS"".""TBL_ID"" = ""TBLS"".""TBL_ID""     and ""TBLS"".""TBL_NAME"" = ?   inner join ""DBS"" on ""TBLS"".""DB_ID"" = ""DBS"".""DB_ID""      and ""DBS"".""NAME"" = ?  order by ""PART_NAME"" asc] with parameters [employee, default]
javax.jdo.JDODataStoreException: Error executing SQL query ""select ""PARTITIONS"".""PART_ID"" from ""PARTITIONS""  inner join ""TBLS"" on ""PARTITIONS"".""TBL_ID"" = ""TBLS"".""TBL_ID""     and ""TBLS"".""TBL_NAME"" = ?   inner join ""DBS"" on ""TBLS"".""DB_ID"" = ""DBS"".""DB_ID""      and ""DBS"".""NAME"" = ?  order by ""PART_NAME"" asc"".
        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1660)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:483)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:403)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1735)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1731)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2391)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1742)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1725)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
        at com.sun.proxy.$Proxy2.getPartitions(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.dropPartitionsAndGetLocations(HiveMetaStore.java:1693)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1532)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1737)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy4.drop_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9256)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9240)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
NestedThrowablesStackTrace:
java.sql.SQLException: Unknown system variable 'OPTION'
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2619)
        at com.mysql.jdbc.StatementImpl.executeSimpleNonQuery(StatementImpl.java:1606)
        at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:2268)
        at com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:174)
        at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)
        at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)
        at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:280)
        at org.datanucleus.store.query.Query.executeQuery(Query.java:1786)
        at org.datanucleus.store.query.AbstractSQLQuery.executeWithArray(AbstractSQLQuery.java:339)
        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:312)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1660)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:483)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:403)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1735)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1731)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2391)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1742)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1725)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
        at com.sun.proxy.$Proxy2.getPartitions(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.dropPartitionsAndGetLocations(HiveMetaStore.java:1693)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1532)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1737)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy4.drop_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9256)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9240)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2017-08-05 13:42:25,999 WARN  metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2426)) - Direct SQL failed, falling back to ORM
MetaException(message:See previous errors; Error executing SQL query ""select ""PARTITIONS"".""PART_ID"" from ""PARTITIONS""  inner join ""TBLS"" on ""PARTITIONS"".""TBL_ID"" = ""TBLS"".""TBL_ID""     and ""TBLS"".""TBL_NAME"" = ?   inner join ""DBS"" on ""TBLS"".""DB_ID"" = ""DBS"".""DB_ID""      and ""DBS"".""NAME"" = ?  order by ""PART_NAME"" asc"".)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1672)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:483)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:403)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1735)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1731)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2391)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1742)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1725)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
        at com.sun.proxy.$Proxy2.getPartitions(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.dropPartitionsAndGetLocations(HiveMetaStore.java:1693)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1532)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1737)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy4.drop_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9256)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9240)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2017-08-05 13:42:26,001 ERROR DataNucleus.Transaction (Log4JLogger.java:error(115)) - Operation rollback failed on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@401afd16, error code UNKNOWN and transaction: [DataNucleus Transaction, ID=Xid=�, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@401afd16]]
2017-08-05 13:42:26,001 ERROR DataNucleus.Transaction (Log4JLogger.java:error(115)) - Operation rollback failed on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@401afd16, error code UNKNOWN and transaction: [DataNucleus Transaction, ID=Xid=�, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@401afd16]]
2017-08-05 13:42:26,008 ERROR metastore.ObjectStore (ObjectStore.java:run(2405)) -
javax.jdo.JDOException: Exception thrown when executing query
        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)
        at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275)
        at org.apache.hadoop.hive.metastore.ObjectStore.listMPartitions(ObjectStore.java:2018)
        at org.apache.hadoop.hive.metastore.ObjectStore.access$100(ObjectStore.java:160)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getJdoResult(ObjectStore.java:1740)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getJdoResult(ObjectStore.java:1731)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2397)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1742)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1725)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
        at com.sun.proxy.$Proxy2.getPartitions(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.dropPartitionsAndGetLocations(HiveMetaStore.java:1693)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1532)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1737)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy4.drop_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9256)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9240)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
NestedThrowablesStackTrace:
java.sql.SQLException: Connection is closed!
        at com.jolbox.bonecp.ConnectionHandle.checkClosed(ConnectionHandle.java:459)
        at com.jolbox.bonecp.ConnectionHandle.prepareStatement(ConnectionHandle.java:1180)
        at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:350)
        at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getPreparedStatementForQuery(RDBMSQueryUtils.java:194)
        at org.datanucleus.store.rdbms.query.JDOQLQuery.performExecute(JDOQLQuery.java:640)
        at org.datanucleus.store.query.Query.executeQuery(Query.java:1786)
        at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
        at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:266)
        at org.apache.hadoop.hive.metastore.ObjectStore.listMPartitions(ObjectStore.java:2018)
        at org.apache.hadoop.hive.metastore.ObjectStore.access$100(ObjectStore.java:160)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getJdoResult(ObjectStore.java:1740)
        at org.apache.hadoop.hive.metastore.ObjectStore$2.getJdoResult(ObjectStore.java:1731)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2397)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1742)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1725)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
        at com.sun.proxy.$Proxy2.getPartitions(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.dropPartitionsAndGetLocations(HiveMetaStore.java:1693)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1532)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1737)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy4.drop_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9256)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9240)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2017-08-05 13:42:26,009 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Exception thrown when executing query)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2406)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1742)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1725)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
        at com.sun.proxy.$Proxy2.getPartitions(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.dropPartitionsAndGetLocations(HiveMetaStore.java:1693)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1532)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1737)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy4.drop_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9256)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9240)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code};;;","05/Aug/17 20:47;eyang;Output from hive shell:

{code}
hive> drop table employee;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Exception thrown when executing query)
hive>
{code};;;","05/Aug/17 20:57;eyang;From the log output, it appears that SQL statement generated by DataNucleus is double quoting table names and variable names.  The syntax is invalid for MariaDB.  It appears that DataNucleus failed to detect dialect properly for this specific combination of OS and MariaDB.  I agree that HIVE-14870 can solve this problem in the long run.  If there is something that can be done in detection of SQL dialect, maybe we can have a solution sooner for this issue.;;;","07/Aug/17 18:21;sershe;Can you get the problematic statement from MariaDB logs? It would most likely need a DataNucleus bug filed.;;;","07/Aug/17 20:19;sershe;Actually I guess I see the query from direct SQL and the issue is not related to the text of the query.
""Unknown system variable 'OPTION'""
Note that direct SQL calls {noformat}SET @@session.sql_mode=ANSI_QUOTES{noformat} on mysql to use double quotes correctly on MySQL and avoid the weird mysql qoutes.
You can disable direct SQL and see if it fixes the issue since DN itself uses the backticks afaik.
However, from the error message it doesn't look related to quotes.;;;","08/Aug/17 20:45;eyang;[~sershe] SET OPTION is removed from MySQL 5.6 and newer.  There is similar error reported to [MariaDB community|https://mariadb.atlassian.net/browse/MDEV-6201].  Perhaps, there is something in the driver layer that prevented the session to be set for ANSI_QUOTES.  The system was using {{mysql-connector-java-5.1.17-6.el6.noarch}}, which came with RHEL6.x family.  This might be problematic because the driver and server are not fully compatible with each other.  I will do more testing this weekend with a new version of MariaDB connector/J to see if we can side step this issue.  I think it is important to do a error check for SET @@session.sql_mode with more user friendly message.  The current code seems to execute SQL queries even if the SET query failed to execute.;;;","09/Aug/17 20:59;eyang;I am incline to close this issue as user error for mismatching mysql-connector-java driver with mariadb versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MoveTask file format check looks only the table input format,HIVE-12264,12907836,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Implemented,prasanth_j,prasanth_j,prasanth_j,26/Oct/15 06:20,12/Nov/15 22:10,28/Nov/24 15:58,12/Nov/15 22:10,1.2.0,1.3.0,2.0.0,,,,,,,,,,0,,,hive.fileformat.check config tracks back to check in MoveTask which only checks the input format provided by table object. If the move happens to a partition directory this check will fail as hive supports partitions with different input format than that of table.,,,,,,,,,,,,,,,,HIVE-11120,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 12 22:10:46 UTC 2015,,,,,,,,,,"0|i2nicf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/15 22:10;prasanth_j;This is covered in HIVE-11120 patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OrcSplit fails to account for columnar projections in its size estimates,HIVE-7428,12727720,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,gopalv,gopalv,gopalv,16/Jul/14 17:44,03/Nov/15 04:10,28/Nov/24 15:58,03/Nov/15 04:10,1.2.0,1.3.0,2.0.0,,,,,,,,,,0,,,"Currently, ORC generates splits based on stripe offset + stripe length.

This means that the splits for all columnar projections are exactly the same size, despite reading the footer which gives the estimated sizes for each column.

This is a hold-out from FileSplit which uses getLen() as the I/O cost of reading a file in a map-task.

RCFile didn't have a footer with column statistics information, but for ORC this would be extremely useful to reduce task overheads when processing extremely wide tables with highly selective column projections.",,,,,,,,,,HIVE-10497,,,HIVE-10397,,,,,TEZ-1993,HIVE-11546,,,,,HIVE-10397,,,"30/Apr/15 18:56;prasanth_j;HIVE-7428.1.patch;https://issues.apache.org/jira/secure/attachment/12729574/HIVE-7428.1.patch","30/Apr/15 19:42;gopalv;HIVE-7428.2.patch;https://issues.apache.org/jira/secure/attachment/12729587/HIVE-7428.2.patch",,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405825,,,,Thu Aug 13 15:32:55 UTC 2015,,,,,,,,,,"0|i1xu5r:",405845,Estimate columnar projection when generating ORC splits,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 18:56;prasanth_j;Same trunk patch from HIVE-10397. I might need to rebase it to master. ;;;","13/Aug/15 15:32;hiveqa;

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12729587/HIVE-7428.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4950/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4950/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4950/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-4950/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 5b67f35 HIVE-10435 : Make HiveSession implementation pluggable through configuration (Akshay Goyal, reviewed by Amareshwari)
+ git clean -f -d
+ git checkout master
Already on 'master'
+ git reset --hard origin/master
HEAD is now at 5b67f35 HIVE-10435 : Make HiveSession implementation pluggable through configuration (Akshay Goyal, reviewed by Amareshwari)
+ git merge --ff-only origin/master
Already up-to-date.
+ git gc
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12729587 - PreCommit-HIVE-TRUNK-Build;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make alter_merge* tests (ORC only) stable across different OSes,HIVE-11058,12839220,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Won't Fix,prasanth_j,prasanth_j,prasanth_j,19/Jun/15 19:50,23/Jun/15 21:46,28/Nov/24 15:58,23/Jun/15 21:46,1.2.0,2.0.0,,,,,,,,,,,0,,,alter_merge* (ORC only) tests are showing stats diff in different OSes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 23 21:46:47 UTC 2015,,,,,,,,,,"0|i2ga4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/15 21:46;prasanth_j;The stats difference can occur when tests are run in different timezones. ORC stores the timezone id in stripe metadata causing difference in file sizes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable directSQL if datanucleus.identifierFactory = datanucleus2,HIVE-11023,12838213,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Fixed,sushanth,sushanth,sushanth,16/Jun/15 16:26,19/Jun/15 03:24,28/Nov/24 15:58,17/Jun/15 23:44,1.2.1,1.3.0,2.0.0,,,,1.2.1,,,Metastore,,,0,,,"We hit an interesting bug in a case where datanucleus.identifierFactory = datanucleus2 .

The problem is that directSql handgenerates SQL strings assuming ""datanucleus1"" naming scheme. If a user has their metastore JDO managed by datanucleus.identifierFactory = datanucleus2 , the SQL strings we generate are incorrect.

One simple example of what this results in is the following: whenever DN persists a field which is held as a List<T>, it winds up storing each T as a separate line in the appropriate mapping table, and has a column called INTEGER_IDX, which holds the position in the list. Then, upon reading, it automatically reads all relevant lines with an ORDER BY INTEGER_IDX, which results in the list retaining its order. In DN2 naming scheme, the column is called IDX, instead of INTEGER_IDX. If the user has run appropriate metatool upgrade scripts, it is highly likely that they have both columns, INTEGER_IDX and IDX.

Whenever they use JDO, such as with all writes, it will then use the IDX field, and when they do any sort of optimized reads, such as through directSQL, it will ORDER BY INTEGER_IDX.

An immediate danger is seen when we consider that the schema of a table is stored as a List<FieldSchema> , and while IDX has 0,1,2,3,... , INTEGER_IDX will contain 0,0,0,0,... and thus, any attempt to describe the table or fetch schema for the table can come up mixed up in the table's native hashing order, rather than sorted by the index.

This can then result in schema ordering being different from the actual table. For eg:, if a user has a (a:int,b:string,c:string), a describe on this may return (c:string, a:int, b: string), and thus, queries which are inserting after selecting from another table can have ClassCastExceptions when trying to insert data in the wong order - this is how we discovered this bug. This problem, however, can be far worse, if there are no type problems - it is possible, for eg., that if a,b&c were all strings, that that insert query would succeed but mix up the order, which then results in user table data being mixed up. This has the potential to be very bad.

We should write a tool to help convert metastores that use ""datanucleus2"" to ""datanucleus1""(more difficult, needs more one-time testing) or change directSql to support both(easier to code, but increases test-coverage matrix significantly and we should really then be testing against both schemes). But in the short term, we should disable directSql if we see that the identifierfactory is ""datanucleus2""",,,,,,,,,,,,HIVE-11039,,,,,,,,,,,,,,,"16/Jun/15 16:32;sushanth;HIVE-11023.patch;https://issues.apache.org/jira/secure/attachment/12739893/HIVE-11023.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 19 03:24:15 UTC 2015,,,,,,,,,,"0|i2g40n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/15 19:44;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12739893/HIVE-11023.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 9008 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4274/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4274/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4274/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12739893 - PreCommit-HIVE-TRUNK-Build;;;","17/Jun/15 02:06;sushanth;[~sershe], could you please review? Thanks!;;;","17/Jun/15 23:05;ashutoshc;+1 Longer term I liked your first suggestion of converting data in RDBMS from DN2 to DN1.;;;","17/Jun/15 23:19;sushanth;Thanks, Ashutosh. Yup, I intend to file a new jira to enable that.;;;","17/Jun/15 23:38;sushanth;Created HIVE-11039 to help convert DN2 to DN1 naming scheme.;;;","17/Jun/15 23:44;sushanth;Committed to branch-1.2, branch-1 and master.

Thanks, Ashutosh!;;;","18/Jun/15 17:44;xuefuz;qq, [~sushanth], does this happen to previous releases as well?;;;","18/Jun/15 21:28;sushanth;[~xuefuz] : Yes, this will happen to all releases with directSql - which means anything past 0.12, I think. That said, the number of installations that override this parameter should hopefully be minimal.

If people are using datanucleus2 as their identifierFactory version, then:
a) They should disable directSql (can be done by conf parameter, does not need this code fix - the code fix simply automates that for current and future releases)
b) They should retain that identifierFactory - a mixed metastore with both is bad.
c) Once we have a way of migrating them, as with HIVE-11039 filed, we should migrate them out of it.

This parameter should never have been a part of hive-site.xml, I think, since it's dangerous if a user changes it. A ""datanucleus1"" installation changing the parameter to ""datanucleus2"" or vice-versa can result in metadata corruption for us.
;;;","18/Jun/15 21:57;xuefuz;Makes sense. Thanks for the explanation.;;;","19/Jun/15 03:24;leftyl;Should this be documented in the wiki?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pass the purge option for drop table to storage handlers,HIVE-15247,13021882,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,18/Nov/16 23:37,27/Feb/24 22:24,28/Nov/24 15:58,22/Nov/16 23:16,1.2.1,2.0.0,2.1.0,,,,2.3.0,,,,,,0,,,This gives storage handler more control on how to handle drop table.,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/16 23:38;ashutoshc;HIVE-15247.patch;https://issues.apache.org/jira/secure/attachment/12839644/HIVE-15247.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 23:16:13 UTC 2016,,,,,,,,,,"0|i36iz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/16 08:37;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12839644/HIVE-15247.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10728 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2211/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2211/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2211/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12839644 - PreCommit-HIVE-Build;;;","21/Nov/16 22:19;ashutoshc;[~jcamachorodriguez] Can you take a quick look? ;;;","22/Nov/16 09:57;jcamacho;+1;;;","22/Nov/16 23:16;ashutoshc;Pushed to master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Socket timeouts happen when other drivers set DriverManager.loginTimeout,HIVE-22196,13256158,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,,Indigestion,Indigestion,11/Sep/19 17:30,16/Feb/21 19:27,28/Nov/24 15:58,16/Feb/21 19:27,1.2.1,2.0.0,3.1.2,,,,,,,JDBC,Thrift API,,0,,,"There are a few somewhat sketchy things happening in Hive/Thrift code in the JDBC client that result in intermittent ""read timed out"" (and subsequently ""out of sequence"") errors when other JDBC drivers are active in the same client JVM that set {{DriverManager.loginTimeout}}.
 # The login timeout used to initialize a {{HiveConnection}} is populated from {{DriverManager.loginTimeout}} in the core Java JDBC library. This sounds like a nice, orthodox place to get a login timeout from, but it's fundamentally problematic and really shouldn't be used. The reason is that it's a *global* singleton value, and any JDBC Driver (or any other piece of code for that matter) can write to it at will (and is implicitly invited to). The Hive JDBC stack _itself_ writes values to this global setting in a couple of places seemingly unrelated to the client connection setup.
 # The _read_ timeout for Thrift _socket-level_ reads is actually populated from this _login_ timeout (a.k.a. ""connect timeout"") setting. (See Thrift's {{TSocket(String host, int port, int timeout)}} and its callers in {{HiveAuthFactory}}. Also note the numerous code comments that speak of setting {{SO_TIMEOUT}} (the socket read timeout) while the actual code references a variable called {{loginTimeout}}.) Socket reads can occur thousands of times in an application that does lots of Hive queries, and their individual workloads are each individually less predictable than simply getting a connection, which typically happens at most a few times. So you have a huge probability that a login timeout setting, which seems to usually receive a reasonable value of 30 seconds if constrained at all, will occasionally (way too often) be inadequate for a socket read.
 # There seems to be no option to set this login timeout (or the actual read timeout) explicitly as an externalized override setting (but see HIVE-12371). 

*Summary:* {\{DriverManager.loginTimeout}} can be innocently set by any JDBC driver present in the JVM, you can't override it, and it's misused by Hive as a socket read timeout. There's no way to prevent intermittent read timeouts in this scenario unless you're lucky enough to find the JDBC driver and reconfigure its timeout setting to something workable for Hive socket reads.

An easy, crude patch:

modify the first line of {{HiveConnection.setupLoginTimeout()}} from:

{{long timeOut = TimeUnit.SECONDS.toMillis(DriverManager.getLoginTimeout());}}

to:

{{long timeOut = TimeUnit.SECONDS.toMillis(0);}}

This is of course not a robust fix, as server issues during socket reads can result in a hung client thread. Some other hardcoded value might be more advisable, as long as it's long enough to prevent spurious read timeouts.

The right approach is to prioritize HIVE-12371 (proposed socket timeout override setting that doesn't depend on {{DriverManager.loginTimeout}}) and implement it in all possible versions.","Any Hive JDBC client that uses other SQL clients besides Hive, or any other kind of JDBC driver (e.g. connection pooling). This can only happen if the other driver writes values to {{DriverManager.setLoginTimeout()}}. HikariCP is one suspect, there are probably others as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 16 19:27:55 UTC 2021,,,,,,,,,,"0|z06jxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/21 19:27;prasanth_j;Fixed by HIVE-12371;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement table property to address Parquet int96 timestamp bug,HIVE-12767,12924972,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Won't Fix,zsombor.klara,spena,spena,30/Dec/15 23:59,02/Feb/18 12:34,28/Nov/24 15:58,02/Feb/18 12:34,1.2.1,2.0.0,,,,,,,,,,,1,,,"Parque timestamps using INT96 are not compatible with other tools, like Impala, due to issues in Hive because it adjusts timezones values in a different way than Impala.

To address such issues. a new table property (parquet.mr.int96.write.zone) must be used in Hive that detects what timezone to use when writing and reading timestamps from Parquet.

The following is the exit criteria for the fix:

* Hive will read Parquet MR int96 timestamp data and adjust values using a time zone from a table property, if set, or using the local time zone if it is absent. No adjustment will be applied to data written by Impala.
* Hive will write Parquet int96 timestamps using a time zone adjustment from the same table property, if set, or using the local time zone if it is absent. This keeps the data in the table consistent.
* New tables created by Hive will set the table property to UTC if the global option to set the property for new tables is enabled.
** Tables created using CREATE TABLE and CREATE TABLE LIKE FILE will not set the property unless the global setting to do so is enabled.
** Tables created using CREATE TABLE LIKE <OTHER TABLE> will copy the property of the table that is copied.",,,,,,,,,,,SPARK-12297,,,,,HIVE-16088,,,,,HIVE-16465,,,,,,"13/Feb/17 15:04;zsombor.klara;HIVE-12767.10.patch;https://issues.apache.org/jira/secure/attachment/12852366/HIVE-12767.10.patch","24/Feb/17 14:55;zsombor.klara;HIVE-12767.11.patch;https://issues.apache.org/jira/secure/attachment/12854485/HIVE-12767.11.patch","07/Jan/16 16:33;spena;HIVE-12767.3.patch;https://issues.apache.org/jira/secure/attachment/12780997/HIVE-12767.3.patch","13/Jan/16 20:35;spena;HIVE-12767.4.patch;https://issues.apache.org/jira/secure/attachment/12782125/HIVE-12767.4.patch","06/Feb/17 17:51;zsombor.klara;HIVE-12767.5.patch;https://issues.apache.org/jira/secure/attachment/12851198/HIVE-12767.5.patch","07/Feb/17 14:46;zsombor.klara;HIVE-12767.6.patch;https://issues.apache.org/jira/secure/attachment/12851409/HIVE-12767.6.patch","08/Feb/17 16:07;zsombor.klara;HIVE-12767.7.patch;https://issues.apache.org/jira/secure/attachment/12851654/HIVE-12767.7.patch","10/Feb/17 13:30;zsombor.klara;HIVE-12767.8.patch;https://issues.apache.org/jira/secure/attachment/12852050/HIVE-12767.8.patch","13/Feb/17 12:56;zsombor.klara;HIVE-12767.9.patch;https://issues.apache.org/jira/secure/attachment/12852347/HIVE-12767.9.patch","04/Feb/16 00:39;rdblue;TestNanoTimeUtils.java;https://issues.apache.org/jira/secure/attachment/12786152/TestNanoTimeUtils.java",,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 12:34:45 UTC 2018,,,,,,,,,,"0|i2qf73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Dec/15 00:01;spena;cc: [~kamrul];;;","06/Jan/16 16:31;spena;Re-attach patch to trigger Jenkins tests.;;;","08/Jan/16 19:21;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12780997/HIVE-12767.3.patch

{color:green}SUCCESS:{color} +1 due to 4 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 16 failed/errored test(s), 10001 tests executed
*Failed tests:*
{noformat}
TestHWISessionManager - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ppd_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ppd_varchar
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_import
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveOperationType.checkHiveOperationTypeMatch
org.apache.hive.jdbc.TestSSL.testSSLVersion
org.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles
org.apache.hive.spark.client.TestSparkClient.testCounters
org.apache.hive.spark.client.TestSparkClient.testErrorJob
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
org.apache.hive.spark.client.TestSparkClient.testMetricsCollection
org.apache.hive.spark.client.TestSparkClient.testRemoteClient
org.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob
org.apache.hive.spark.client.TestSparkClient.testSyncRpc
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6554/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6554/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-6554/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 16 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12780997 - PreCommit-HIVE-TRUNK-Build;;;","14/Jan/16 01:01;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12782125/HIVE-12767.4.patch

{color:green}SUCCESS:{color} +1 due to 4 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10005 tests executed
*Failed tests:*
{noformat}
TestHWISessionManager - did not produce a TEST-*.xml file
TestSparkCliDriver-timestamp_lazy.q-bucketsortoptimize_insert_4.q-date_udf.q-and-12-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_import
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6616/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6616/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-6616/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12782125 - PreCommit-HIVE-TRUNK-Build;;;","04/Feb/16 00:38;rdblue;[~spena], the q tests from the last patch look great and make me confident that the create table and property behavior are correct. I still think that the time zone conversion isn't implementing what it should (see below). I also found a couple of minor things:

# Minor: The CTAS test says the zone property on the resulting table should be PST, but is is (and should be) UTC. I think the comment is wrong and the behavior is correct.
# It would be nice to validate that the table property always overrides the global option
# Is there a test that files are created with the table property in key/value metadata? I think that the copy from one table to another with a different property set wouldn't work otherwise, but I want to make sure.
# What is the Hive 2.0 test doing? It looks like it is validating that setting the table property to UTC converts values back to local time? Or was the file written with UTC? (If so, we should have one written in another zone.)

The Hive 2.0 test I think demonstrates that the time zone conversion isn't correct. It is correct for the with and without conversion cases. But, it isn't a correct implementation for the meaning of the table property's time zone. In the Hive 2.0 test, it is set to UTC, but the value is read in local (if I'm reading this correctly).

Here's the problem: the back-end converts julian/nanos to year/month/day/hour values, then sets those on a calendar. It then uses the backing time in milliseconds to create a Timestamp. That timestamp is in _local_ time because of HIVE-12192, so to have the time _not_ adjusted, the calendar needs to also use the local zone. So using the local zone ends up not adjusting the value.

The problem is that the current patch uses this implementation to adjust values to the table property zone. That doesn't work because the ""no offset"" zone for the table property is UTC, not the local zone. I'm attaching a file that demonstrates the problem. It uses [Spark's implementation of fromJulianDay|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L180] to demonstrate what I'm talking about. I chose a date/time at random and tested out the two implementations.

Tests that show _correct_ behavior:
* {{testKnownTimestampWithoutConversion}} shows that when UTC is used, the fromJulianDay value is the same as the ""skip conversion"" value returned by NanoTimeUtils.
* {{testKnownTimestampWithConversion}} shows that when the local zone is used (for me, PST), the fromJulianDay value is the same as the ""do not skip conversion"" value returned by NanoTimeUtils.

Tests that show _incorrect_ behavior:
* {{testKnownWithZoneArgumentUTC}} shows that passing a UTC calendar to NanoTimeUtils does not match the fromJulianDay/UTC value that matched the ""skip conversion"" in tests above. This doesn't pass, but should.
* {{testKnownWithWrongZone}} shows that passing a PST calendar to NanoTimeUtils matches the fromJulianDay/UTC value that corresponds to ""skip conversion"". This shouldn't pass, but does.

I'm attaching my test to this and can help if you have any questions about it.;;;","02/Feb/17 18:33;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12786152/TestNanoTimeUtils.java

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3331/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3331/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3331/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-02-02 18:33:04.296
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-3331/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-02-02 18:33:04.300
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 7cca097 HIVE-14420: Fix orc_llap_counters.q test failure in master (Prasanth Jayachandran reviewed by Siddharth Seth)
+ git clean -f -d
Removing ql/src/test/queries/clientpositive/parquet_ppd_multifiles.q
Removing ql/src/test/results/clientpositive/parquet_ppd_multifiles.q.out
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 7cca097 HIVE-14420: Fix orc_llap_counters.q test failure in master (Prasanth Jayachandran reviewed by Siddharth Seth)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-02-02 18:33:05.296
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
patch: **** Only garbage was found in the patch input.
patch: **** Only garbage was found in the patch input.
patch: **** Only garbage was found in the patch input.
fatal: unrecognized input
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12786152 - PreCommit-HIVE-Build;;;","06/Feb/17 17:51;zsombor.klara;As discussed with [~spena] I will continue working on this issue.
I have added [~rdblue]'s unit tests for the NanoTimeUtils class and refactored the utility methods based on the comments.
Unit tests should be passing, but I would like to see the qtests being run by the precommit.
Tomorrow I will work on refactoring/optimising my current implementation.;;;","06/Feb/17 18:05;zsombor.klara;I cannot take over or edit Sergio's review request, so I've raised my own.;;;","06/Feb/17 19:12;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12851198/HIVE-12767.5.patch

{color:green}SUCCESS:{color} +1 due to 6 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10224 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=100)
	[skewjoinopt15.q,auto_join18.q,list_bucket_dml_2.q,input1_limit.q,load_dyn_part3.q,union_remove_14.q,auto_sortmerge_join_14.q,auto_sortmerge_join_15.q,union10.q,bucket_map_join_tez2.q,groupby5_map_skew.q,join_reorder.q,sample1.q,bucketmapjoin8.q,union34.q]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_external_time] (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_int96_timestamp] (batchId=33)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.testWaitQueuePreemption (batchId=278)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3396/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3396/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3396/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12851198 - PreCommit-HIVE-Build;;;","07/Feb/17 14:46;zsombor.klara;Fixed a regression I caused when refactoring the NanoTimeUtils.
To prevent the adjustment we need to use the GMT timezone for impala written parquet files.
This should take care of the parquet qtest failures.;;;","07/Feb/17 15:50;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12851409/HIVE-12767.6.patch

{color:green}SUCCESS:{color} +1 due to 6 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10142 tests executed
*Failed tests:*
{noformat}
TestCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=17)
	[list_bucket_dml_1.q,ppd_join3.q,auto_join23.q,list_bucket_dml_11.q,join10.q,avro_type_evolution.q,create_struct_table.q,cbo_const.q,skewjoin_mapjoin9.q,hook_context_cs.q,subquery_unqualcolumnrefs.q,exim_22_import_exist_authsuccess.q,groupby1.q,cbo_rp_udf_udaf.q,udf_regexp_replace.q,vector_decimal_aggregate.q,authorization_grant_public_role.q,partition_wise_fileformat.q,sort_merge_join_desc_5.q,union_ppr.q,spark_combine_equivalent_work.q,stats_partial_size.q,partition_date2.q,join32.q,list_bucket_dml_14.q,input34.q,insert_values_acid_not_bucketed.q,udf_parse_url.q,schema_evol_text_nonvec_part.q,ctas_char.q]
TestCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=18)
	[encryption_move_tbl.q,cbo_rp_udf_percentile2.q,udf_lower.q,union_remove_11.q,transform_acid.q,constprog_dp.q,join45.q,subquery_multi.q,orc_merge1.q,groupby_multi_single_reducer2.q,offset_limit_global_optimizer.q,dbtxnmgr_query4.q,multi_column_in.q,create_func1.q,create_skewed_table1.q,authorization_view_1.q,show_tables.q,orc_empty_files.q,ba_table2.q,acid_vectorization_project.q,union_remove_19.q,vector_decimal_3.q,stats3.q,update_where_no_match.q,parquet_join.q,exim_24_import_nonexist_authsuccess.q,outer_join_ppr.q,quotedid_partition.q,ansi_sql_arithmetic.q,join26.q]
TestCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=19)
	[cp_mj_rc.q,order.q,udf_bitwise_shiftleft.q,groupby1_limit.q,insert_values_non_partitioned.q,skewjoinopt10.q,extrapolate_part_stats_date.q,udf_sin.q,vectorized_math_funcs.q,vectorization_2.q,join14.q,query_result_fileformat.q,udf3.q,cbo_union_view.q,drop_function.q,tez_union_decimal.q,exim_23_import_part_authsuccess.q,load_dyn_part10.q,nonmr_fetch.q,order_null.q,cbo_rp_views.q,lvj_mapjoin.q,insert_acid_dynamic_partition.q,skewjoinopt6.q,cbo_rp_cross_product_check_2.q,mapreduce7.q,update_two_cols.q,correlationoptimizer11.q,stats_empty_partition.q,authorization_2.q]
TestCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=22)
	[multi_insert_mixed.q,specialChar.q,join_cond_pushdown_3.q,cast_on_constant.q,union_remove_10.q,skewjoinopt21.q,input_part0.q,vector_join_nulls.q,combine2_hadoop20.q,infer_join_preds.q,udf_hex.q,reducesink_dedup.q,insert1.q,input16.q,udf_in_file.q,vector_empty_where.q,udf_variance.q,join42.q,auto_join12.q,groupby_sort_2.q,subquery_alias.q,cte_mat_3.q,parenthesis_star_by.q,vector_decimal_round_2.q,udf_conv.q,column_names_with_leading_and_trailing_spaces.q,vectorized_mapjoin2.q,union_stats.q,nullgroup4.q,authorization_view_disable_cbo_2.q]
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_int96_timestamp] (batchId=33)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3417/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3417/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3417/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12851409 - PreCommit-HIVE-Build;;;","08/Feb/17 15:44;zsombor.klara;Instead of logging a warning throw an exception if an unexpected timezone value is found. (To prevent incorrect data from being written into the table in case of a typo made by the user.);;;","08/Feb/17 16:13;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12851654/HIVE-12767.7.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3442/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3442/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3442/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-02-08 16:13:13.361
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-3442/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-02-08 16:13:13.364
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 8a06b9e HIVE-15626: beeline should not exit after canceling the query on ctrl-c (Vihang Karajgaonkar via Chaoyu Tang)
+ git clean -f -d
Removing ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RepartitionShuffler.java
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 8a06b9e HIVE-15626: beeline should not exit after canceling the query on ctrl-c (Vihang Karajgaonkar via Chaoyu Tang)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-02-08 16:13:14.446
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
fatal: git diff header lacks filename information when removing 0 leading pathname components (line 20)
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12851654 - PreCommit-HIVE-Build;;;","09/Feb/17 20:31;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12851905/HIVE-12767.8.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3471/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3471/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3471/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-02-09 20:31:57.050
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-3471/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-02-09 20:31:57.053
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 36ff484 HIVE-15851 CompactorMR.launchCompactionJob() should use JobClient.submitJob() not runJob (Eugene Koifman, reviewed by Wei Zheng)
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 36ff484 HIVE-15851 CompactorMR.launchCompactionJob() should use JobClient.submitJob() not runJob (Eugene Koifman, reviewed by Wei Zheng)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-02-09 20:31:58.075
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java:127
error: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java: patch does not apply
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12851905 - PreCommit-HIVE-Build;;;","10/Feb/17 13:30;zsombor.klara;Rebased from master because the patch did not apply anymore.;;;","10/Feb/17 14:41;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12852050/HIVE-12767.8.patch

{color:green}SUCCESS:{color} +1 due to 8 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10258 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3490/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3490/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3490/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12852050 - PreCommit-HIVE-Build;;;","10/Feb/17 14:53;zsombor.klara;query14 and encryption_join_with_different_encryption_keys are know flaky tests (HIVE-15744, HIVE-15696);;;","13/Feb/17 12:56;zsombor.klara;Updated based on reviewboard comments.;;;","13/Feb/17 13:57;zsombor.klara;Renamed ParquetTableUtils.PARQUET_INT96_DEFAULT_WRITE_ZONE constant to make the its purpose clearer.;;;","13/Feb/17 14:07;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12852347/HIVE-12767.9.patch

{color:green}SUCCESS:{color} +1 due to 8 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10250 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=140)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3517/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3517/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3517/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12852347 - PreCommit-HIVE-Build;;;","13/Feb/17 14:56;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12852362/HIVE-12767.10.patch

{color:green}SUCCESS:{color} +1 due to 8 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 10250 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3518/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3518/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3518/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12852362 - PreCommit-HIVE-Build;;;","13/Feb/17 16:38;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12852366/HIVE-12767.10.patch

{color:green}SUCCESS:{color} +1 due to 8 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10236 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=101)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3520/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3520/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3520/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12852366 - PreCommit-HIVE-Build;;;","22/Feb/17 17:29;spena;Great, thanks [~zsombor.klara] for continuing on this work. I just left a couple of comments on the RB.;;;","24/Feb/17 14:55;zsombor.klara;Addressed Sergio's review board comments.;;;","24/Feb/17 23:17;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12854485/HIVE-12767.11.patch

{color:green}SUCCESS:{color} +1 due to 8 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10274 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=211)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3764/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3764/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3764/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12854485 - PreCommit-HIVE-Build;;;","27/Feb/17 23:21;spena;+1

Thanks [~zsombor.klara]. I think the patch is good now. 
Thanks for continuing the work on this patch.;;;","28/Feb/17 17:50;spena;Thanks [~zsombor.klara] for the patch. I committed this to master.

This new patch added new configuration variables and table properties. Could you write this info on the wiki?;;;","01/Mar/17 08:26;leftyl;Here's where the documentation belongs:

* [Parquet | https://cwiki.apache.org/confluence/display/Hive/Parquet]
* [Configuration Properties -- Parquet | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-Parquet] for *parquet.mr.int96.enable.utc.write.zone* 
* [Table Properties | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-listTableProperties]

Question:  Shouldn't the config name start with ""hive."" as in ""hive.parquet.mr.int96.enable.utc.write.zone""?

Edit (2/Mar/17):  The configuration parameter was renamed to include the ""hive."" prefix by HIVE-16088.;;;","01/Mar/17 10:18;zsombor.klara;Thanks [~leftylev] for the help, I updated the docs you mentioned.
As for the ""hive."" prefix, I don't know if this is a strict convention or not. While most configuration properties start with the ""hive"" prefix, others do not, like ""parquet.memory.pool.ratio"", and I can't see a clear rule clarifying when the prefix is needed.

[~spena] should we (should I) rename it or was is meant to be like this?;;;","02/Mar/17 09:46;zsombor.klara;I've raised HIVE-16088 to fix the misnamed configuration property.;;;","19/May/17 04:54;leftyl;Thanks for the docs, [~zsombor.klara].  I changed the version numbers from 2.2.0 to 2.4.0 in the wiki and removed the TODOC2.2 label from this issue.

Here are the doc links again:

* [Parquet -- Hive2.4.0 | https://cwiki.apache.org/confluence/display/Hive/Parquet#Parquet-Hive2.4.0]
* [Configuration Properties -- hive.parquet.mr.int96.enable.utc.write.zone | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.parquet.mr.int96.enable.utc.write.zone]
* [DDL -- TBLPROPERTIES | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-listTableProperties];;;","28/Jun/17 22:09;spena;This patch is reverted from branch-2 and master due to some to other ideas on how to solve this issue.;;;","06/Jul/17 21:35;leftyl;Added a TODOC2.4 label in case we need to revert the documentation (3 wikidocs listed in comment 19/May/17).  But let's wait a while and see how the issue is solved.;;;","05/Sep/17 06:54;leftyl;The documentation was reverted (thanks, [~zsombor.klara]) so I'll remove the TODOC2.4 label.;;;","02/Feb/18 12:34;zi;Hive already has a workaround based on a the writer metadata. This issue was about a more sophisticated and complicated solution based on table properties. But since the Spark community decided to implement a similar workaround to the one that already exists in Hive (based on a the writer metadata), the solution using table properties is not needed any more.;;;",,,,,,,,,,
Extended ACLs are not handled according to specification,HIVE-13989,12977453,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,cdrome,cdrome,cdrome,10/Jun/16 00:47,18/Sep/17 04:52,28/Nov/24 15:58,09/Sep/17 19:24,1.2.1,2.0.0,,,,,2.2.1,2.3.0,,HCatalog,,,1,,,"Hive takes two approaches to working with extended ACLs depending on whether data is being produced via a Hive query or HCatalog APIs. A Hive query will run an FsShell command to recursively set the extended ACLs for a directory sub-tree. HCatalog APIs will attempt to build up the directory sub-tree programmatically and runs some code to set the ACLs to match the parent directory.

Some incorrect assumptions were made when implementing the extended ACLs support. Refer to https://issues.apache.org/jira/browse/HDFS-4685 for the design documents of extended ACLs in HDFS. These documents model the implementation after the POSIX implementation on Linux, which can be found at http://www.vanemery.com/Linux/ACL/POSIX_ACL_on_Linux.html.

The code for setting extended ACLs via HCatalog APIs is found in HdfsUtils.java:

{code}
    if (aclEnabled) {
      aclStatus =  sourceStatus.getAclStatus();
      if (aclStatus != null) {
        LOG.trace(aclStatus.toString());
        aclEntries = aclStatus.getEntries();
        removeBaseAclEntries(aclEntries);

        //the ACL api's also expect the tradition user/group/other permission in the form of ACL
        aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.USER, sourcePerm.getUserAction()));
        aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.GROUP, sourcePerm.getGroupAction()));
        aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.OTHER, sourcePerm.getOtherAction()));
      }
    }
{code}

We found that DEFAULT extended ACL rules were not being inherited properly by the directory sub-tree, so the above code is incomplete because it effectively drops the DEFAULT rules. The second problem is with the call to {{sourcePerm.getGroupAction()}}, which is incorrect in the case of extended ACLs. When extended ACLs are used the GROUP permission is replaced with the extended ACL mask. So the above code will apply the wrong permissions to the GROUP. Instead the correct GROUP permissions now need to be pulled from the AclEntry as returned by {{getAclStatus().getEntries()}}. See the implementation of the new method {{getDefaultAclEntries}} for details.

Similar issues exist with the HCatalog API. None of the API accounts for setting extended ACLs on the directory sub-tree. The changes to the HCatalog API allow the extended ACLs to be passed into the required methods similar to how basic permissions are passed in. When building the directory sub-tree the extended ACLs of the table directory are inherited by all sub-directories, including the DEFAULT rules.

Replicating the problem:

Create a table to write data into (I will use acl_test as the destination and words_text as the source) and set the ACLs as follows:

{noformat}
$ hdfs dfs -setfacl -m default:user::rwx,default:group::r-x,default:mask::rwx,default:user:hdfs:rwx,group::r-x,user:hdfs:rwx /user/cdrome/hive/acl_test

$ hdfs dfs -ls -d /user/cdrome/hive/acl_test
drwxrwx---+  - cdrome hdfs          0 2016-07-13 20:36 /user/cdrome/hive/acl_test

$ hdfs dfs -getfacl -R /user/cdrome/hive/acl_test
# file: /user/cdrome/hive/acl_test
# owner: cdrome
# group: hdfs
user::rwx
user:hdfs:rwx
group::r-x
mask::rwx
other::---
default:user::rwx
default:user:hdfs:rwx
default:group::r-x
default:mask::rwx
default:other::---
{noformat}

Note that the basic GROUP permission is set to {{rwx}} after setting the ACLs. The ACLs explicitly set the DEFAULT rules and a rule specifically for the {{hdfs}} user.

Run the following query to populate the table:

{noformat}
insert into acl_test partition (dt='a', ds='b') select a, b from words_text where dt = 'c';
{noformat}

Note that words_text only has a single partition key.

Now examine the ACLs for the resulting directories:

{noformat}
$ hdfs dfs -getfacl -R /user/cdrome/hive/acl_test
# file: /user/cdrome/hive/acl_test
# owner: cdrome
# group: hdfs
user::rwx
user:hdfs:rwx
group::r-x
mask::rwx
other::---
default:user::rwx
default:user:hdfs:rwx
default:group::r-x
default:mask::rwx
default:other::---

# file: /user/cdrome/hive/acl_test/dt=a
# owner: cdrome
# group: hdfs
user::rwx
user:hdfs:rwx
group::rwx
mask::rwx
other::---
default:user::rwx
default:user:hdfs:rwx
default:group::rwx
default:mask::rwx
default:other::---

# file: /user/cdrome/hive/acl_test/dt=a/ds=b
# owner: cdrome
# group: hdfs
user::rwx
user:hdfs:rwx
group::rwx
mask::rwx
other::---
default:user::rwx
default:user:hdfs:rwx
default:group::rwx
default:mask::rwx
default:other::---

# file: /user/cdrome/hive/acl_test/dt=a/ds=b/000000_0.deflate
# owner: cdrome
# group: hdfs
user::rwx
user:hdfs:rwx
group::rwx
mask::rwx
other::---
{noformat}

Note that the GROUP permission is now erroneously set to {{rwx}} because of the code mentioned above; it is set to the same value as the ACL mask.

The code changes for the HCatalog APIs is synonymous to the {{applyGroupAndPerms}} method which ensures that all new directories are created with the same permissions as the table. This patch will ensure that changes to intermediate directories will not be propagated, instead the table ACLs will be applied to all new directories created.

I would also like to call out that the older versions of HDFS which support ACLs had a number issues in addition to those mentioned here which appear to have been addressed in later versions of Hadoop. This patch was originally written to work with a version of Hadoop-2.6, we are now using Hadoop-2.7 which appears to have fixed some of them. However, I think that this patch is still required for correct behavior of ACLs with Hive/HCatalog.",,,,,,,,,,,,,,HIVE-11481,,,,,,,,,,,HIVE-13990,,"10/Jun/16 20:15;cdrome;HIVE-13989-branch-1.patch;https://issues.apache.org/jira/secure/attachment/12809542/HIVE-13989-branch-1.patch","25/Jul/17 20:47;cdrome;HIVE-13989-branch-2.2.patch;https://issues.apache.org/jira/secure/attachment/12878901/HIVE-13989-branch-2.2.patch","17/Jul/17 20:13;vgumashta;HIVE-13989-branch-2.2.patch;https://issues.apache.org/jira/secure/attachment/12877663/HIVE-13989-branch-2.2.patch","17/Jul/17 18:36;vgumashta;HIVE-13989-branch-2.2.patch;https://issues.apache.org/jira/secure/attachment/12877640/HIVE-13989-branch-2.2.patch","22/Jun/16 20:34;cdrome;HIVE-13989.1-branch-1.patch;https://issues.apache.org/jira/secure/attachment/12812614/HIVE-13989.1-branch-1.patch","22/Jun/16 20:34;cdrome;HIVE-13989.1.patch;https://issues.apache.org/jira/secure/attachment/12812615/HIVE-13989.1.patch","18/Aug/17 05:08;cdrome;HIVE-13989.4-branch-2.2.patch;https://issues.apache.org/jira/secure/attachment/12882512/HIVE-13989.4-branch-2.2.patch","24/Aug/17 06:45;cdrome;HIVE-13989.4-branch-2.patch;https://issues.apache.org/jira/secure/attachment/12883493/HIVE-13989.4-branch-2.patch",,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Mon Sep 18 04:52:49 UTC 2017,,,,,,,,,,"0|i2z9fb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/16 21:06;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12812615/HIVE-13989.1.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10256 tests executed
*Failed tests:*
{noformat}
TestNegativeMinimrCliDriver-udf_local_resource.q-mapreduce_stack_trace_turnoff_hadoop20.q-mapreduce_stack_trace.q-and-6-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vector_complex_all
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vector_complex_join
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_index_bitmap3
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/235/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/235/console
Test logs: http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-235/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12812615 - PreCommit-HIVE-MASTER-Build;;;","23/Jun/16 23:23;ashutoshc;[~cdrome] can you create a ReviewBoard for this? And also, describe what the issue you are trying to fix here?;;;","09/Jul/16 16:48;ashutoshc;[~spena] Might be of interest to you.;;;","11/Jul/16 16:11;spena;[~cdrome] Besides the description of the issue, could you add an example on how to reproduce it?

[~ashutoshc] Are we continuing adding fixes to the branch-1?;;;","13/Jul/16 19:51;cdrome;[~ashutoshc], [~spena], sorry for the delay in updating details about this ticket.

This is a patch that we have had to use internally since 0.13.
I don't have access to a branch-2 cluster, but I can add some notes about how to replicate these failures on branch-1 with the version of Hadoop we use.;;;","14/Jul/16 06:04;cdrome;[~ashutoshc], I created the following reviewboard request: https://reviews.apache.org/r/50018/;;;","03/Sep/16 00:52;ashutoshc;[~cdrome] You noted all these issues may not be present on hadoop 2.7 Now that Hive is on 2.7 , is whole of this patch still needed?;;;","03/Sep/16 01:09;cdrome;[~ashutoshc], I believe that I confirmed the Hive side of this patch is still necessary, but the HCatalog side of the patch may not be.

I likely won't have time to give solid confirmation next week, but will try to get closure on this issue the follow week.;;;","04/Sep/16 01:08;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12812615/HIVE-13989.1.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1104/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1104/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-1104/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.8.0_25 ]]
+ export JAVA_HOME=/usr/java/jdk1.8.0_25
+ JAVA_HOME=/usr/java/jdk1.8.0_25
+ export PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-MASTER-Build-1104/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at b74c4d0 HIVE-13383 : RetryingMetaStoreClient retries non retriable embedded metastore client (Thejas Nair via Ashutosh Chauhan)
+ git clean -f -d
Removing ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java.orig
Removing ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSortArrayByField.java
Removing ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFSortArrayByField.java
Removing ql/src/test/queries/clientnegative/udf_sort_array_by_wrong1.q
Removing ql/src/test/queries/clientnegative/udf_sort_array_by_wrong2.q
Removing ql/src/test/queries/clientnegative/udf_sort_array_by_wrong3.q
Removing ql/src/test/queries/clientpositive/udf_sort_array_by.q
Removing ql/src/test/results/clientnegative/udf_sort_array_by_wrong1.q.out
Removing ql/src/test/results/clientnegative/udf_sort_array_by_wrong2.q.out
Removing ql/src/test/results/clientnegative/udf_sort_array_by_wrong3.q.out
Removing ql/src/test/results/clientpositive/udf_sort_array_by.q.out
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at b74c4d0 HIVE-13383 : RetryingMetaStoreClient retries non retriable embedded metastore client (Thejas Nair via Ashutosh Chauhan)
+ git merge --ff-only origin/master
Already up-to-date.
+ git gc
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
error: patch failed: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:32
error: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java: patch does not apply
error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:2916
error: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java: patch does not apply
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12812615 - PreCommit-HIVE-MASTER-Build;;;","07/Sep/16 01:01;caritaou;Hi [~cdrome] and [~ashutoshc]

The patch in HIVE-13989 doesn't test ACL inheritance correctly. In FolderPermissionBase.java, some of the tests explicitly set the permissions for the table and the partition directories instead of letting the partitions inherit its permissions from its parent directory. Also, the tests doesn't determine the correct ACLs the child directory should inherit. For example, if the parent directory has a DEFAULT ACL entry set to ""default:user:bar:rw-"", the test should check that the child directory/file has the corresponding ACCESS ACL entry ""access:user:bar:rw-"".

Besides the testcases, I noticed some of the DEFAULT|ACCESS ACLs the parent directory has were not inherited as ACCESS ACLs in the child directory.
{noformat}
For example, the warehouse directory has the following ACLs set:
ACCESS        type:USER       name: null      perm: ALL
ACCESS        type:GROUP      name: null      perm: READ_WRITE
ACCESS        type:OTHER      name: null      perm: NONE
ACCESS        type:USER       name: bar       perm: READ_WRITE
ACCESS        type:USER       name: foo       perm: READ_EXECUTE
ACCESS        type:GROUP      name: bar       perm: READ_WRITE
ACCESS        type:GROUP      name: foo       perm: READ_EXECUTE
DEFAULT	      type:USER       name: null      perm: ALL
DEFAULT	      type:USER       name: foo       perm: READ
DEFAULT	      type:GROUP      name: null      perm: READ
DEFAULT	      type:OTHER      name: null      perm: READ

but the table dualstaticpart has the following ACLs:
ACCESS        type:USER       name: bar       perm: READ_WRITE
ACCESS        type:USER       name: foo       perm: READ_EXECUTE
ACCESS        type:GROUP      name: null      perm: READ_WRITE
ACCESS        type:GROUP      name: bar       perm: READ_WRITE
ACCESS        type:GROUP      name: foo       perm: READ_EXECUTE
DEFAULT       type:USER       name: null      perm: ALL
DEFAULT       type:USER       name: foo       perm: READ
DEFAULT       type:GROUP      name: null      perm: READ
DEFAULT       type:MASK       name: null      perm: READ
DEFAULT       type:OTHER      name: null      perm: READ

Instead, it should be:
ACCESS        type:USER       name: bar       perm: READ_WRITE
ACCESS        type:USER       name: foo       perm: READ
ACCESS        type:GROUP      name: null      perm: READ
ACCESS        type:GROUP      name: bar       perm: READ_WRITE
ACCESS        type:GROUP      name: foo       perm: READ_EXECUTE
DEFAULT       type:USER       name: null      perm: ALL
DEFAULT       type:USER       name: foo       perm: READ
DEFAULT       type:GROUP      name: null      perm: READ
DEFAULT       type:MASK       name: null      perm: READ
DEFAULT       type:OTHER      name: null      perm: READ
{noformat}

I closed HIVE-11481 as a duplicate of this one since both Jiras have the same description and merged the changes from both Jiras together with some additional changes on top. I'm not able to upload a new patch, but here is the link to the reviewboard: https://reviews.apache.org/r/51684/

This patch fixes the group permissions and DEFAULT ACL inheritance as described in the description, and additionally, fix the FolderPermissionBase test cases for the partitions to inherit the parent's permissions.;;;","12/Sep/16 21:55;cdrome;Thanks for the comment [~caritaou].

I will look into it.;;;","31/Oct/16 20:11;gates;Cancelling patch as it looks like further investigation is required.;;;","10/Jul/17 23:14;vgumashta;[~cdrome] Thanks for the work so far. Looks like a bug we should definitely merge into master. Will you have time to address [~caritaou]'s review comments?;;;","11/Jul/17 00:25;cdrome;[~vgumashta], yes, I will come back to this and verify whether there are still issues in trunk (this patch was originally written against 1.2).;;;","11/Jul/17 00:49;vgumashta;Thanks a lot [~cdrome];;;","14/Jul/17 20:06;vgumashta;Attaching patch for branch 2.3. ;;;","14/Jul/17 20:07;vgumashta;[~cdrome] I was able to rebase on 2.3 (the jira is not relevant for master due to HIVE-16392). Would be great if you could take a look whenever you get time. Thanks;;;","14/Jul/17 20:57;cdrome;[~vgumashta], thanks for the rebase. I'll try to look at it next week.;;;","14/Jul/17 21:02;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877381/HIVE-13989-branch-2.3.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6040/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6040/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6040/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-07-14 21:01:32.392
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-6040/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z branch-2.3 ]]
+ [[ -d apache-github-branch-2.3-source ]]
+ [[ ! -d apache-github-branch-2.3-source/.git ]]
+ [[ ! -d apache-github-branch-2.3-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-07-14 21:01:32.395
+ cd apache-github-branch-2.3-source
+ git fetch origin
From https://github.com/apache/hive
   31cee7e..6f4c35c  branch-2.3 -> origin/branch-2.3
   4514ec9..d3ba76d  master     -> origin/master
 * [new tag]         release-2.3.0-rc1 -> release-2.3.0-rc1
+ git reset --hard HEAD
HEAD is now at 31cee7e HIVE-15144: JSON.org license is now CatX (Owen O'Malley, reviewed by Alan Gates)
+ git clean -f -d
+ git checkout branch-2.3
Already on 'branch-2.3'
Your branch is behind 'origin/branch-2.3' by 2 commits, and can be fast-forwarded.
  (use ""git pull"" to update your local branch)
+ git reset --hard origin/branch-2.3
HEAD is now at 6f4c35c Release Notes
+ git merge --ff-only origin/branch-2.3
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-07-14 21:01:36.454
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
Going to apply patch with: patch -p1
patching file hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java
patching file itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestExtendedAcls.java
patching file itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/FolderPermissionBase.java
patching file ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
patching file ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
patching file shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java
patching file shims/common/src/main/test/org/apache/hadoop/hive/io/TestHdfsUtils.java
+ [[ maven == \m\a\v\e\n ]]
+ rm -rf /data/hiveptest/working/maven/org/apache/hive
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
[ERROR] Failed to execute goal on project spark-client: Could not resolve dependencies for project org.apache.hive:spark-client:jar:2.3.0: Could not find artifact org.apache.hive:hive-storage-api:jar:2.4.0 in datanucleus (http://www.datanucleus.org/downloads/maven2) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :spark-client
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877381 - PreCommit-HIVE-Build;;;","17/Jul/17 18:18;vgumashta;The branch-2.3 patch is not relevant due to changes in HIVE-15385. Attaching patch for branch-2.2 which merges [~caritaou]'s changes as well.;;;","17/Jul/17 18:22;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12812615/HIVE-13989.1.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6064/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6064/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6064/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-07-17 18:22:39.938
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-6064/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-07-17 18:22:39.940
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 1fe8db6 HIVE-14988 : Support INSERT OVERWRITE into a partition on transactional tables (Wei Zheng, reviewed by Eugene Koifman)
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 1fe8db6 HIVE-14988 : Support INSERT OVERWRITE into a partition on transactional tables (Wei Zheng, reviewed by Eugene Koifman)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-07-17 18:22:42.710
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: patch failed: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:32
error: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java: patch does not apply
error: itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestExtendedAcls.java: No such file or directory
error: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/FolderPermissionBase.java: No such file or directory
error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:2720
error: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java: patch does not apply
error: patch failed: shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java:19
error: shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java: patch does not apply
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12812615 - PreCommit-HIVE-Build;;;","17/Jul/17 18:32;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877638/HIVE-13989-branch-2.2.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6065/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6065/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6065/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Comparable.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/core/PackagesResourceConfig.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-servlet/1.14/jersey-servlet-1.14.jar(com/sun/jersey/spi/container/servlet/ServletContainer.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/FileInputStream.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/ql/target/hive-exec-2.2.0-SNAPSHOT.jar(org/apache/commons/lang3/StringUtils.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/ql/target/hive-exec-2.2.0-SNAPSHOT.jar(org/apache/commons/lang3/ArrayUtils.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-hdfs/2.8.0/hadoop-hdfs-2.8.0.jar(org/apache/hadoop/hdfs/web/AuthFilter.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/Utils.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/security/UserGroupInformation.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-auth/2.8.0/hadoop-auth-2.8.0.jar(org/apache/hadoop/security/authentication/client/PseudoAuthenticator.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-auth/2.8.0/hadoop-auth-2.8.0.jar(org/apache/hadoop/security/authentication/server/PseudoAuthenticationHandler.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/util/GenericOptionsParser.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/rewrite/handler/RedirectPatternRule.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/rewrite/handler/RewriteHandler.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/server/Handler.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/server/Server.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/server/handler/HandlerList.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/FilterHolder.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/FilterMapping.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/ServletContextHandler.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/ServletHolder.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/xml/XmlConfiguration.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/slf4j/jul-to-slf4j/1.7.10/jul-to-slf4j-1.7.10.jar(org/slf4j/bridge/SLF4JBridgeHandler.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar(javax/servlet/http/HttpServletRequest.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceAudience$LimitedPrivate.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability$Unstable.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/ByteArrayOutputStream.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/OutputStream.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/Closeable.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/AutoCloseable.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/Flushable.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(javax/xml/bind/annotation/XmlRootElement.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/ExecuteException.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/security/PrivilegedExceptionAction.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/HadoopShimsSecure.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/ShimLoader.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/HadoopShims.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/HadoopShims$WebHCatJTShim.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/util/ToolRunner.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/InterruptedException.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Boolean.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/ql/target/hive-exec-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/ql/ErrorMsg.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Integer.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapred/JobStatus.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/FileNotFoundException.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URISyntaxException.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URI.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/fs/FileSystem.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/metastore/target/hive-metastore-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/metastore/api/MetaException.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/io/Text.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/security/Credentials.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/security/token/Token.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar(org/apache/thrift/TException.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/InetAddress.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/UnknownHostException.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/text/MessageFormat.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/regex/Matcher.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/regex/Pattern.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/DELETE.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/FormParam.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/GET.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/POST.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/PUT.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/Path.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/PathParam.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/Produces.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/QueryParam.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/Context.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/SecurityContext.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriInfo.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapred/JobProfile.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Long.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/JavaUtils.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/commons-lang/commons-lang/2.6/commons-lang-2.6.jar(org/apache/commons/lang/StringUtils.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/fs/FileStatus.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/wadl/config/WadlGeneratorConfig.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/wadl/config/WadlGeneratorDescription.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/server/wadl/generators/resourcedoc/WadlGeneratorResourceDocSupport.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/BufferedReader.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/InputStream.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/InputStreamReader.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/PrintWriter.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Map$Entry.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/Semaphore.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/CommandLine.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/DefaultExecutor.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/ExecuteWatchdog.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/PumpStreamHandler.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/util/Shell.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Thread.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Runnable.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/ext/ExceptionMapper.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/ext/Provider.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/NotFoundException.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapred/JobID.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/security/Groups.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/HashSet.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Set.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/ConcurrentHashMap.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hive/common/util/HiveVersionInfo.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability$Evolving.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/DataInput.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/DataOutput.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/InputSplit.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFramework.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/CreateMode.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/KeeperException.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs$Ids.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/OutputStreamWriter.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URLConnection.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapred/JobClient.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapred/JobConf.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapred/RunningJob.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/StringTokenizer.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Process.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/StringBuilder.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/io/NullWritable.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/InputFormat.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/JobContext.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/RecordReader.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/TaskAttemptContext.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/conf/Configured.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/Job.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/JobID.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/security/token/delegation/DelegationTokenIdentifier.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/util/Tool.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.8.0/hadoop-common-2.8.0.jar(org/apache/hadoop/conf/Configurable.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/ClassNotFoundException.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFrameworkFactory.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar(org/apache/curator/retry/ExponentialBackoffRetry.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/Mapper.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Iterator.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/LinkedList.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/ExecutorService.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/Executors.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/TimeUnit.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.8.0/hadoop-mapreduce-client-core-2.8.0.jar(org/apache/hadoop/mapreduce/Mapper$Context.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URLDecoder.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Enumeration.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Properties.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriBuilder.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/LogUtils.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Class.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Annotation.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-annotations/2.8.0/hadoop-annotations-2.8.0.jar(org/apache/hadoop/classification/InterfaceAudience.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-annotations/2.8.0/hadoop-annotations-2.8.0.jar(org/apache/hadoop/classification/InterfaceAudience$LimitedPrivate.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Retention.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/RetentionPolicy.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Target.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/ElementType.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/HttpMethod.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/SuppressWarnings.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Override.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(sun/misc/Contended.class)]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatException$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatDelegator$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/Server$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$3.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/LauncherDelegator$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$2.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/HDFSStorage$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonUtils$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/ZooKeeperStorage$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-branch-2.2-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/LogRetriever$1.class]]
[done in 3175 ms]
+ [[ -d itests ]]
+ cd itests
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
[ERROR] COMPILATION ERROR : 
[ERROR] /data/hiveptest/working/apache-github-branch-2.2-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestInheritPermsExtendedAcls.java:[159,61] cannot find symbol
  symbol:   method getAclStatus()
  location: variable parentFullFileStatus of type org.apache.hadoop.hive.io.HdfsUtils.HadoopFileStatus
[ERROR] /data/hiveptest/working/apache-github-branch-2.2-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestInheritPermsExtendedAcls.java:[161,61] cannot find symbol
  symbol:   method getAclStatus()
  location: variable actualFullFileStatus of type org.apache.hadoop.hive.io.HdfsUtils.HadoopFileStatus
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-it-unit-hadoop2: Compilation failure: Compilation failure:
[ERROR] /data/hiveptest/working/apache-github-branch-2.2-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestInheritPermsExtendedAcls.java:[159,61] cannot find symbol
[ERROR] symbol:   method getAclStatus()
[ERROR] location: variable parentFullFileStatus of type org.apache.hadoop.hive.io.HdfsUtils.HadoopFileStatus
[ERROR] /data/hiveptest/working/apache-github-branch-2.2-source/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestInheritPermsExtendedAcls.java:[161,61] cannot find symbol
[ERROR] symbol:   method getAclStatus()
[ERROR] location: variable actualFullFileStatus of type org.apache.hadoop.hive.io.HdfsUtils.HadoopFileStatus
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-unit-hadoop2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877638 - PreCommit-HIVE-Build;;;","17/Jul/17 20:06;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877640/HIVE-13989-branch-2.2.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 140 failed/errored test(s), 10605 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=245)
TestJdbcDriver2 - did not produce a TEST-*.xml file (likely timed out) (batchId=226)
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=168)
	[acid_globallimit.q,alter_merge_2_orc.q]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_globallimit] (batchId=27)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[avro_nullable_union] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_union_view] (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[druid_basic2] (batchId=10)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[druid_intervals] (batchId=21)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[druid_timeseries] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[druid_topn] (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynpart_sort_optimization_acid] (batchId=41)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[explain_logical] (batchId=59)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join32] (batchId=17)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[order_null] (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[position_alias_test_1] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_union_view] (batchId=27)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[show_create_table_db_table] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats_list_bucket] (batchId=62)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_notin] (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_notin_having] (batchId=45)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_views] (batchId=13)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[table_access_keys_stats] (batchId=65)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_fast_stats] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_view] (batchId=14)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[view_cbo] (batchId=62)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=173)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_move_tbl] (batchId=170)
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver[hbase_viewjoins] (batchId=88)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[dynamic_partition_pruning] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[join_acid_non_acid] (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_llap_counters1] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_llap_counters] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_acid_mapwork_part] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_acid_mapwork_table] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_acidvec_mapwork_part] (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_acidvec_mapwork_table] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_fetchwork_part] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_fetchwork_table] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_part] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_part_all_primitive] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_table] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_part] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_part_all_primitive] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_table] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_part] (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_part_all_primitive] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_table] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vec_mapwork_part] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vec_mapwork_part_all_primitive] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vec_mapwork_table] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_part] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_part_all_primitive] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_table] (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[subquery_multi] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[subquery_scalar] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[tez_dynpart_hashjoin_3] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_complex_join] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[constprog_partitioner] (batchId=176)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[auto_sortmerge_join_12] (batchId=101)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=94)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_1] (batchId=102)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[limit_pushdown] (batchId=112)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[metadataonly1] (batchId=111)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_acid_mapwork_part] (batchId=103)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_acid_mapwork_table] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_acidvec_mapwork_part] (batchId=116)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_acidvec_mapwork_table] (batchId=105)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_fetchwork_part] (batchId=106)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_fetchwork_table] (batchId=102)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_part] (batchId=93)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_part_all_primitive] (batchId=113)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_table] (batchId=112)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_part] (batchId=105)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_part_all_primitive] (batchId=104)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_table] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_part] (batchId=115)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_part_all_primitive] (batchId=96)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_table] (batchId=101)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vec_mapwork_part] (batchId=111)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vec_mapwork_part_all_primitive] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vec_mapwork_table] (batchId=98)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_part] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_part_all_primitive] (batchId=95)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_table] (batchId=114)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[unionDistinct_1] (batchId=106)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_join30] (batchId=101)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join0] (batchId=109)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join1] (batchId=105)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join2] (batchId=100)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join3] (batchId=101)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join4] (batchId=116)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join5] (batchId=116)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[avro_non_nullable_union] (batchId=85)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query83] (batchId=236)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cbo_subq_not_in] (batchId=140)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[count] (batchId=130)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[date_udf] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join0] (batchId=144)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_view] (batchId=154)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[leftsemijoin] (batchId=138)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[metadata_only_queries] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[outer_join_ppr] (batchId=128)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ptf] (batchId=126)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_exists] (batchId=136)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_in] (batchId=146)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multiinsert] (batchId=155)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[table_access_keys_stats] (batchId=149)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[temp_table_gb1] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[udf_percentile] (batchId=146)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union25] (batchId=148)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_null] (batchId=154)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_between_in] (batchId=143)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_cast_constant] (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_mapjoin_reduce] (batchId=153)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_0] (batchId=154)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_3] (batchId=152)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_div0] (batchId=148)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_short_regress] (batchId=139)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_ptf] (batchId=146)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[windowing] (batchId=141)
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1 (batchId=219)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testCreateDb (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testCreateTable (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testCtas (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testExim (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testExternalTable (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertDualDynamicPartitions (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertNonPartTable (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertSingleDynamicPartition (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertStaticDualPartition (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertStaticSinglePartition (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testLoad (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testLoadLocal (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testPartition (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testTruncateTable (batchId=233)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgress (batchId=223)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0] (batchId=182)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0] (batchId=182)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1] (batchId=182)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching (batchId=229)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6066/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6066/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6066/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 140 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877640 - PreCommit-HIVE-Build;;;","17/Jul/17 20:14;vgumashta;Attaching a dummy patch to see what tests fail in 2.2 w/o this patch;;;","17/Jul/17 22:39;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877663/HIVE-13989-branch-2.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 129 failed/errored test(s), 10591 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=245)
TestJdbcDriver2 - did not produce a TEST-*.xml file (likely timed out) (batchId=226)
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=168)
	[acid_globallimit.q,alter_merge_2_orc.q]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_globallimit] (batchId=27)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=44)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[avro_nullable_union] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_union_view] (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[druid_basic2] (batchId=10)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[druid_intervals] (batchId=21)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[druid_timeseries] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[druid_topn] (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynpart_sort_optimization_acid] (batchId=41)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[explain_logical] (batchId=59)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join32] (batchId=17)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_12] (batchId=46)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_13] (batchId=23)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[order_null] (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[position_alias_test_1] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_union_view] (batchId=27)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[show_create_table_db_table] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats_list_bucket] (batchId=62)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_notin] (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_notin_having] (batchId=45)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_views] (batchId=13)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[table_access_keys_stats] (batchId=65)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_fast_stats] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_view] (batchId=14)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[view_cbo] (batchId=62)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=173)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_move_tbl] (batchId=170)
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver[hbase_viewjoins] (batchId=88)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[dynamic_partition_pruning] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[join_acid_non_acid] (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_llap_counters1] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_llap_counters] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_acid_mapwork_part] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_acid_mapwork_table] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_acidvec_mapwork_part] (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_acidvec_mapwork_table] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_fetchwork_part] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_fetchwork_table] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_part] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_part_all_primitive] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_table] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_part] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_part_all_primitive] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_table] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_part] (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_part_all_primitive] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_table] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vec_mapwork_part] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vec_mapwork_part_all_primitive] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vec_mapwork_table] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_part] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_part_all_primitive] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_table] (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[subquery_multi] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[subquery_scalar] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[tez_dynpart_hashjoin_3] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_complex_join] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[constprog_partitioner] (batchId=176)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=94)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_1] (batchId=102)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[limit_pushdown] (batchId=112)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[metadataonly1] (batchId=111)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_acid_mapwork_part] (batchId=103)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_acid_mapwork_table] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_acidvec_mapwork_part] (batchId=116)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_acidvec_mapwork_table] (batchId=105)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_fetchwork_part] (batchId=106)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_fetchwork_table] (batchId=102)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_part] (batchId=93)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_part_all_primitive] (batchId=113)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_nonvec_mapwork_table] (batchId=112)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_part] (batchId=105)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_part_all_primitive] (batchId=104)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_orc_vec_mapwork_table] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_part] (batchId=115)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_part_all_primitive] (batchId=96)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_nonvec_mapwork_table] (batchId=101)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vec_mapwork_part] (batchId=111)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vec_mapwork_part_all_primitive] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vec_mapwork_table] (batchId=98)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_part] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_part_all_primitive] (batchId=95)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[schema_evol_text_vecrow_mapwork_table] (batchId=114)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[unionDistinct_1] (batchId=106)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_join30] (batchId=101)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join0] (batchId=109)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join1] (batchId=105)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join2] (batchId=100)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join3] (batchId=101)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join4] (batchId=116)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_outer_join5] (batchId=116)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[avro_non_nullable_union] (batchId=85)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=236)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query83] (batchId=236)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cbo_subq_not_in] (batchId=140)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[count] (batchId=130)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[date_udf] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join0] (batchId=144)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_view] (batchId=154)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[leftsemijoin] (batchId=138)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[metadata_only_queries] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[outer_join_ppr] (batchId=128)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ptf] (batchId=126)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_exists] (batchId=136)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_in] (batchId=146)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multiinsert] (batchId=155)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[table_access_keys_stats] (batchId=149)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[temp_table_gb1] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[udf_percentile] (batchId=146)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union25] (batchId=148)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_null] (batchId=154)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_between_in] (batchId=143)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_cast_constant] (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_mapjoin_reduce] (batchId=153)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_0] (batchId=154)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_3] (batchId=152)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_div0] (batchId=148)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_short_regress] (batchId=139)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_ptf] (batchId=146)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[windowing] (batchId=141)
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1 (batchId=219)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=223)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0] (batchId=182)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0] (batchId=182)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1] (batchId=182)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching (batchId=229)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6068/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6068/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6068/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 129 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877663 - PreCommit-HIVE-Build;;;","17/Jul/17 22:59;vgumashta;[~cdrome] FYI, with the 2.2 patch, the test case added in the patch itself fails. The diffs b/w previous 2 runs (run1 with patch and run2 w/o):
Failures in Run1 and not in Run2:
{code}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[auto_sortmerge_join_12] (batchId=101)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testCreateDb (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testCreateTable (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testCtas (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testExim (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testExternalTable (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertDualDynamicPartitions (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertNonPartTable (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertSingleDynamicPartition (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertStaticDualPartition (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testInsertStaticSinglePartition (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testLoad (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testLoadLocal (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testPartition (batchId=233)
org.apache.hadoop.hive.ql.security.TestInheritPermsExtendedAcls.testTruncateTable (batchId=233)
{code}

Failures in Run2 but not in Run1:
{code}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=44)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_12] (batchId=46)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_13] (batchId=23)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=236)
{code}

Run2 was thru a dummy patch which just modified a comment on 2.2 line.;;;","17/Jul/17 23:14;cdrome;[~vgumashta], thanks for checking on the tests. The tests that are failing are from HIVE-11481.

I've got an environment running where I can test branch-2.2, so I'll take a look at those tests.;;;","17/Jul/17 23:18;vgumashta;Thanks [~cdrome];;;","25/Jul/17 20:57;cdrome;Uploaded a new patch for branch-2.2.

I've elected to remove the unittest that was introduced as part of HIVE-11481 because the assumptions are not correct when verifying permissions/ACLs.

Specifically, the comparison always checks for inheritance of ACLs from the parent directory even if the current directory was manually set to something that is inconsistent with the parent directory (which happens in most of the test cases).

Also, I don't feel that sub-classing FolderPermissionBase.java for inheritance types unittests is the right approach. The tests and code in FolderPermissionBase.java is not conducive to fine enough control of different types of inheritance situations.;;;","28/Jul/17 23:35;vgumashta;[~cdrome] Thanks for the patch. I have a couple of questions on the overall approach (doc I'm using for reference: https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control#permissions-on-new-files-and-folders).
1. It appears for child directories, HDFS should correctly transfer the default ACLs. However, I understand that in Hive we want to avoid the HDFS permissions umasking (the traditional file permissions and not ACLs). Would it make sense to first let HDFS create the child directory (so that it transfers the default/access ACLs) and then set the desired permissions?
2. This comment will be relevant if we decide to manage ACL transfer from parent to child: referring the above doc, it seems when transferring access ACLs, the rwx on other should be removed if it exist. We might need to consider that in the code.

;;;","31/Jul/17 20:18;cdrome;[~vgumashta], I'll review your comments and update accordingly.;;;","02/Aug/17 22:51;cdrome;[~vgumashta], I checked the behavior of hadoop-2.7 and hadoop-2.8, which matches what you describe about zeroing out the 'other' permissions.

My intention was to let HDFS create and manage the child directories where possible.
However, the reason for this patch was because early versions of ACL support in hadoop combined with the original treatment of ACLs in hive/hcat were generating incorrect results.

Let me revisit the patch and submit a new version.;;;","15/Aug/17 22:36;cdrome;[~vgumashta], I've done a bunch of testing and rewriting the unittests to ensure they are testing the correct things.

I've incorporated your comments about permissions on OTHER getting converted to none.

However, your first comment will not work. The problem is that data gets written to a temp directory relative to the table root and then moved to the final location. So the data in the temp directory will inherit permissions/acls from the table directory, which might be different from that of the destination.

{{FolderPermissionBase.testInsertSingleDynamicPartition}} tests this use case. Without the additional {{setfacl}} call after the move, the part file acls are in an inconsistent state relative to the parent (partition) directory.

I'm in the middle of cleaning things up, so I should have a new patch to review shortly.;;;","18/Aug/17 05:08;cdrome;Uploaded new version of branch-2.2 patch.;;;","18/Aug/17 08:22;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12882512/HIVE-13989.4-branch-2.2.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 58 failed/errored test(s), 9934 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=244)
TestJdbcDriver2 - did not produce a TEST-*.xml file (likely timed out) (batchId=225)
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=167)
	[acid_globallimit.q,alter_merge_2_orc.q]
TestMiniSparkOnYarnCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=173)
	[infer_bucket_sort_reducers_power_two.q,list_bucket_dml_10.q,orc_merge9.q,orc_merge6.q,leftsemijoin_mr.q,bucket6.q,bucketmapjoin7.q,uber_reduce.q,empty_dir_in_table.q,vector_outer_join3.q,index_bitmap_auto.q,vector_outer_join2.q,vector_outer_join1.q,orc_merge1.q,orc_merge_diff_fs.q,load_hdfs_file_with_space_in_the_name.q,scriptfile1_win.q,quotedid_smb.q,truncate_column_buckets.q,orc_merge3.q]
TestMiniSparkOnYarnCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=174)
	[infer_bucket_sort_num_buckets.q,gen_udf_example_add10.q,insert_overwrite_directory2.q,orc_merge5.q,bucketmapjoin6.q,import_exported_table.q,vector_outer_join0.q,orc_merge4.q,temp_table_external.q,orc_merge_incompat1.q,root_dir_external_table.q,constprog_semijoin.q,auto_sortmerge_join_16.q,schemeAuthority.q,index_bitmap3.q,external_table_with_space_in_location_path.q,parallel_orderby.q,infer_bucket_sort_map_operators.q,bucketizedhiveinputformat.q,remote_script.q]
TestMiniSparkOnYarnCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=175)
	[scriptfile1.q,vector_outer_join5.q,file_with_header_footer.q,bucket4.q,input16_cc.q,bucket5.q,infer_bucket_sort_merge.q,constprog_partitioner.q,orc_merge2.q,reduce_deduplicate.q,schemeAuthority2.q,load_fs2.q,orc_merge8.q,orc_merge_incompat2.q,infer_bucket_sort_bucketed_table.q,vector_outer_join4.q,disable_merge_for_bucketing.q,vector_inner_join.q,orc_merge7.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=118)
	[bucketmapjoin4.q,bucket_map_join_spark4.q,union21.q,groupby2_noskew.q,timestamp_2.q,date_join1.q,mergejoins.q,smb_mapjoin_11.q,auto_sortmerge_join_3.q,mapjoin_test_outer.q,vectorization_9.q,merge2.q,groupby6_noskew.q,auto_join_without_localtask.q,multi_join_union.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=119)
	[join_cond_pushdown_unqual4.q,union_remove_7.q,join13.q,join_vc.q,groupby_cube1.q,bucket_map_join_spark2.q,sample3.q,smb_mapjoin_19.q,stats16.q,union23.q,union.q,union31.q,cbo_udf_udaf.q,ptf_decimal.q,bucketmapjoin2.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=120)
	[parallel_join1.q,union27.q,union12.q,groupby7_map_multi_single_reducer.q,varchar_join1.q,join7.q,join_reorder4.q,skewjoinopt2.q,bucketsortoptimize_insert_2.q,smb_mapjoin_17.q,script_env_var1.q,groupby7_map.q,groupby3.q,bucketsortoptimize_insert_8.q,union20.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=121)
	[ptf_general_queries.q,auto_join_reordering_values.q,sample2.q,join1.q,decimal_join.q,mapjoin_subquery2.q,join32_lessSize.q,mapjoin1.q,order2.q,skewjoinopt18.q,union_remove_18.q,join25.q,groupby9.q,bucketsortoptimize_insert_6.q,ctas.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=122)
	[groupby_map_ppr.q,nullgroup4_multi_distinct.q,join_rc.q,union14.q,smb_mapjoin_12.q,vector_cast_constant.q,union_remove_4.q,auto_join11.q,load_dyn_part7.q,udaf_collect_set.q,vectorization_12.q,groupby_sort_skew_1.q,groupby_sort_skew_1_23.q,smb_mapjoin_25.q,skewjoinopt12.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=123)
	[skewjoinopt15.q,auto_join18.q,list_bucket_dml_2.q,input1_limit.q,load_dyn_part3.q,union_remove_14.q,auto_sortmerge_join_14.q,auto_sortmerge_join_15.q,union10.q,bucket_map_join_tez2.q,groupby5_map_skew.q,join_reorder.q,sample1.q,bucketmapjoin8.q,union34.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=124)
	[avro_joins.q,skewjoinopt16.q,auto_join14.q,vectorization_14.q,auto_join26.q,stats1.q,cbo_stats.q,auto_sortmerge_join_6.q,union22.q,union_remove_24.q,union_view.q,smb_mapjoin_22.q,stats15.q,ptf_matchpath.q,transform_ppr1.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=125)
	[limit_pushdown2.q,skewjoin_noskew.q,leftsemijoin_mr.q,bucket3.q,skewjoinopt13.q,bucketmapjoin9.q,auto_join15.q,ptf.q,join22.q,vectorized_nested_mapjoin.q,sample4.q,union18.q,multi_insert_gby.q,join33.q,join_cond_pushdown_unqual2.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=126)
	[vector_decimal_aggregate.q,ppd_join3.q,auto_join23.q,join10.q,union_remove_11.q,union_ppr.q,join32.q,groupby_multi_single_reducer2.q,input18.q,stats3.q,cbo_simple_select.q,parquet_join.q,join26.q,groupby1.q,join_reorder2.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=127)
	[skewjoinopt19.q,order.q,join_merge_multi_expressions.q,skewjoinopt10.q,insert_into1.q,vectorized_math_funcs.q,vectorization_4.q,vectorization_2.q,skewjoinopt6.q,union_remove_19.q,decimal_1_1.q,join14.q,outer_join_ppr.q,rcfile_bigdata.q,load_dyn_part10.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=128)
	[skewjoinopt3.q,smb_mapjoin_4.q,timestamp_comparison.q,union_remove_10.q,mapreduce2.q,bucketmapjoin_negative.q,udf_in_file.q,union5.q,auto_join12.q,skewjoin.q,vector_left_outer_join.q,semijoin.q,skewjoinopt9.q,smb_mapjoin_3.q,stats10.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=129)
	[bucketsortoptimize_insert_4.q,multi_insert_mixed.q,vectorization_10.q,auto_join18_multi_distinct.q,join_cond_pushdown_3.q,custom_input_output_format.q,skewjoinopt5.q,vectorization_part_project.q,vector_count_distinct.q,skewjoinopt4.q,count.q,parallel.q,union33.q,union_lateralview.q,nullgroup4.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=130)
	[skewjoin_union_remove_2.q,avro_decimal_native.q,skewjoinopt8.q,bucketmapjoin_negative3.q,stats6.q,groupby2_map.q,stats_only_null.q,insert_into3.q,join18_multi_distinct.q,vectorization_6.q,cross_join.q,stats9.q,auto_join7.q,timestamp_1.q,join24.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=131)
	[auto_join30.q,timestamp_null.q,union32.q,join16.q,groupby_ppr.q,bucketmapjoin7.q,smb_mapjoin_18.q,join19.q,vector_varchar_4.q,union6.q,cbo_subq_in.q,vectorization_part.q,sample8.q,vectorized_timestamp_funcs.q,join_star.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=132)
	[union_remove_1.q,ppd_outer_join2.q,date_udf.q,groupby1_noskew.q,join20.q,smb_mapjoin_13.q,groupby_rollup1.q,temp_table_gb1.q,vector_string_concat.q,smb_mapjoin_6.q,metadata_only_queries.q,auto_sortmerge_join_12.q,groupby_bigdata.q,groupby3_map_multi_distinct.q,innerjoin.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=133)
	[groupby_grouping_id2.q,input17.q,bucketmapjoin12.q,ppd_gby_join.q,auto_join10.q,ptf_rcfile.q,vector_elt.q,multi_insert.q,ppd_join5.q,ppd_join.q,join_filters_overlap.q,join_cond_pushdown_1.q,timestamp_3.q,load_dyn_part6.q,stats_noscan_2.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=134)
	[tez_joins_explain.q,vectorized_rcfile_columnar.q,transform2.q,cbo_semijoin.q,bucketmapjoin13.q,union_remove_6_subq.q,groupby2_map_multi_distinct.q,load_dyn_part9.q,multi_insert_gby2.q,vectorization_11.q,groupby_position.q,avro_compression_enabled_native.q,smb_mapjoin_8.q,join21.q,auto_join16.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=135)
	[enforce_order.q,smb_mapjoin_21.q,load_dyn_part15.q,udf_min.q,groupby_resolution.q,mapjoin_memcheck.q,subquery_exists.q,groupby5.q,join27.q,alter_merge_stats_orc.q,union_remove_2.q,vector_orderby_5.q,groupby6_map_skew.q,join12.q,union9.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=136)
	[vectorization_16.q,join_casesensitive.q,transform_ppr2.q,join23.q,groupby7_map_skew.q,ppd_join2.q,ppd_outer_join5.q,create_merge_compressed.q,louter_join_ppr.q,sample9.q,smb_mapjoin_16.q,vectorization_not.q,having.q,ppd_outer_join1.q,union_remove_12.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=137)
	[bucketmapjoin3.q,load_dyn_part5.q,union_date.q,cbo_gby.q,auto_join31.q,auto_sortmerge_join_1.q,join_cond_pushdown_unqual1.q,ppd_outer_join3.q,bucket_map_join_spark3.q,union28.q,statsfs.q,escape_sortby1.q,leftsemijoin.q,union_remove_6.q,join29.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=138)
	[escape_distributeby1.q,join9.q,groupby2.q,groupby4_map.q,udf_max.q,vectorization_pushdown.q,cbo_gby_empty.q,join_cond_pushdown_unqual3.q,vectorization_short_regress.q,join8.q,sample10.q,cross_product_check_1.q,auto_join_stats.q,input_part2.q,groupby_multi_single_reducer3.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=139)
	[groupby_map_ppr_multi_distinct.q,vectorization_13.q,mapjoin_mapjoin.q,union2.q,join41.q,groupby8_map.q,cbo_subq_not_in.q,identity_project_remove_skip.q,stats5.q,groupby8_map_skew.q,nullgroup2.q,mapjoin_subquery.q,bucket2.q,smb_mapjoin_1.q,union_remove_8.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=140)
	[join39.q,bucketsortoptimize_insert_7.q,vector_distinct_2.q,bucketmapjoin10.q,join11.q,union13.q,auto_sortmerge_join_16.q,windowing.q,union_remove_3.q,skewjoinopt7.q,stats7.q,annotate_stats_join.q,multi_insert_lateral_view.q,ptf_streaming.q,join_1to1.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=141)
	[timestamp_lazy.q,union29.q,runtime_skewjoin_mapjoin_spark.q,auto_join22.q,union8.q,groupby5_map.q,dynamic_rdd_cache.q,auto_join29.q,groupby6.q,merge1.q,mapjoin_distinct.q,vector_decimal_mapjoin.q,sample5.q,multi_insert_move_tasks_share_dependencies.q,join_array.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=142)
	[load_dyn_part2.q,smb_mapjoin_7.q,vectorization_5.q,smb_mapjoin_2.q,ppd_join_filter.q,column_access_stats.q,stats0.q,vector_between_in.q,vectorized_string_funcs.q,bucket_map_join_2.q,groupby4_map_skew.q,groupby_ppr_multi_distinct.q,temp_table_join1.q,vectorized_case.q,stats_noscan_1.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=143)
	[groupby4_noskew.q,groupby3_map_skew.q,join_cond_pushdown_2.q,union19.q,union24.q,union_remove_5.q,groupby7_noskew_multi_single_reducer.q,vectorization_1.q,index_auto_self_join.q,auto_smb_mapjoin_14.q,script_env_var2.q,pcr.q,auto_join_filters.q,join0.q,join37.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=144)
	[stats12.q,groupby4.q,union_top_level.q,stats2.q,groupby10.q,mapjoin_filter_on_outerjoin.q,auto_sortmerge_join_4.q,limit_partition_metadataonly.q,load_dyn_part4.q,union3.q,groupby_multi_single_reducer.q,smb_mapjoin_14.q,groupby3_noskew_multi_distinct.q,stats18.q,union_remove_21.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=145)
	[auto_sortmerge_join_13.q,join4.q,join35.q,udf_percentile.q,join_reorder3.q,subquery_in.q,auto_join19.q,stats14.q,vectorization_15.q,union7.q,vectorization_nested_udf.q,vector_groupby_3.q,vectorized_ptf.q,auto_join2.q,groupby1_map_skew.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=146)
	[groupby3_map.q,union26.q,mapreduce1.q,mapjoin_addjar.q,bucket_map_join_spark1.q,udf_example_add.q,multi_insert_with_join.q,sample7.q,auto_join_nulls.q,ppd_outer_join4.q,load_dyn_part8.q,alter_merge_orc.q,sample6.q,bucket_map_join_1.q,auto_sortmerge_join_9.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=147)
	[groupby_complex_types.q,multigroupby_singlemr.q,union11.q,groupby7.q,join5.q,bucketmapjoin_negative2.q,vectorization_div0.q,union_script.q,add_part_multiple.q,limit_pushdown.q,union_remove_17.q,uniquejoin.q,metadata_only_queries_with_filters.q,union25.q,load_dyn_part13.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=148)
	[table_access_keys_stats.q,bucketmapjoin11.q,auto_join4.q,mapjoin_decimal.q,join34.q,nullgroup.q,mergejoins_mixed.q,sort.q,stats8.q,auto_join28.q,join17.q,union17.q,skewjoinopt11.q,groupby1_map.q,load_dyn_part11.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=149)
	[ptf_seqfile.q,union_remove_23.q,parallel_join0.q,union_remove_9.q,join_nullsafe.q,skewjoinopt14.q,vectorized_mapjoin.q,union4.q,auto_join5.q,vectorized_shufflejoin.q,smb_mapjoin_20.q,groupby8_noskew.q,auto_sortmerge_join_10.q,groupby11.q,union_remove_16.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=150)
	[smb_mapjoin_15.q,script_pipe.q,auto_join24.q,filter_join_breaktask.q,bucket4.q,ppd_multi_insert.q,skewjoinopt20.q,join_thrift.q,multi_insert_gby3.q,groupby8.q,join_map_ppr.q,auto_sortmerge_join_8.q,escape_clusterby1.q,groupby_multi_insert_common_distinct.q,join6.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=151)
	[ppd_transform.q,auto_join9.q,auto_join1.q,vector_data_types.q,input13.q,input14.q,input12.q,union_remove_22.q,vectorization_3.q,groupby1_map_nomap.q,cbo_union.q,disable_merge_for_bucketing.q,reduce_deduplicate_exclude_join.q,filter_join_breaktask2.q,join30.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=152)
	[router_join_ppr.q,auto_join13.q,union30.q,vector_mapjoin_reduce.q,ptf_register_tblfn.q,join_merging.q,union_date_trim.q,groupby3_noskew.q,optimize_nullscan.q,join3.q,join38.q,skewjoinopt1.q,join_alt_syntax.q,groupby_sort_1_23.q,timestamp_udf.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=153)
	[groupby6_map.q,stats13.q,groupby2_noskew_multi_distinct.q,load_dyn_part12.q,join15.q,auto_join17.q,join_hive_626.q,tez_join_tests.q,auto_join21.q,join_view.q,join_cond_pushdown_4.q,vectorization_0.q,union_null.q,auto_join3.q,vectorization_decimal_date.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=154)
	[union_remove_15.q,bucket_map_join_tez1.q,scriptfile1.q,groupby7_noskew.q,bucketmapjoin1.q,subquery_multiinsert.q,auto_join8.q,auto_join6.q,groupby2_map_skew.q,lateral_view_explode2.q,join28.q,load_dyn_part1.q,skewjoinopt17.q,union_remove_20.q,bucketmapjoin5.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=155)
	[join2.q,join36.q,avro_joins_native.q,join18.q,smb_mapjoin_10.q,temp_table.q,union_remove_13.q,auto_sortmerge_join_5.q,groupby5_noskew.q,auto_join0.q,vectorization_17.q,auto_join_stats2.q,skewjoin_union_remove_1.q,union16.q,join_literals.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=156)
	[auto_sortmerge_join_7.q,auto_join20.q,smb_mapjoin_5.q,vector_char_4.q,cross_product_check_2.q,union15.q,union_remove_25.q,insert_into2.q,join31.q,auto_join27.q,escape_orderby1.q,cbo_limit.q,stats_partscan_1_23.q,groupby_complex_types_multi_single_reducer.q,load_dyn_part14.q]
TestSparkNegativeCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=242)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_globallimit] (batchId=27)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[avrocountemptytbl] (batchId=74)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[columnStatsUpdateForStatsOptimizer_1] (batchId=31)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[index_compact_binary_search] (batchId=55)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[selectindate] (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_fast_stats] (batchId=47)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=94)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=109)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=222)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0] (batchId=181)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0] (batchId=181)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1] (batchId=181)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6452/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6452/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6452/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 58 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12882512 - PreCommit-HIVE-Build;;;","18/Aug/17 22:57;cdrome;For the tests that failed (as opposed to those that timed out), I reran on our dev hardware.
I wanted to see if the failure was reproducable and if it also failed at the 2.2.1 fork point.

|| Test || branch-2.2.1 fork (1ed1f28) || branch-2.2 HEAD + HIVE-13989 ||
| org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_globallimit] | FAILED | FAILED |
| org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[avrocountemptytbl] | PASSED | PASSED |
| org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[columnStatsUpdateForStatsOptimizer_1] | FAILED | FAILED |
| org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[index_compact_binary_search] | PASSED | PASSED |
| org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[selectindate] | PASSED | PASSED |
| org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_fast_stats] | FAILED | FAILED |
| org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] | FAILED | FAILED |
| org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] | PASSED | PASSED |
| org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel | PASSED | PASSED |
| org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0] | PASSED | PASSED |
| org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0] | PASSED | PASSED |
| org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1] | PASSED | PASSED |

Based on this, HIVE-13989 doesn't appear to be responsible for any of these failures.;;;","24/Aug/17 06:38;cdrome;Uploaded branch-2 patch.;;;","24/Aug/17 06:46;cdrome;Uploaded rebased patch for branch-2.;;;","24/Aug/17 16:25;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12883493/HIVE-13989.4-branch-2.patch

{color:green}SUCCESS:{color} +1 due to 3 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10606 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[comments] (batchId=35)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[explaindenpendencydiffengs] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=144)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[explaindenpendencydiffengs] (batchId=115)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_ptf] (batchId=125)
org.apache.hive.hcatalog.api.TestHCatClient.testTransportFailure (batchId=176)
org.apache.hive.jdbc.TestJdbcDriver2.testYarnATSGuid (batchId=222)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6520/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6520/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6520/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12883493 - PreCommit-HIVE-Build;;;","01/Sep/17 13:35;vgumashta;+1

[~cdrome] I wasn't able to check the test report in time - did you get a chance to look at them (they don't seem related though). ;;;","06/Sep/17 04:32;cdrome;[~vgumashta], I checked all of the failures on the branch-2.2 build.
See https://issues.apache.org/jira/browse/HIVE-13989?focusedCommentId=16133742&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16133742 for my comments on that build.

The test results for the branch-2 build are not available anymore.
Shall I submit another patch and rerun?;;;","06/Sep/17 22:46;cdrome;Ran tests locally before and after the patch on branch-2 and none of the failures appear to be attributable to the patch:

|| Test ||  branch-2 HEAD (b3a6e52) || branch-2 HEAD + HIVE-13989 ||
| org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[comments] | PASSED | PASSED |
| org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[explaindenpendencydiffengs] | FAILED | FAILED |
| org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] | FAILED | FAILED |
| org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] | FAILED | FAILED |
| org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] | PASSED | PASSED |
| org.apache.hive.hcatalog.api.TestHCatClient.testTransportFailure | FAILED | FAILED |
| org.apache.hive.jdbc.TestJdbcDriver2.testYarnATSGuid | PASSED | PASSED |;;;","07/Sep/17 18:03;vgumashta;Thanks for the analysis [~cdrome]. Will commit this shortly;;;","09/Sep/17 19:24;vgumashta;Committed to branch-2.2 and branch-2. Thanks a lot [~cdrome]!;;;","18/Sep/17 04:52;leftyl;[~vgumashta], this jira is marked as fixed in 2.3.0 and 2.2.1 but you committed it to branch-2 and branch-2.2 so I think it should say fixed in 2.4.0 and 2.2.1.;;;"
JDBC: fix Statement.cancel,HIVE-13456,12957001,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,07/Apr/16 20:33,26/May/17 13:56,28/Nov/24 15:58,04/Apr/17 20:59,1.2.1,2.0.0,,,,,,,,HiveServer2,JDBC,,0,,,"JDBC Statement.cancel is supposed to work by cancelling the underlying execution and freeing resources. However, in my testing, I see it failing in some runs for the same query.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 13:56:11 UTC 2017,,,,,,,,,,"0|i2vt2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/17 20:59;vgumashta;HIVE-16172 fixes this.;;;","26/May/17 13:56;jdbcworries;Hi Vaibhav, I'm interested in this issue, can you tell me what causes/effects are?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive-1.2.1 compile spark-1.5.0 output error,HIVE-11795,12863276,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Won't Fix,tomzeng,rememberwen,rememberwen,11/Sep/15 03:07,22/Dec/15 17:05,28/Nov/24 15:58,06/Nov/15 20:33,1.2.1,2.0.0,,,,,1.2.1,,,,,10/Sep/15 00:00,1,,,"When input command: ""mvn clean install -Phadoop-2,dist -DskipTests -Dhadoop-23.version=2.7.1 -Dspark.version=1.5.0"" to compile hadoo-2.7.1 spark-1.5.0, system output an error, as follows:
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project spark-client: Compilation failure
[ERROR] /opt/modules/apache-hive-1.2.1-src/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[441,11] org.apache.hive.spark.client.RemoteDriver.ClientListener is not abstract and does not override abstract method onBlockUpdated(org.apache.spark.scheduler.SparkListenerBlockUpdated) in org.apache.spark.scheduler.SparkListener
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :spark-client","operation system: centos 6.5
softwares: maven 3.3.3; hive-1.2.1; hadoop-2.7.1; jdk_1.7.0_71
All these softwares in directory: /opt/modules/",,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/15 01:13;tomzeng;HIVE-11795.patch;https://issues.apache.org/jira/secure/attachment/12770924/HIVE-11795.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 06 20:34:08 UTC 2015,,,,,,,,,,"0|i2k18v:",9223372036854775807,This is resolved by HIVE-11473,,,,,,,,,,,,,1.2.1,2.0.0,,,,,,,,"06/Nov/15 09:00;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12770924/HIVE-11795.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5944/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5944/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5944/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
     [copy] Copying 14 files to /data/hive-ptest/working/apache-github-source-source/ant/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-ant ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-ant ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-ant ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/ant/target/hive-ant-2.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-ant ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-ant ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/ant/target/hive-ant-2.0.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-ant/2.0.0-SNAPSHOT/hive-ant-2.0.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/ant/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-ant/2.0.0-SNAPSHOT/hive-ant-2.0.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Llap Client 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-llap-client ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/llap-client/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/llap-client (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-llap-client ---
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-llap-client ---
[INFO] Source directory: /data/hive-ptest/working/apache-github-source-source/llap-client/src/gen/protobuf/gen-java added.
[INFO] Source directory: /data/hive-ptest/working/apache-github-source-source/llap-client/src/gen/thrift/gen-javabean added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-llap-client ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-llap-client ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/llap-client/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-llap-client ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-llap-client ---
[INFO] Compiling 9 source files to /data/hive-ptest/working/apache-github-source-source/llap-client/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-llap-client ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/llap-client/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-llap-client ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/llap-client/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/llap-client/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/llap-client/target/tmp/conf
     [copy] Copying 14 files to /data/hive-ptest/working/apache-github-source-source/llap-client/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-llap-client ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-llap-client ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-llap-client ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/llap-client/target/hive-llap-client-2.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-llap-client ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-llap-client ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/llap-client/target/hive-llap-client-2.0.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-llap-client/2.0.0-SNAPSHOT/hive-llap-client-2.0.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/llap-client/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-llap-client/2.0.0-SNAPSHOT/hive-llap-client-2.0.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Remote Client 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-client ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/spark-client/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/spark-client (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ spark-client ---
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ spark-client ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ spark-client ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ spark-client ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ spark-client ---
[INFO] Compiling 28 source files to /data/hive-ptest/working/apache-github-source-source/spark-client/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientUtilities.java: /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientUtilities.java uses or overrides a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientUtilities.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcDispatcher.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcDispatcher.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[66,34] cannot find symbol
  symbol:   class SparkListenerBlockUpdated
  location: package org.apache.spark.scheduler
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[538,32] cannot find symbol
  symbol:   class SparkListenerBlockUpdated
  location: class org.apache.hive.spark.client.RemoteDriver.ClientListener
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[66,34] cannot find symbol
  symbol:   class SparkListenerBlockUpdated
  location: package org.apache.spark.scheduler
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[538,32] cannot find symbol
  symbol:   class SparkListenerBlockUpdated
  location: class org.apache.hive.spark.client.RemoteDriver.ClientListener
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[537,5] method does not override or implement a method from a supertype
[INFO] 5 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [10.133s]
[INFO] Hive Shims Common ................................. SUCCESS [19.671s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [9.987s]
[INFO] Hive Shims Scheduler .............................. SUCCESS [3.813s]
[INFO] Hive Shims ........................................ SUCCESS [3.073s]
[INFO] Hive Storage API .................................. SUCCESS [4.556s]
[INFO] Hive Common ....................................... SUCCESS [25.015s]
[INFO] Hive Serde ........................................ SUCCESS [16.459s]
[INFO] Hive Metastore .................................... SUCCESS [51.599s]
[INFO] Hive Ant Utilities ................................ SUCCESS [2.035s]
[INFO] Hive Llap Client .................................. SUCCESS [10.731s]
[INFO] Spark Remote Client ............................... FAILURE [17.222s]
[INFO] Hive Query Language ............................... SKIPPED
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive Accumulo Handler ............................. SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HPL/SQL ...................................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Llap Server .................................. SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:57.586s
[INFO] Finished at: Fri Nov 06 04:00:23 EST 2015
[INFO] Final Memory: 102M/476M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project spark-client: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[66,34] cannot find symbol
[ERROR] symbol:   class SparkListenerBlockUpdated
[ERROR] location: package org.apache.spark.scheduler
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[538,32] cannot find symbol
[ERROR] symbol:   class SparkListenerBlockUpdated
[ERROR] location: class org.apache.hive.spark.client.RemoteDriver.ClientListener
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[66,34] cannot find symbol
[ERROR] symbol:   class SparkListenerBlockUpdated
[ERROR] location: package org.apache.spark.scheduler
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[538,32] cannot find symbol
[ERROR] symbol:   class SparkListenerBlockUpdated
[ERROR] location: class org.apache.hive.spark.client.RemoteDriver.ClientListener
[ERROR] /data/hive-ptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[537,5] method does not override or implement a method from a supertype
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :spark-client
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12770924 - PreCommit-HIVE-TRUNK-Build;;;","06/Nov/15 17:17;tomzeng;Looks the build server didn't have access to the Spark 1.5.0 libs.  This patch was tested on Mac OS and Linux, on master and 1.2 branch and against Spark 1.5.0 and 1.5.1.;;;","06/Nov/15 18:27;cos;[~tomzeng], I believe this issue has been solved in HIVE-11473 already;;;","06/Nov/15 18:33;tomzeng;[~cos] two more overrides were needed since HIVE-11473 which is already in Hive 1.2.1.;;;","06/Nov/15 18:42;cos;I don't see HIVE-11473 in 1.2.1. It has been released in July, but Spark 1.5.1 wasn't until like August/September. So if you look at HIVE-11473 its patch was added around the later timeframe.;;;","06/Nov/15 19:08;tomzeng;That could be possible if branch-1.2 is more recent than the 1.2.1 release artifact.  But the error I saw when building BIGTOP it seemed that HIVE-11473 was already included, new error was not included in that patch.;;;","06/Nov/15 19:38;tomzeng;Just checked that HIVE-11473 was not included 1.2.1 and it does fix this issue as well and in a better way. We should resolve this this issue.;;;","06/Nov/15 19:38;cos;There's also a move from interface to a class extension. Anyway, it works properly as we can see in [this Bigtop CI run|https://issues.apache.org/jira/browse/BIGTOP-2114?focusedCommentId=14994258&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14994258];;;","06/Nov/15 19:55;cos;The resolution should be ""Won't fix"" as the issue has been fixed by a different ticket, but I leave it to Hive community to decide.;;;","06/Nov/15 20:34;tomzeng;Done;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Orc fast file merging/concatenation should be disabled for ACID tables,HIVE-12451,12913943,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,prasanth_j,prasanth_j,18/Nov/15 01:16,27/Feb/24 22:23,28/Nov/24 15:58,09/Jan/19 23:58,1.3.0,2.0.0,,,,,3.0.0,,,ORC,Transactions,,0,,,"For ACID tables merging of small files should happen only through compaction. We should disable ""alter table .. concatenate"" for ACID tables. We should also disable ConditionalMergeFileTask if destination is an ACID table.",,,,,,,,,,,,,,,,,,,,,,,HIVE-18288,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 25 16:29:29 UTC 2016,,,,,,,,,,"0|i2ojt3:",9223372036854775807,Concatenate is now supported on Acid tables,,,,,,,,,,,,,1.3.0,3.0.0,,,,,,,,"18/Nov/15 01:18;prasanth_j;[~ekoifman] fyi..;;;","25/May/16 16:29;jcamacho;Removing 2.1.0 target. Please feel free to commit to branch-2.1 anyway and fix for 2.1.0 if this happens before the release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid expensive call to contains/containsAll in DefaultGraphWalker,HIVE-11847,12873065,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Won't Fix,jcamacho,jcamacho,,16/Sep/15 18:10,27/Feb/24 22:13,28/Nov/24 15:58,22/Sep/15 14:50,1.3.0,2.0.0,,,,,,,,Logical Optimizer,Physical Optimizer,,0,,,Continuing work started in HIVE-11652.,,,,,,,,,,,,,,,HIVE-11315,,,HIVE-11652,,,,,,,,,"16/Sep/15 18:14;HIVE-11847.patch;https://issues.apache.org/jira/secure/attachment/12756315/HIVE-11847.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 17 14:33:50 UTC 2015,,,,,,,,,,"0|i2k9jb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/15 14:33;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12756315/HIVE-11847.patch

{color:red}ERROR:{color} -1 due to 80 failed/errored test(s), 9448 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join27
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_stats
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join_stats2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucket_map_join_spark4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_cross_product_check_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cross_product_check_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_rdd_cache
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join28
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join33
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join42
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join43
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_star
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_subquery2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multi_join_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_outer_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_repeated_alias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_runtime_skewjoin_mapjoin_spark
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin_mapjoin9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_exists_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_in_having
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_views
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_join_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_left_outer_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_leftsemi_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_outer_join5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_context
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_nested_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join_filters
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_constprog_dpp
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_group_by
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_join_filters
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join27
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join_filters
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_dynamic_rdd_cache
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join19
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_outer_join4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_25
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5310/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5310/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5310/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 80 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12756315 - PreCommit-HIVE-TRUNK-Build;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Metrics for the number of Hive operations waiting for compile,HIVE-13813,12971676,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,csun,csun,csun,20/May/16 21:54,21/Jul/17 17:45,28/Nov/24 15:58,08/Aug/16 20:29,1.3.0,2.0.0,,,,,2.3.0,,,Query Processor,,,0,,,"Currently, without {{hive.driver.parallel.compilation}} introduced in HIVE-4239, only one SQL operation can enter the compilation block per HS2 instance, and all the rest will be blocked. We should add metrics info for the number of operations that are blocked.",,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/16 00:04;csun;HIVE-13183.1.patch;https://issues.apache.org/jira/secure/attachment/12820606/HIVE-13183.1.patch","08/Aug/16 17:32;csun;HIVE-13183.2.patch;https://issues.apache.org/jira/secure/attachment/12822614/HIVE-13183.2.patch","20/May/16 23:01;csun;HIVE-13183.patch;https://issues.apache.org/jira/secure/attachment/12805337/HIVE-13183.patch",,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 08 20:29:26 UTC 2016,,,,,,,,,,"0|i2yawn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/May/16 23:05;csun;cc [~jxiang] and [~xuefuz].;;;","20/May/16 23:11;xuefuz;Patch looks good to me, but it's good if [~jxiang] can also take a look.;;;","21/May/16 00:23;jxiang;Looks good to me too. +1;;;","23/May/16 08:13;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12805337/HIVE-13183.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 87 failed/errored test(s), 9950 tests executed
*Failed tests:*
{noformat}
TestHWISessionManager - did not produce a TEST-*.xml file
TestMiniLlapCliDriver - did not produce a TEST-*.xml file
TestMiniTezCliDriver-auto_join30.q-vector_decimal_10_0.q-acid_globallimit.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-constprog_dpp.q-dynamic_partition_pruning.q-vectorization_10.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-groupby2.q-tez_dynpart_hashjoin_1.q-custom_input_output_format.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-schema_evol_text_nonvec_mapwork_table.q-vector_decimal_trailing.q-subquery_in.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-vectorization_13.q-auto_sortmerge_join_13.q-tez_bmj_schema_evolution.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-bucketmapjoin3.q-enforce_order.q-union11.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-ptf_rcfile.q-bucketmapjoin_negative.q-bucket_map_join_spark2.q-and-12-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ivyDownload
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_queries
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_bucket4
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_bucket6
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_constprog_partitioner
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_disable_merge_for_bucketing
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_infer_bucket_sort_map_operators
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_infer_bucket_sort_num_buckets
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_list_bucket_dml_10
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_orc_merge1
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_orc_merge2
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_orc_merge9
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_orc_merge_diff_fs
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_reduce_deduplicate
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join1
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join2
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join3
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join4
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.org.apache.hadoop.hive.cli.TestMiniTezCliDriver
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_stats
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert_orig_table
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf_matchpath
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_orc_acid_mapwork_table
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_orc_vec_mapwork_part_all_complex
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_selectDistinctStar
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union_decimal
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_update_all_types
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_complex_all
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_groupby_3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_interval_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_0
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_gby
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_limit
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby3_map_skew
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_multi_single_reducer3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats18
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_mapjoin_reduce
org.apache.hadoop.hive.llap.tez.TestConverters.testFragmentSpecToTaskSpec
org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure
org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInRemote.org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInRemote
org.apache.hadoop.hive.metastore.TestFilterHooks.org.apache.hadoop.hive.metastore.TestFilterHooks
org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs
org.apache.hadoop.hive.metastore.TestHiveMetaStoreStatsMerge.testStatsMerge
org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.testMetaStoreAuthorization
org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.testEndFunctionListener
org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.testEventStatus
org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.testMetaStoreInitListener
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics
org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.testAppendPartitionWithCommas
org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.testAppendPartitionWithValidCharacters
org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler
org.apache.hadoop.hive.metastore.hbase.TestHBaseImport.org.apache.hadoop.hive.metastore.hbase.TestHBaseImport
org.apache.hadoop.hive.ql.TestTxnCommands.testSimpleAcidInsert
org.apache.hadoop.hive.ql.security.TestExtendedAcls.org.apache.hadoop.hive.ql.security.TestExtendedAcls
org.apache.hadoop.hive.ql.security.TestFolderPermissions.org.apache.hadoop.hive.ql.security.TestFolderPermissions
org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.testSimplePrivileges
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropPartition
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.testSimplePrivileges
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.testSimplePrivileges
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadDbSuccess
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadTableFailure
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testSaslWithHiveMetaStore
org.apache.hive.hcatalog.api.repl.commands.TestCommands.org.apache.hive.hcatalog.api.repl.commands.TestCommands
org.apache.hive.hcatalog.listener.TestDbNotificationListener.dropDatabase
org.apache.hive.minikdc.TestJdbcWithDBTokenStore.org.apache.hive.minikdc.TestJdbcWithDBTokenStore
org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.org.apache.hive.service.TestHS2ImpersonationWithRemoteMS
org.apache.hive.spark.client.TestSparkClient.testSyncRpc
{noformat}

Test results: http://ec2-54-177-240-2.us-west-1.compute.amazonaws.com/job/PreCommit-HIVE-MASTER-Build/363/testReport
Console output: http://ec2-54-177-240-2.us-west-1.compute.amazonaws.com/job/PreCommit-HIVE-MASTER-Build/363/console
Test logs: http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-363/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 87 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12805337 - PreCommit-HIVE-MASTER-Build;;;","28/Jul/16 00:04;csun;Oops forgot about this. Re-attaching patch to test again.;;;","30/Jul/16 00:53;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12820606/HIVE-13183.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 187 failed/errored test(s), 10382 tests executed
*Failed tests:*
{noformat}
TestMsgBusConnection - did not produce a TEST-*.xml file
TestPrivilegesV2 - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_nullable_union
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketcontext_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketizedhiveinputformat_auto
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_auto_join1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_filters
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nulls
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_join_partition_key
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_smb_mapjoin_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sort_merge_join_desc_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_multiinsert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_bucketmapjoin1
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_cte_5
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_cte_mat_4
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_cte_mat_5
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_hybridgrace_hashjoin_1
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_orc_llap
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_tez_dynpart_hashjoin_1
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_tez_join_hash
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_tez_join_tests
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_tez_joins_explain
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_tez_self_join
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_tez_smb_main
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_tez_vector_dynpart_hashjoin_1
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vector_join_part_col_char
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_infer_bucket_sort_map_operators
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_quotedid_smb
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join21
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join29
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join30
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join_filters
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_join_nulls
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_10
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_11
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_12
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_13
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_14
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_bucket_map_join_tez1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_gby
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_limit
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_semijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_simple_select
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_subq_exists
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_subq_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_subq_not_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_views
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_constprog_semijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cte_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cte_mat_4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cte_mat_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_filter_join_breaktask2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_hybridgrace_hashjoin_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_hybridgrace_hashjoin_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join0
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_leftsemijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_lvj_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mergejoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mrr
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf_streaming
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_selectDistinctStar
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_exists
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_bmj_schema_evolution
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_dynpart_hashjoin_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_hash
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_tests
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_joins_explain
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_self_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_smb_empty
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_smb_main
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_vector_dynpart_hashjoin_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_unionDistinct_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_join30
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_join_filters
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_join_nulls
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_join_part_col_char
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_left_outer_join2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_leftsemi_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_mapjoin_reduce
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_nullsafe_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_outer_join5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_part
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_part_varchar
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_bucketmapjoin1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_shufflejoin
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_map_operators
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_quotedid_smb
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_avro_non_nullable_union
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_13
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_16
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_sortmerge_join_9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_nullsafe
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_parquet_join
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_12
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_16
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_17
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_3
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_4
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_6
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_7
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_smb_mapjoin_8
org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.stringifyValidTxns
org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.testTxnRange
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/689/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/689/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-689/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 187 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12820606 - PreCommit-HIVE-MASTER-Build;;;","08/Aug/16 17:32;csun;Re-attach to kick off another test.;;;","08/Aug/16 20:07;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12822614/HIVE-13183.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 10441 tests executed
*Failed tests:*
{noformat}
TestMsgBusConnection - did not produce a TEST-*.xml file
TestQueryLifeTimeHook - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_orc_llap_counters
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/815/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/815/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-815/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12822614 - PreCommit-HIVE-MASTER-Build;;;","08/Aug/16 20:29;csun;Pushed to master branch. Thanks Jimmy and Xuefu for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FS based stats collection generates wrong results for tez (for union queries),HIVE-11863,12873211,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,pxiong,prasanth_j,prasanth_j,17/Sep/15 07:01,21/Jul/17 17:45,28/Nov/24 15:58,16/Jul/16 19:12,1.3.0,2.0.0,,,,,2.3.0,,,,,,0,,,"FS based stats collection is the default way to collect stats. However, there are some cases (involving unions) where it generates wrong results. Refer test case in HIVE-11860 and compare test cli driver results against tez results. Also it will be good to extend statsfs.q test case with union queries.",,,,,,,,,,,HIVE-11860,,,HIVE-14236,,,,HIVE-12065,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 16 19:12:40 UTC 2016,,,,,,,,,,"0|i2kafj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/15 07:03;prasanth_j;[~ashutoshc] Is this a known issue?;;;","17/Sep/15 07:58;ashutoshc;I am not aware of any such issue. Does this happen only on Tez and not on MR ?;;;","17/Sep/15 19:53;prasanth_j;[~ashutoshc] Yes. It happens only in Tez. I just checked MR and it works fine.;;;","28/Dec/15 23:49;pxiong;to repro, set engine as tez
{code}
create table final_temp as select * from src union all select * from src1;
desc extended final_temp;
{code}
the results are
{code}
numRows=0, rawDataSize=0
{code};;;","16/Jul/16 18:51;ashutoshc;[~pxiong] Can this be closed as dupe of HIVE-14236 ?;;;","16/Jul/16 19:12;pxiong;duplicate of HIVE-14236;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tune TxnHandler.performTimeOuts() batch size,HIVE-13335,12952662,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ekoifman,ekoifman,ekoifman,23/Mar/16 01:25,21/Jul/17 17:45,28/Nov/24 15:58,01/Mar/17 19:28,1.3.0,2.0.0,,,,,2.3.0,,,Transactions,,,0,,,"look for usages - it's no longer useful; in fact may be a perf hit
made obsolete by HIVE-12439",,,,,,,,,,,,,,,,,,,,,HIVE-16109,HIVE-12439,,,,,"28/Feb/17 23:20;ekoifman;HIVE-13335.01.patch;https://issues.apache.org/jira/secure/attachment/12855248/HIVE-13335.01.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 06:46:15 UTC 2017,,,,,,,,,,"0|i2v2bz:",9223372036854775807,,,,,,,,,,,,,,2.2.0,,,,,,,,,"28/Feb/17 23:53;ekoifman;on 2nd thought it's better to keep this to safeguard against extremely large number of Hive transactions to abort (and keep DB undo log reasonable) but increase the batch size so that we run fewer queries;;;","01/Mar/17 02:26;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12855248/HIVE-13335.01.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10256 tests executed
*Failed tests:*
{noformat}
TestCommandProcessorFactory - did not produce a TEST-*.xml file (likely timed out) (batchId=271)
TestDbTxnManager - did not produce a TEST-*.xml file (likely timed out) (batchId=271)
TestDummyTxnManager - did not produce a TEST-*.xml file (likely timed out) (batchId=271)
TestHiveInputSplitComparator - did not produce a TEST-*.xml file (likely timed out) (batchId=271)
TestIndexType - did not produce a TEST-*.xml file (likely timed out) (batchId=271)
TestSplitFilter - did not produce a TEST-*.xml file (likely timed out) (batchId=271)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_table] (batchId=147)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3853/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3853/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3853/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12855248 - PreCommit-HIVE-Build;;;","01/Mar/17 02:50;ekoifman;no related failures
[~wzheng], could you review please?;;;","01/Mar/17 18:28;wzheng;+1. May need to edit the jira subject to reflect the change;;;","01/Mar/17 19:28;ekoifman;committed to master https://github.com/apache/hive/commit/6dace60af4b6ab4d5200310a0ad94c4530c2bec3
thanks Wei for the review;;;","02/Mar/17 06:46;leftyl;[~ekoifman], the commit to master omitted the JIRA number.  Please update errata.txt for commit 6dace60af4b6ab4d5200310a0ad94c4530c2bec3.  Thanks.

Example of updating errata.txt:  HIVE-11876.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC Driver parsing error when reading principal from ZooKeeper,HIVE-11966,12896446,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,25/Sep/15 17:59,07/Jul/16 00:32,28/Nov/24 15:58,03/Nov/15 00:02,1.3.0,2.0.0,,,,,1.3.0,2.0.1,2.1.0,JDBC,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/15 18:02;vgumashta;HIVE-11966.1.patch;https://issues.apache.org/jira/secure/attachment/12762419/HIVE-11966.1.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 07 00:32:18 UTC 2016,,,,,,,,,,"0|i2lkm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/15 19:54;thejas;+1;;;","28/Sep/15 02:44;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12762419/HIVE-11966.1.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 9620 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_groupby_reduce
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_groupby_reduce
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5442/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5442/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5442/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12762419 - PreCommit-HIVE-TRUNK-Build;;;","03/Nov/15 00:02;gates;Patch committed.;;;","07/Jul/16 00:32;vgumashta;Committed to branch-1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add test case for HIVE-10592,HIVE-12472,12914647,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,prasanth_j,prasanth_j,prasanth_j,19/Nov/15 22:46,05/Apr/16 18:16,28/Nov/24 15:58,20/Nov/15 23:38,1.3.0,2.0.0,,,,,1.3.0,2.0.0,,,,,0,,,"HIVE-10592 has a fix for the following NPE issue (table should have all columns values as null for timestamp and date columns)
{code:title=query}
set hive.optimize.index.filter=true;
select count(*) from orctable where timestamp_col is null;
select count(*) from orctable where date_col is null;
{code}
{code:title=exception}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl$TimestampStatisticsImpl.getMinimum(ColumnStatisticsImpl.java:845)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getMin(RecordReaderImpl.java:308)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicateProto(RecordReaderImpl.java:332)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$SargApplier.pickRowGroups(RecordReaderImpl.java:710)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:751)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:777)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:205)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:598)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1235)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1117)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:249)
	... 26 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:1, Vertex vertex_1446768202865_0008_5_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/15 23:05;prasanth_j;HIVE-12472.patch;https://issues.apache.org/jira/secure/attachment/12773368/HIVE-12472.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 20 23:38:37 UTC 2015,,,,,,,,,,"0|i2oo53:",9223372036854775807,,,,,,,,,,,,,,1.3.0,2.0.0,,,,,,,,"19/Nov/15 23:06;prasanth_j;[~ashutoshc] Can you take a look at this one? This just adds a test case to already fixed bug.;;;","19/Nov/15 23:07;prasanth_j;I don't think we need a full precommit test run for this.;;;","20/Nov/15 23:35;ashutoshc;+1;;;","20/Nov/15 23:38;prasanth_j;Committed to branch-1 and master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check SessionState status before performing cleanup,HIVE-12453,12914011,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Invalid,wzheng,wzheng,wzheng,18/Nov/15 05:57,18/Nov/15 23:03,28/Nov/24 15:58,18/Nov/15 23:03,1.3.0,2.0.0,,,,,,,,Hive,,,0,,,,,,,,,,,,,,,,,,,,,HIVE-12266,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2015-11-18 05:57:40.0,,,,,,,,,,"0|i2ok87:",9223372036854775807,,,,,,,,,,,,,,1.3.0,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Unable to create tables using ""STORED AS""",HIVE-13023,12937631,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Cannot Reproduce,sershe,prasanth_j,prasanth_j,08/Feb/16 20:50,08/Feb/16 22:33,28/Nov/24 15:58,08/Feb/16 22:33,2.0.0,,,,,,,,,,,,0,,,"When testing the new RC for 2.0.0 release, I got the following exception when creating ORC table
{code}
hive> 
    > create table src_orc(k string, v int) stored as orc;
Exception in thread ""b3a2d83b-bdc2-46f4-82c0-eb79d59590d9 b3a2d83b-bdc2-46f4-82c0-eb79d59590d9 main"" java.lang.AssertionError: Unknown token: [@-1,0:0='TOK_FILEFORMAT_GENERIC',<715>,0:-1]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:10875)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:9989)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10093)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:229)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:239)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:319)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1255)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1301)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1184)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1172)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:778)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:717)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:645)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 08 22:33:47 UTC 2016,,,,,,,,,,"0|i2sktb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/16 22:33;sershe;This appears to be a packaging problem with RC1. I will double check RC2 to see that it doesn't happen.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ship hive-storage-api along with hive-exec jar to all Tasks,HIVE-11423,12850502,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Duplicate,,gopalv,gopalv,31/Jul/15 04:12,01/Aug/15 01:30,28/Nov/24 15:58,01/Aug/15 01:30,2.0.0,,,,,,,,,File Formats,,,0,,,"After moving critical classes into hive-storage-api, those classes are needed for queries to execute successfully.

Currently all queries run fail with ClassNotFound exceptions on a large cluster.

{code}
Caused by: java.lang.NoClassDefFoundError: Lorg/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatch;
        at java.lang.Class.getDeclaredFields0(Native Method)
        at java.lang.Class.privateGetDeclaredFields(Class.java:2583)
        at java.lang.Class.getDeclaredFields(Class.java:1916)
        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:150)
        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:109)
        ... 57 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 62 more
{code}

Temporary workaround added to hiverc: {{add jar ./dist/hive/lib/hive-storage-api-2.0.0-SNAPSHOT.jar;}}",,,,,,,,,,,,,,,,HIVE-11425,,HIVE-11253,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 31 04:18:50 UTC 2015,,,,,,,,,,"0|i2i5yn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/15 04:18;xuefuz;FYI: this issue was found in Spark branch (HIVE-10863) and fix was included  in patch for HIVE-10166.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniTezCli tests are hanging,HIVE-11038,12838594,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,Done,wzheng,wzheng,wzheng,17/Jun/15 21:36,18/Jun/15 00:33,28/Nov/24 15:58,18/Jun/15 00:33,2.0.0,,,,,,,,,Hive,Tez,,0,,,"Whenever running a MiniTezCli test, it just hangs.

Here's the maven command to run a test:
{code}
$ mvn test -Phadoop-2 -Dtest=TestMiniTezCliDriver -Dqfile=dynamic_partition_pruning.q
{code}
Here's the tail of org.apache.hadoop.hive.cli.TestMiniTezCliDriver-output.txt:
{code}
Status: Running (Executing on YARN cluster with App id application_1434574617753_0001)

Map 1: -/-	Reducer 2: 0/1
Map 1: 1/1	Reducer 2: 1/1
POSTHOOK: query: analyze table lineitem compute statistics for columns
POSTHOOK: type: QUERY
POSTHOOK: Input: default@lineitem
POSTHOOK: Output: file:/Users/wzheng/bf/hive/itests/qtest/target/tmp/localscratchdir/c684ea6a-11b1-4253-a529-c3778695b72a/hive_2015-06-17_13-57-19_047_1275844087077606719-1/-mr-10000
OK
Time taken: 0.387 seconds
Begin query: dynamic_partition_pruning.q
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/Users/wzheng/bf/hive/conf/ivysettings.xml will be used
{code}
And here's the jstack output (partial):
{code}
""main"" #1 prio=5 os_prio=31 tid=0x00007fc75e805800 nid=0x1303 waiting on condition [0x0000000101d84000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.monitorExecution(TezJobMonitor.java:378)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:168)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1657)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1416)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1197)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1061)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1051)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1033)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1007)
	at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:146)
	at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynamic_partition_pruning(TestMiniTezCliDriver.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at junit.framework.TestCase.runTest(TestCase.java:176)
	at junit.framework.TestCase.runBare(TestCase.java:141)
	at junit.framework.TestResult$1.protect(TestResult.java:122)
	at junit.framework.TestResult.runProtected(TestResult.java:142)
	at junit.framework.TestResult.run(TestResult.java:125)
	at junit.framework.TestCase.run(TestCase.java:129)
	at junit.framework.TestSuite.runTest(TestSuite.java:255)
	at junit.framework.TestSuite.run(TestSuite.java:250)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

""VM Thread"" os_prio=31 tid=0x00007fc75e830800 nid=0x3103 runnable

""GC task thread#0 (ParallelGC)"" os_prio=31 tid=0x00007fc75e811800 nid=0x2103 runnable

""GC task thread#1 (ParallelGC)"" os_prio=31 tid=0x00007fc75f000000 nid=0x2303 runnable

""GC task thread#2 (ParallelGC)"" os_prio=31 tid=0x00007fc75f001000 nid=0x2503 runnable

""GC task thread#3 (ParallelGC)"" os_prio=31 tid=0x00007fc75f800000 nid=0x2703 runnable

""GC task thread#4 (ParallelGC)"" os_prio=31 tid=0x00007fc75f801000 nid=0x2903 runnable

""GC task thread#5 (ParallelGC)"" os_prio=31 tid=0x00007fc75f801800 nid=0x2b03 runnable

""GC task thread#6 (ParallelGC)"" os_prio=31 tid=0x00007fc75f802000 nid=0x2d03 runnable

""GC task thread#7 (ParallelGC)"" os_prio=31 tid=0x00007fc75f802800 nid=0x2f03 runnable

""VM Periodic Task Thread"" os_prio=31 tid=0x00007fc75c00c800 nid=0x5303 waiting on condition

JNI global references: 293
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 18 00:32:19 UTC 2015,,,,,,,,,,"0|i2g6bj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/15 00:32;wzheng;The issue is with running tez in local mode. Using the latest version tez 0.8.0-SNAPSHOT seems to solve the problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update/deletes on ACID table throws ArrayIndexOutOfBoundsException,HIVE-15756,13039001,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Not A Bug,ekoifman,kavansuresh@gmail.com,kavansuresh@gmail.com,30/Jan/17 21:01,18/Aug/17 11:03,28/Nov/24 15:58,21/Feb/17 18:14,2.0.0,,,,,,,,,Transactions,,,0,,,"Update and delete queries on ACID tables fail throwing ArrayIndexOutOfBoundsException.
{noformat}
hive> update customer_acid set c_comment = 'foo bar' where c_custkey % 100 = 1;
Query ID = cstm-hdfs_20170128005823_efa1cdb7-2ad2-4371-ac80-0e35868ad17c
Total jobs = 1
Launching Job 1 out of 1
Tez session was closed. Reopening...
Session re-established.


Status: Running (Executing on YARN cluster with App id application_1485331877667_0036)

--------------------------------------------------------------------------------
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
--------------------------------------------------------------------------------
Map 1 ..........   SUCCEEDED     14         14        0        0       0       0
Reducer 2             FAILED      1          0        0        1       1       0
--------------------------------------------------------------------------------
VERTICES: 01/02  [========================>>--] 93%   ELAPSED TIME: 23.68 s    
--------------------------------------------------------------------------------
Status: Failed
Vertex failed, vertexName=Reducer 2, vertexId=vertex_1485331877667_0036_1_01, diagnostics=[Task failed, taskId=task_1485331877667_0036_1_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":72,""bucketid"":1,""rowid"":0}},""value"":{""_col0"":103601,""_col1"":""Customer#000103601"",""_col2"":""3cYSrJtAA36vth35 emuIk"",""_col3"":20,""_col4"":""30-526-248-3190"",""_col5"":8047.21,""_col6"":""MACHINERY ""}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:173)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:347)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:194)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:185)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:185)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:181)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":72,""bucketid"":1,""rowid"":0}},""value"":{""_col0"":103601,""_col1"":""Customer#000103601"",""_col2"":""3cYSrJtAA36vth35 emuIk"",""_col3"":20,""_col4"":""30-526-248-3190"",""_col5"":8047.21,""_col6"":""MACHINERY ""}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:284)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:252)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:150)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":72,""bucketid"":1,""rowid"":0}},""value"":{""_col0"":103601,""_col1"":""Customer#000103601"",""_col2"":""3cYSrJtAA36vth35 emuIk"",""_col3"":20,""_col4"":""30-526-248-3190"",""_col5"":8047.21,""_col6"":""MACHINERY ""}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:352)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:274)
	... 16 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:780)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:838)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:343)
	... 17 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1485331877667_0036_1_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]
DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 2, vertexId=vertex_1485331877667_0036_1_01, diagnostics=[Task failed, taskId=task_1485331877667_0036_1_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":72,""bucketid"":1,""rowid"":0}},""value"":{""_col0"":103601,""_col1"":""Customer#000103601"",""_col2"":""3cYSrJtAA36vth35 emuIk"",""_col3"":20,""_col4"":""30-526-248-3190"",""_col5"":8047.21,""_col6"":""MACHINERY ""}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:173)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:347)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:194)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:185)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:185)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:181)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":72,""bucketid"":1,""rowid"":0}},""value"":{""_col0"":103601,""_col1"":""Customer#000103601"",""_col2"":""3cYSrJtAA36vth35 emuIk"",""_col3"":20,""_col4"":""30-526-248-3190"",""_col5"":8047.21,""_col6"":""MACHINERY ""}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:284)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:252)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:150)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":72,""bucketid"":1,""rowid"":0}},""value"":{""_col0"":103601,""_col1"":""Customer#000103601"",""_col2"":""3cYSrJtAA36vth35 emuIk"",""_col3"":20,""_col4"":""30-526-248-3190"",""_col5"":8047.21,""_col6"":""MACHINERY ""}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:352)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:274)
	... 16 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:780)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:838)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:343)
	... 17 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1485331877667_0036_1_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
{noformat}

{noformat}
hive> explain extended update customer_acid set c_comment = 'foo bar' where c_custkey % 100 = 1;
OK
ABSTRACT SYNTAX TREE:
  
TOK_UPDATE_TABLE
   TOK_TABNAME
      customer_acid
   TOK_SET_COLUMNS_CLAUSE
      =
         TOK_TABLE_OR_COL
            c_comment
         'foo bar'
   TOK_WHERE
      =
         %
            TOK_TABLE_OR_COL
               c_custkey
            100
         1


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
      DagId: cstm-hdfs_20170128012834_4d41e184-1e40-443c-9990-147cfdc6ea15:5
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
      DagName: 
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: customer_acid
                  filterExpr: ((c_custkey % 100) = 1) (type: boolean)
                  Statistics: Num rows: 25219 Data size: 8700894 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: ((c_custkey % 100) = 1) (type: boolean)
                    Statistics: Num rows: 12609 Data size: 4350274 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: ROW__ID (type: struct<transactionid:bigint,bucketid:int,rowid:bigint>), c_custkey (type: int), c_name (type: string), c_address (type: string), c_nationkey (type: int), c_phone (type: char(15)), c_acctbal (type: decimal(15,2)), c_mktsegment (type: char(10))
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                      Statistics: Num rows: 12609 Data size: 4350274 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: struct<transactionid:bigint,bucketid:int,rowid:bigint>)
                        sort order: +
                        Statistics: Num rows: 12609 Data size: 4350274 Basic stats: COMPLETE Column stats: NONE
                        tag: -1
                        value expressions: _col1 (type: int), _col2 (type: string), _col3 (type: string), _col4 (type: int), _col5 (type: char(15)), _col6 (type: decimal(15,2)), _col7 (type: char(10))
                        auto parallelism: true
            Path -> Alias:
              hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid [customer_acid]
            Path -> Partition:
              hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid 
                Partition
                  base file name: customer_acid
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    bucket_count 8
                    bucket_field_name c_custkey
                    columns c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment
                    columns.comments 
                    columns.types int:string:string:int:char(15):decimal(15,2):char(10):string
                    file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    location hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid
                    name tpch.customer_acid
                    numFiles 12
                    numRows 0
                    rawDataSize 0
                    serialization.ddl struct customer_acid { i32 c_custkey, string c_name, string c_address, i32 c_nationkey, char(15) c_phone, decimal(15,2) c_acctbal, char(10) c_mktsegment, string c_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 8700894
                    transactional true
                    transient_lastDdlTime 1485548417
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      bucket_count 8
                      bucket_field_name c_custkey
                      columns c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment
                      columns.comments 
                      columns.types int:string:string:int:char(15):decimal(15,2):char(10):string
                      file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                      file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                      location hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid
                      name tpch.customer_acid
                      numFiles 12
                      numRows 0
                      rawDataSize 0
                      serialization.ddl struct customer_acid { i32 c_custkey, string c_name, string c_address, i32 c_nationkey, char(15) c_phone, decimal(15,2) c_acctbal, char(10) c_mktsegment, string c_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 8700894
                      transactional true
                      transient_lastDdlTime 1485548417
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: tpch.customer_acid
                  name: tpch.customer_acid
            Truncated Path -> Alias:
              /tpch.db/customer_acid [customer_acid]
        Reducer 2 
            Needs Tagging: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: struct<transactionid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: int), VALUE._col1 (type: string), VALUE._col2 (type: string), VALUE._col3 (type: int), VALUE._col4 (type: char(15)), VALUE._col5 (type: decimal(15,2)), VALUE._col6 (type: char(10)), 'foo bar' (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 12609 Data size: 4350274 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  GlobalTableId: 1
                  directory: hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid/.hive-staging_hive_2017-01-28_01-28-34_547_5091220054599015088-1/-ext-10000
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 12609 Data size: 4350274 Basic stats: COMPLETE Column stats: NONE
                  Stats Publishing Key Prefix: hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid/.hive-staging_hive_2017-01-28_01-28-34_547_5091220054599015088-1/-ext-10000/
                  table:
                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                      properties:
                        bucket_count 8
                        bucket_field_name c_custkey
                        columns c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment
                        columns.comments 
                        columns.types int:string:string:int:char(15):decimal(15,2):char(10):string
                        file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                        file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                        location hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid
                        name tpch.customer_acid
                        numFiles 12
                        numRows 0
                        rawDataSize 0
                        serialization.ddl struct customer_acid { i32 c_custkey, string c_name, string c_address, i32 c_nationkey, char(15) c_phone, decimal(15,2) c_acctbal, char(10) c_mktsegment, string c_comment}
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                        totalSize 8700894
                        transactional true
                        transient_lastDdlTime 1485548417
                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      name: tpch.customer_acid
                  TotalFiles: 1
                  GatherStats: true
                  MultiFileSpray: false

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: false
          source: hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid/.hive-staging_hive_2017-01-28_01-28-34_547_5091220054599015088-1/-ext-10000
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 8
                bucket_field_name c_custkey
                columns c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment
                columns.comments 
                columns.types int:string:string:int:char(15):decimal(15,2):char(10):string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid
                name tpch.customer_acid
                numFiles 12
                numRows 0
                rawDataSize 0
                serialization.ddl struct customer_acid { i32 c_custkey, string c_name, string c_address, i32 c_nationkey, char(15) c_phone, decimal(15,2) c_acctbal, char(10) c_mktsegment, string c_comment}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                totalSize 8700894
                transactional true
                transient_lastDdlTime 1485548417
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: tpch.customer_acid

  Stage: Stage-3
    Stats-Aggr Operator
      Stats Aggregation Key Prefix: hdfs://hive-acid-upgrade-issue-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/customer_acid/.hive-staging_hive_2017-01-28_01-28-34_547_5091220054599015088-1/-ext-10000/

Time taken: 0.422 seconds, Fetched: 189 row(s)

{noformat}
",,,,,,,,,,,,,,,,,,HIVE-15844,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 11:03:07 UTC 2017,,,,,,,,,,"0|i39dxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/17 21:05;kavansuresh@gmail.com;Another occurrence of the issue:
{code:java}
0: jdbc:hive2://tesths2-merge-ks-1.openstackl> merge into lineitem using (select * from lineitem_stage WHERE L_ORDERKEY%2=1) sub on sub.L_ORDERKEY = lineitem.L_ORDERKEY when matched then delete;
INFO  : Session is already open
INFO  : Dag name: merge into lineitem using (select *...delete(Stage-1)
INFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896
INFO  : 

INFO  : Status: Running (Executing on YARN cluster with App id application_1485398058799_0109)

INFO  : Map 1: -/-	Map 3: 0/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0(+1)/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0(+2)/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0(+3)/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0(+4)/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0(+6)/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0(+7)/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0(+9)/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 0(+10)/10	Reducer 2: 0/2	
INFO  : Map 1: 0/5	Map 3: 1(+9)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+1)/5	Map 3: 1(+9)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+1)/5	Map 3: 1(+9)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+1)/5	Map 3: 2(+8)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+2)/5	Map 3: 2(+8)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+2)/5	Map 3: 2(+8)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+2)/5	Map 3: 2(+8)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+2)/5	Map 3: 2(+8)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+2)/5	Map 3: 2(+8)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+2)/5	Map 3: 4(+6)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+4)/5	Map 3: 4(+6)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+4)/5	Map 3: 6(+4)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+5)/5	Map 3: 6(+4)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+5)/5	Map 3: 7(+3)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+5)/5	Map 3: 8(+2)/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+5)/5	Map 3: 10/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+5)/5	Map 3: 10/10	Reducer 2: 0/2	
INFO  : Map 1: 0(+5)/5	Map 3: 10/10	Reducer 2: 0/2	
INFO  : Map 1: 1(+4)/5	Map 3: 10/10	Reducer 2: 0/2	
INFO  : Map 1: 2(+3)/5	Map 3: 10/10	Reducer 2: 0/2	
INFO  : Map 1: 2(+2)/5	Map 3: 10/10	Reducer 2: 0/2	
INFO  : Map 1: 3(+2)/5	Map 3: 10/10	Reducer 2: 0/2	
INFO  : Map 1: 3(+2)/5	Map 3: 10/10	Reducer 2: 0/2	
INFO  : Map 1: 4(+1)/5	Map 3: 10/10	Reducer 2: 0/1	
INFO  : Map 1: 4(+1)/5	Map 3: 10/10	Reducer 2: 0(+1)/1	
INFO  : Map 1: 5/5	Map 3: 10/10	Reducer 2: 0(+1)/1	
INFO  : Map 1: 5/5	Map 3: 10/10	Reducer 2: 0(+1,-1)/1	
INFO  : Map 1: 5/5	Map 3: 10/10	Reducer 2: 0(+1,-2)/1	
INFO  : Map 1: 5/5	Map 3: 10/10	Reducer 2: 0(+1,-3)/1	
ERROR : Status: Failed
ERROR : Vertex failed, vertexName=Reducer 2, vertexId=vertex_1485398058799_0109_3_02, diagnostics=[Task failed, taskId=task_1485398058799_0109_3_02_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":38,""bucketid"":1,""rowid"":0}},""value"":null}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:173)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:347)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:194)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:185)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:185)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:181)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":38,""bucketid"":1,""rowid"":0}},""value"":null}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:284)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:266)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:150)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":38,""bucketid"":1,""rowid"":0}},""value"":null}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:352)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:274)
	... 16 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:780)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:841)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:343)
	... 17 more
{code}
Hs2 logs:
{code:java}
2017-01-26 22:13:20,924 INFO  [HiveServer2-Background-Pool: Thread-648]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - </PERFLOG method=TezRunVertex.Map 1 start=1485468754798 end=1485468800924 duration=46126 from=org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor>
2017-01-26 22:13:20,925 INFO  [HiveServer2-Background-Pool: Thread-648]: SessionState (SessionState.java:printInfo(971)) - Map 1: 5/5   Map 3: 10/10    Reducer 2: 0(+1)/1
2017-01-26 22:13:22,744 INFO  [HiveServer2-Background-Pool: Thread-648]: SessionState (SessionState.java:printInfo(971)) - Map 1: 5/5   Map 3: 10/10    Reducer 2: 0(+1,-1)/1
2017-01-26 22:13:24,369 INFO  [HiveServer2-Background-Pool: Thread-648]: SessionState (SessionState.java:printInfo(971)) - Map 1: 5/5   Map 3: 10/10    Reducer 2: 0(+1,-2)/1
2017-01-26 22:13:26,194 INFO  [HiveServer2-Background-Pool: Thread-648]: SessionState (SessionState.java:printInfo(971)) - Map 1: 5/5   Map 3: 10/10    Reducer 2: 0(+1,-3)/1
2017-01-26 22:13:28,221 ERROR [HiveServer2-Background-Pool: Thread-648]: SessionState (SessionState.java:printError(980)) - Status: Failed
2017-01-26 22:13:28,222 ERROR [HiveServer2-Background-Pool: Thread-648]: SessionState (SessionState.java:printError(980)) - Vertex failed, vertexName=Reducer 2, vertexId=vertex_1485398058799_0108_2_02, diagnostics=[Task failed, taskId=task_1485398058799_0108_2_02_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":38,""bucketid"":1,""rowid"":0}},""value"":null}
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:173)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:347)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:194)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:185)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:185)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:181)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":38,""bucketid"":1,""rowid"":0}},""value"":null}
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:284)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:266)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:150)
        ... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":{""transactionid"":38,""bucketid"":1,""rowid"":0}},""value"":null}
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:352)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:274)
        ... 16 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:780)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:841)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:343)
        ... 17 more

{code}
;;;","21/Feb/17 18:12;ekoifman;this is caused by hive.enforce.bucketing=false, which is not supported (i.e. misconfiguration).  In fact, this property doesn't even exist in Hive 2.2;;;","18/Aug/17 11:03;ankur555varshney@gmail.com;Hi Team,

Please let us the work around for this issue.

Hive Vesion:-hive-2.1
I can see error in logs ,But in yarn it is showing Succeeded. Please let us
know why it is showing succeeded if it has error in yarn logs.

Error Details:-

2017-08-18 01:43:23,877 [ERROR] [TezChild] |tez.ReduceRecordSource|:
org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while
processing row (tag=0)
{""key"":{""reducesinkkey0"":{""transactionid"":0,""bucketid"":-1,""rowid"":0}},""value"":null}
        at
org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:357)
        at
org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:279)
        at
org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:279)
        at
org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
        at
org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
        at
org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
        at
org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
        at
org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1595)
        at
org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
        at
org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
        at
org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
        at
org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:779)
        at
org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
        at
org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
        at
org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:348)
        ... 17 more

Thanks
Ankur
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
select partitioned acid table order by throws java.io.FileNotFoundException,HIVE-12572,12917733,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,Duplicate,gates,taksaito,taksaito,02/Dec/15 21:17,10/Dec/15 20:18,28/Nov/24 15:58,10/Dec/15 20:18,2.0.0,,,,,,,,,Transactions,,,0,,,"Run the below queries:
{noformat}
create table test_acid (a int) partitioned by (b int) clustered by (a) into 2 buckets stored as orc tblproperties ('transactional'='true');
insert into table test_acid partition (b=1) values (1), (2), (3), (4);
select * from acid_partitioned order by a;
{noformat}

The above fails with the following error:
{noformat}
15/12/02 21:12:30 INFO SessionState: Map 1: 0(+0,-4)/1	Reducer 2: 0/1
Status: Failed
15/12/02 21:12:30 ERROR SessionState: Status: Failed
Vertex failed, vertexName=Map 1, vertexId=vertex_1449077191499_0023_1_00, diagnostics=[Task failed, taskId=task_1449077191499_0023_1_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task: attempt_1449077191499_0023_1_00_000000_0:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:195)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:160)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:348)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:60)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:60)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:74)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:340)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:172)
	... 14 more
Caused by: java.io.IOException: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:141)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:113)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62)
	... 16 more
Caused by: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1242)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1227)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1215)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:303)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:269)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:261)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1540)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:462)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:338)
	at org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.<init>(ReaderImpl.java:33)
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedOrcFile.createReader(EncodedOrcFile.java:28)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.ensureOrcReader(OrcEncodedDataReader.java:580)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.getOrReadFileMetadata(OrcEncodedDataReader.java:594)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:217)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)
	... 5 more
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1358)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy35.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy36.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1240)
	... 30 more
{noformat}",,,,,,,,,,,,,,HIVE-12632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 10 20:18:13 UTC 2015,,,,,,,,,,"0|i2p767:",9223372036854775807,,,,,,,,,,,,,,2.0.0,,,,,,,,,"10/Dec/15 20:18;sershe;Sorry, didn't realize there was already a JIRA. The other one has a patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calcite planner might have a thread-safety issue compiling in parallel,HIVE-11165,12842183,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Cannot Reproduce,jcamacho,gopalv,gopalv,02/Jul/15 04:12,27/Feb/24 22:24,28/Nov/24 15:58,03/May/16 22:35,2.0.0,,,,,,,,,CBO,,,0,,,"After about 6 minutes trying to plan a query, the HiveServer2 was killed to restore functionality to a test run.

The HEP planner is stuck on a TopologicalOrder traversal and there were no queries being fed into the HiveServer2 after it got stuck.

TPC-DS query13 was the query in question, at 4 way parallel, which triggered the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-4239,"02/Jul/15 04:13;gopalv;RunJar-2015-06-30.snapshot;https://issues.apache.org/jira/secure/attachment/12743223/RunJar-2015-06-30.snapshot",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 03 22:35:39 UTC 2016,,,,,,,,,,"0|i2grq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/15 04:13;gopalv;Yourkit snapshot as record of the planner state;;;","09/Jul/15 21:06;sershe;I have seen the following callstack that may be related, after ctrl-c-ing HiveServer2 that was stuck forever
{noformat}
Exception in thread ""HiveServer2-Handler-Pool: Thread-81"" java.lang.OutOfMemoryError: GC overhead limit exceeded
   at java.util.HashMap.resize(HashMap.java:703)
   at java.util.HashMap.putVal(HashMap.java:662)
   at java.util.HashMap.put(HashMap.java:611)
   at java.util.HashSet.add(HashSet.java:219)
   at org.apache.calcite.util.graph.BreadthFirstIterator.reachable(BreadthFirstIterator.java:61)
   at org.apache.calcite.plan.hep.HepPlanner.collectGarbage(HepPlanner.java:900)
   at org.apache.calcite.plan.hep.HepPlanner.getGraphIterator(HepPlanner.java:427)
   at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:400)
   at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:285)
   at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:72)
   at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:207)
   at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:194)
   at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.hepPlan(CalcitePlanner.java:1035)
   at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyPreJoinOrderingTransforms(CalcitePlanner.java:964)
...
{noformat};;;","09/Jul/15 21:06;sershe;[~jpullokkaran] [~pxiong] can you guys comment?
;;;","09/Jul/15 21:21;pxiong;I attached query 13 here. I did not know the root cause yet but I saw lots of predicates. I suspect that this is related to the recent optimization on PPD? [~jpullokkaran]?
{code}
select avg(ss_quantity)
       ,avg(ss_ext_sales_price)
       ,avg(ss_ext_wholesale_cost)
       ,sum(ss_ext_wholesale_cost)
 from store_sales
     ,store
     ,customer_demographics
     ,household_demographics
     ,customer_address
     ,date_dim
 where store.s_store_sk = store_sales.ss_store_sk
 and  store_sales.ss_sold_date_sk = date_dim.d_date_sk and date_dim.d_year = 2001
 and ss_sold_date between '2001-01-01' and '2001-12-31'
 and((store_sales.ss_hdemo_sk=household_demographics.hd_demo_sk
  and customer_demographics.cd_demo_sk = store_sales.ss_cdemo_sk
  and customer_demographics.cd_marital_status = 'M'
  and customer_demographics.cd_education_status = '4 yr Degree'
  and store_sales.ss_sales_price between 100.00 and 150.00
  and household_demographics.hd_dep_count = 3   
     )or
     (store_sales.ss_hdemo_sk=household_demographics.hd_demo_sk
  and customer_demographics.cd_demo_sk = store_sales.ss_cdemo_sk
  and customer_demographics.cd_marital_status = 'D'
  and customer_demographics.cd_education_status = 'Primary'
  and store_sales.ss_sales_price between 50.00 and 100.00   
  and household_demographics.hd_dep_count = 1
     ) or 
     (store_sales.ss_hdemo_sk=household_demographics.hd_demo_sk
  and customer_demographics.cd_demo_sk = ss_cdemo_sk
  and customer_demographics.cd_marital_status = 'U'
  and customer_demographics.cd_education_status = 'Advanced Degree'
  and store_sales.ss_sales_price between 150.00 and 200.00 
  and household_demographics.hd_dep_count = 1  
     ))
 and((store_sales.ss_addr_sk = customer_address.ca_address_sk
  and customer_address.ca_country = 'United States'
  and customer_address.ca_state in ('KY', 'GA', 'NM')
  and store_sales.ss_net_profit between 100 and 200  
     ) or
     (store_sales.ss_addr_sk = customer_address.ca_address_sk
  and customer_address.ca_country = 'United States'
  and customer_address.ca_state in ('MT', 'OR', 'IN')
  and store_sales.ss_net_profit between 150 and 300  
     ) or
     (store_sales.ss_addr_sk = customer_address.ca_address_sk
  and customer_address.ca_country = 'United States'
  and customer_address.ca_state in ('WI', 'MO', 'WV')
  and store_sales.ss_net_profit between 50 and 250  
     ))
;
{code};;;","09/Jul/15 22:44;gopalv;[~pxiong]: this doesn't always happen - I have to increase concurrency to trigger this.

The current test workaround is that there's a RandomOrderController in my tests so that it doesn't plan the same query (with the same vertex names, predicates etc) at the same time.;;;","09/Jul/15 23:15;jpullokkaran;[~gopalv] How did we get to the conclusion that Calcite is not thread safe?;;;","09/Jul/15 23:30;gopalv;[~jpullokkaran]: Take a look at the timeline between the threads in the attached snapshot (ThreadID 83, 84, 85) from the 12th min to the 15th minute.

They're always switching into topological order and BFS garbage collection, until I killed it.;;;","14/Jul/15 20:54;jpullokkaran;I am going on vacation, [~jcamachorodriguez] Could you take a look?;;;","03/May/16 22:26;sershe;Any update here? [~jcamachorodriguez] [~julianhyde];;;","03/May/16 22:33;julianhyde;I don't have an update. It's not obviously a thread-safety issue; the graph which is blowing up in that call stack is not shared between threads. More likely, the planner is firing rules over and over again until the graph of RelNodes gets really large. Thread-safety is one of several possible causes of that.;;;","03/May/16 22:35;jcamacho;I explored this one, and we could not reproduce it the problem. Agreed with [~gopalv] that if it showed up again, we would re-open the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify ColumnPruner when CBO optimizes the query,HIVE-12393,12912522,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,jcamacho,jcamacho,,12/Nov/15 16:43,27/Feb/24 22:24,28/Nov/24 15:58,08/Jul/16 20:21,2.0.0,,,,,,,,,Logical Optimizer,,,0,,,"The plan for any given query optimized by CBO will always contain a Project operator on top of the TS that prunes that columns that are not needed.

Thus, there is no need for Hive optimizer to traverse the whole plan to check which columns can be pruned. In fact, Hive ColumnPruner optimizer only needs to match TS operators when CBO optimized the plan.",,,,,,,,,,,,,,HIVE-12900,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 20 09:10:43 UTC 2015,,,,,,,,,,"0|i2ob3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/15 13:22;jcamacho;[~jpullokkaran], this does not seem as simple as I thought...

First problem is that in Calcite the only operator that can prune columns is Project. In contrast, in Hive there are other operators that are capable of pruning columns e.g. Join operator. Thus, we need to cover those operators in the simplified ColumnPruner or we will end up with operators producing more columns than they need.

Another problem is that ColumnPruner is the responsible of removing some Select operators if their columns are not read by follow-up operators. We could not remove those Select operators before, as they are introduced by the Hive plan generation or other optimizations.

Thus, not sure if we will be able to include this in 2.0.0.;;;","19/Nov/15 23:13;sershe;Is it a big perf gain?;;;","20/Nov/15 09:10;jcamacho;It does not need to be included in 2.0.0. More work needs to be done...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when more than 1 mapper for HBase storage handler,HIVE-13280,12949946,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Invalid,damien.carol,damien.carol,damien.carol,14/Mar/16 16:52,02/Oct/17 16:26,28/Nov/24 15:58,12/Oct/16 11:44,2.0.0,,,,,,,,,HBase Handler,,,2,,,"With a simple query (select from orc table and insert into HBase external table):
{code:sql}
insert into table register.register  select * from aa_temp
{code}
The aa_temp table have 45 orc files. It generate 45 mappers.
Some mappers fail with this error:
{noformat}
Caused by: java.lang.IllegalArgumentException: Must specify table name
        at org.apache.hadoop.hbase.mapreduce.TableOutputFormat.setConf(TableOutputFormat.java:188)
        at org.apache.hive.common.util.ReflectionUtil.setConf(ReflectionUtil.java:101)
        at org.apache.hive.common.util.ReflectionUtil.newInstance(ReflectionUtil.java:87)
        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveOutputFormat(HiveFileFormatUtils.java:300)
        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveOutputFormat(HiveFileFormatUtils.java:290)
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createHiveOutputFormat(FileSinkOperator.java:1126)
        ... 25 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:35, Vertex vertex_1457964631631_0015_3_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0 (state=08S01,code=2)
{noformat}

If I do an ALTER CONCATENATE for aa_temp. And redo the query. Everything is fine because there are only one mapper.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 02 16:26:30 UTC 2017,,,,,,,,,,"0|i2um27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/May/16 08:41;turbopape;I have the same problem,
I am on HDP 2.4
I created a big external hive table with a location
I've set some hive views on that table
And I launch a hive to insert the contents of these views inside a hbase storage backed table.
I get the exact same error you're getting.
;;;","02/Jun/16 18:49;seva_ostapenko;We are observing a similar behavior in HDP 2.4.0 when using TEZ execution engine.
Inserts into HBased-backed tables that do not require multiple mappers succeed.
Ones that require multiple mappers fail with exactly the same errors.

If hive execution engine is set to MR, statements succeed and data are inserted as expected.
;;;","11/Oct/16 20:42;vgumashta;[~damien.carol] Per the docs here: https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration, the {{hbase.mapred.output.outputtable}} table property will need to be set for an insert query. Can you check the output of show create table and describe formatted to see if that property is set?;;;","12/Oct/16 11:44;damien.carol;Yes this fix the pb.;;;","12/Oct/16 11:44;damien.carol;Yes this fix the pb.;;;","12/Oct/16 14:24;seva_ostapenko;Hive-Hbase integration documentation (https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration) states that hbase.mapred.output.outputtable property is optional, and needed only when one wants to insert into a table. The latter statement is obviously incorrect, as prior to Feb 26, 2016, this property wasn't even documented and inserts into HBase-backed tables were working just fine with MR engine.

If TEZ does require hbase.mapred.output.outputtable property to be explicitly set, documentation needs to be updated to indicate that fact.

One more thing, all the existing samples have hbase.mapred.output.outputtable and hbase.table.name set to the same value. If there is no use case when they are different, why the former even needed?;;;","02/Oct/17 16:26;smehtaji;I have tried by setting up {code} hbase.mapred.output.outputtable {code} but it's throwing the same error but when I only try to insert one record into the table then it works properly and no error. 
Even after the load job fails some of the data gets to the table anyways.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DBNotificationlistener leaks JDOPersistenceManager,HIVE-15766,13039163,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,vgumashta,vgumashta,vgumashta,31/Jan/17 11:56,19/Sep/17 06:19,28/Nov/24 15:58,20/Mar/17 23:27,2.0.0,2.1.1,,,,,2.3.0,,,Metastore,,,0,,,,,,,,,,,,,,,,HIVE-16016,,,HIVE-7353,HIVE-15373,,,,,,,,,,"31/Jan/17 16:47;vgumashta;HIVE-15766.1.patch;https://issues.apache.org/jira/secure/attachment/12850260/HIVE-15766.1.patch","08/Feb/17 23:02;vgumashta;HIVE-15766.2.patch;https://issues.apache.org/jira/secure/attachment/12851745/HIVE-15766.2.patch","01/Mar/17 10:33;vgumashta;HIVE-15766.3.patch;https://issues.apache.org/jira/secure/attachment/12855342/HIVE-15766.3.patch","02/Mar/17 18:58;vgumashta;HIVE-15766.4.patch;https://issues.apache.org/jira/secure/attachment/12855668/HIVE-15766.4.patch","17/Mar/17 07:33;vgumashta;HIVE-15766.5.patch;https://issues.apache.org/jira/secure/attachment/12859240/HIVE-15766.5.patch",,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue Mar 21 17:04:18 UTC 2017,,,,,,,,,,"0|i39exj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/17 18:14;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12850260/HIVE-15766.1.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 11015 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
org.apache.hadoop.hive.ql.TestReplicationScenarios.testAlters (batchId=208)
org.apache.hadoop.hive.ql.TestReplicationScenarios.testDrops (batchId=208)
org.apache.hadoop.hive.ql.TestReplicationScenarios.testDropsWithCM (batchId=208)
org.apache.hadoop.hive.ql.TestReplicationScenarios.testIncrementalAdds (batchId=208)
org.apache.hadoop.hive.ql.TestReplicationScenarios.testStatus (batchId=208)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3286/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3286/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3286/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12850260 - PreCommit-HIVE-Build;;;","01/Feb/17 16:51;sushanth;+1, LGTM.;;;","01/Feb/17 19:43;thejas;+1
Thanks for adding the test case as well.
;;;","01/Feb/17 19:45;thejas;Moved this to a top level issue as this is seen independent of the replication v2 work.;;;","06/Feb/17 19:06;thejas;Are the TestReplicationScenarios failures related ?
;;;","09/Feb/17 02:08;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12851745/HIVE-15766.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10242 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_count_distinct] (batchId=106)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3450/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3450/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3450/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12851745 - PreCommit-HIVE-Build;;;","23/Feb/17 02:31;vgumashta;Test failures unrelated. [~thejas] your +1 still holds?

cc [~mohitsabharwal];;;","25/Feb/17 22:44;mohitsabharwal;LGTM, +1

A small nit that process() assumes that HMSHandler.getRawStore() threadlocal is initialized. That threadlocal gets 
initialized inside HMSHandler.getMS() which, in practice always gets called before process() ( 
because we persist metadata before notification) -- so it will work in practice. Maybe good to put a null check
to be safe.;;;","28/Feb/17 02:27;akolb;Can someone post a description - what was causing the leak in the first place?;;;","28/Feb/17 17:30;thejas;[~vgumashta]
It would be safer to call HMSHandler.getMS() instead of HMSHandler.getRawStore(). That will ensure that it gets a non null value. That would also address what [~mohitsabharwal] raised.
HMSHandler.getRawStore() is currently used only in places where you don't want to initialize a new MS if one doesn't exist (ie, when you are trying to cleanup).
;;;","28/Feb/17 17:58;thejas;[~akolb]
This is related to leak described  in HIVE-7353 . ThreadWithGarbageCleanup was added in that to explicitly call ObjectStore.shutdown to cleanup the leak. However, the cleanup added in ThreadWithGarbageCleanup doesn't work if you create a non-thread local RawStore/ObjecStore. This change to use the thread local one address the leak.;;;","01/Mar/17 10:33;vgumashta;[~thejas] Can't use HMSHandler#getMS as it needs the HMSHandler object. HMSHandler#getRawStore is just a way to access the threadlocal. I've added code to set the threadlocal to a new RawStore object if it returns null (should ideally never happen since the threadlocal MS is always initialized early in call chain. Let me know if this looks good.;;;","01/Mar/17 12:58;mohitsabharwal;Wondering if it's better to create a new static version of getMS that takes hiveConf as argument. That way, both existing getMS() and DbNotificationHandler can call this method. It will allow us to use the same code path as current getMS() (including ms.verifySchema() call etc.)  and newRawStore() can be static as well. 

Also wondering if there is any point storing rawStoreClassName separately in HMSHandler since it can be looked up via hiveConf and it's only used once when RawStore gets created. ;;;","01/Mar/17 13:59;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12855342/HIVE-15766.3.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10323 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_table] (batchId=147)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=224)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=224)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3868/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3868/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3868/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12855342 - PreCommit-HIVE-Build;;;","01/Mar/17 18:44;thejas;I agree with [~mohitsabharwal], getMS() is something that looks like should be a static method, it is operating only on the thread local.

;;;","02/Mar/17 04:34;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12855541/HIVE-15766.4.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3886/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3886/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3886/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-03-02 04:31:23.165
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-3886/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-03-02 04:31:23.167
+ cd apache-github-source-source
+ git fetch origin
From https://github.com/apache/hive
   f883d67..e76e8d0  hive-14535 -> origin/hive-14535
+ git reset --hard HEAD
HEAD is now at e759bba HIVE-16045 : Print progress bar along with operation log (Anishek Agarwal via Thejas Nair)
+ git clean -f -d
Removing ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/OctetLength.java
Removing ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCharacterLength.java
Removing ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLength.java
Removing ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOctetLength.java
Removing ql/src/test/queries/clientpositive/udf_character_length.q
Removing ql/src/test/queries/clientpositive/udf_octet_length.q
Removing ql/src/test/queries/clientpositive/vector_udf_character_length.q
Removing ql/src/test/queries/clientpositive/vector_udf_octet_length.q
Removing ql/src/test/results/clientpositive/udf_character_length.q.out
Removing ql/src/test/results/clientpositive/udf_octet_length.q.out
Removing ql/src/test/results/clientpositive/vector_udf_character_length.q.out
Removing ql/src/test/results/clientpositive/vector_udf_octet_length.q.out
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at e759bba HIVE-16045 : Print progress bar along with operation log (Anishek Agarwal via Thejas Nair)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-03-02 04:31:25.531
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
Going to apply patch with: patch -p1
patching file hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java
patching file metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
+ [[ maven == \m\a\v\e\n ]]
+ rm -rf /data/hiveptest/working/maven/org/apache/hive
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
ANTLR Parser Generator  Version 3.5.2
Output file /data/hiveptest/working/apache-github-source-source/metastore/target/generated-sources/antlr3/org/apache/hadoop/hive/metastore/parser/FilterParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/parser/Filter.g
org/apache/hadoop/hive/metastore/parser/Filter.g
DataNucleus Enhancer (version 4.1.6) for API ""JDO""
DataNucleus Enhancer : Classpath
>>  /usr/share/maven/boot/plexus-classworlds-2.x.jar
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDatabase
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFieldSchema
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MType
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MConstraint
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MOrder
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStringList
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartition
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MIndex
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRole
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRoleMap
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMasterKey
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDelegationToken
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MVersionTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MResourceUri
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFunction
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationLog
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationNextId
DataNucleus Enhancer completed with success for 30 classes. Timings : input=149 ms, enhance=240 ms, total=389 ms. Consult the log for full details
ANTLR Parser Generator  Version 3.5.2
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveLexer.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
org/apache/hadoop/hive/ql/parse/HiveLexer.g
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
org/apache/hadoop/hive/ql/parse/HiveParser.g
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HintParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HintParser.g
org/apache/hadoop/hive/ql/parse/HintParser.g
Generating vector expression code
Generating vector expression test code
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-exec: Error resolving project artifact: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to datanucleus (http://www.datanucleus.org/downloads/maven2): Connect to www.datanucleus.org:80 [www.datanucleus.org/80.86.85.8] failed: Connection timed out (Connection timed out) for project org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12855541 - PreCommit-HIVE-Build;;;","03/Mar/17 02:32;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12855668/HIVE-15766.4.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 503 failed/errored test(s), 10020 tests executed
*Failed tests:*
{noformat}
TestCommandProcessorFactory - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestDbTxnManager - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestDummyTxnManager - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestHiveInputSplitComparator - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestIndexType - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestSplitFilter - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[escape_comments] (batchId=229)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[insert_into_dynamic_partitions] (batchId=232)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[insert_overwrite_dynamic_partitions] (batchId=232)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_subquery] (batchId=36)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_table_stats] (batchId=49)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[alter_partition_change_col] (batchId=24)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[alter_partition_coltype] (batchId=24)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[alter_table_cascade] (batchId=81)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[alter_table_partition_drop] (batchId=14)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[analyze_table_null_partition] (batchId=75)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[annotate_stats_part] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[autoColumnStats_3] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[autoColumnStats_6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[autoColumnStats_8] (batchId=13)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[avro_partitioned] (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[avro_partitioned_native] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[avro_schema_evolution_native] (batchId=52)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[combine2] (batchId=6)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[constprog_dp] (batchId=18)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cp_sel] (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dbtxnmgr_query4] (batchId=18)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[delete_all_partitioned] (batchId=26)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[delete_where_partitioned] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[delete_whole_partition] (batchId=9)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[drop_partitions_filter4] (batchId=42)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynamic_partition_insert] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynamic_partition_skip_default] (batchId=74)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynpart_merge] (batchId=34)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynpart_sort_opt_bucketing] (batchId=79)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[extrapolate_part_stats_full] (batchId=32)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[extrapolate_part_stats_partial] (batchId=44)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[filter_numeric] (batchId=73)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[implicit_cast_during_insert] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert0] (batchId=4)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert_acid_dynamic_partition] (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert_into6] (batchId=66)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert_into_with_schema2] (batchId=36)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert_values_dynamic_partitioned] (batchId=69)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_1] (batchId=17)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_5] (batchId=35)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_6] (batchId=30)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_7] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[list_bucket_dml_8] (batchId=67)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[llap_acid] (batchId=74)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part10] (batchId=20)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part11] (batchId=66)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part12] (batchId=75)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part13] (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part14] (batchId=82)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part15] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part1] (batchId=77)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part2] (batchId=53)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part3] (batchId=11)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part4] (batchId=58)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part6] (batchId=33)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part8] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[load_dyn_part9] (batchId=36)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[lock3] (batchId=49)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[lock4] (batchId=48)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[merge3] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[merge4] (batchId=11)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[merge_dynamic_partition2] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[merge_dynamic_partition3] (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[merge_dynamic_partition4] (batchId=31)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[merge_dynamic_partition5] (batchId=31)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[merge_dynamic_partition] (batchId=38)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mi] (batchId=9)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[nested_column_pruning] (batchId=31)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_int_type_promotion] (batchId=38)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_merge10] (batchId=59)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_merge1] (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_merge2] (batchId=82)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_merge_diff_fs] (batchId=1)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_merge_incompat2] (batchId=77)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_partitioned] (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_decode_name] (batchId=74)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_special_char] (batchId=41)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[quotedid_partition] (batchId=18)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[smb_join_partition_key] (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats12] (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats13] (batchId=74)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats2] (batchId=56)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats4] (batchId=73)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats6] (batchId=25)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats7] (batchId=48)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats8] (batchId=66)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats_noscan_1] (batchId=53)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats_only_null] (batchId=26)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats_partscan_1_23] (batchId=82)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[statsfs] (batchId=43)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_remove_15] (batchId=79)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_remove_16] (batchId=68)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_remove_17] (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_remove_18] (batchId=6)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_remove_25] (batchId=81)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[updateAccessTime] (batchId=35)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[update_all_partitioned] (batchId=48)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[update_where_partitioned] (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_non_string_partition] (batchId=31)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_drop_partition] (batchId=157)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_insert_partition_dynamic] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_stats] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_merge10] (batchId=136)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_merge1] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_merge2] (batchId=138)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_merge_diff_fs] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[rcfile_merge2] (batchId=137)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[tez_union_dynamic_partition] (batchId=136)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[autoColumnStats_1] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[autoColumnStats_2] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_sortmerge_join_16] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[delete_all_partitioned] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[delete_where_partitioned] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[delete_whole_partition] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[dynamic_partition_pruning] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[dynpart_sort_opt_vectorization] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[dynpart_sort_optimization2] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[dynpart_sort_optimization] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[dynpart_sort_optimization_acid] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[escape1] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[escape2] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[extrapolate_part_stats_partial_ndv] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_into_with_schema] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_dynamic_partitioned] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_partitioned] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[load_dyn_part1] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[load_dyn_part2] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[load_dyn_part3] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[load_dyn_part5] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[orc_analyze] (batchId=141)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[orc_create] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[orc_merge7] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[orc_merge_incompat2] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sample10] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_orc_acid_part_update] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_orc_acidvec_part_update] (batchId=141)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_table] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_noscan_1] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_only_null] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_dml] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[update_all_partitioned] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[update_where_partitioned] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_count_distinct] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_partitioned_date_time] (batchId=153)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorized_dynamic_partition_pruning] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[auto_sortmerge_join_16] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[infer_bucket_sort_num_buckets] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[orc_merge1] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[orc_merge2] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[orc_merge7] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[orc_merge_diff_fs] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[orc_merge_incompat2] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[vector_non_string_partition] (batchId=93)
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver[infer_bucket_sort_dyn_part] (batchId=83)
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver[infer_bucket_sort_num_buckets] (batchId=83)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_partialscan_autogether] (batchId=85)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_partscan_norcfile] (batchId=86)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=224)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_sortmerge_join_16] (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part10] (batchId=104)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part11] (batchId=125)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part12] (batchId=130)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part13] (batchId=124)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part14] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part15] (batchId=112)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part1] (batchId=131)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part2] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part3] (batchId=100)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part4] (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part5] (batchId=114)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part6] (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part8] (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[load_dyn_part9] (batchId=111)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[sample10] (batchId=115)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[stats12] (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[stats13] (batchId=130)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[stats2] (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[stats6] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[stats7] (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[stats8] (batchId=125)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[stats_noscan_1] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[stats_only_null] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[stats_partscan_1_23] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[statsfs] (batchId=114)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_remove_15] (batchId=131)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_remove_16] (batchId=126)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_remove_17] (batchId=124)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_remove_18] (batchId=98)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_remove_25] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_between_in] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_count_distinct] (batchId=106)
org.apache.hadoop.hive.hooks.TestHs2Hooks.testHookContexts (batchId=209)
org.apache.hadoop.hive.metastore.TestAdminUser.testCreateAdminNAddUser (batchId=200)
org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInRemote.org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInRemote (batchId=204)
org.apache.hadoop.hive.metastore.TestFilterHooks.org.apache.hadoop.hive.metastore.TestFilterHooks (batchId=193)
org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf (batchId=189)
org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs (batchId=188)
org.apache.hadoop.hive.metastore.TestHiveMetaStoreStatsMerge.testStatsMerge (batchId=196)
org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.testEnvironmentContext (batchId=199)
org.apache.hadoop.hive.metastore.TestHiveMetaTool.testExecuteJDOQL (batchId=204)
org.apache.hadoop.hive.metastore.TestHiveMetaTool.testListFSRoot (batchId=204)
org.apache.hadoop.hive.metastore.TestHiveMetaTool.testUpdateFSRootLocation (batchId=204)
org.apache.hadoop.hive.metastore.TestMarkPartition.testMarkingPartitionSet (batchId=204)
org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.testMarkingPartitionSet (batchId=204)
org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.testMetaStoreAuthorization (batchId=197)
org.apache.hadoop.hive.metastore.TestMetaStoreConnectionUrlHook.testUrlHook (batchId=196)
org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.testEndFunctionListener (batchId=196)
org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.testListener (batchId=201)
org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.testEventStatus (batchId=204)
org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.testMetaStoreInitListener (batchId=204)
org.apache.hadoop.hive.metastore.TestMetaStoreListenersError.testEventListenerException (batchId=204)
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics (batchId=193)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testAlterPartition (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testAlterTable (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testAlterViewParititon (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testColumnStatistics (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testComplexTable (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testComplexTypeApi (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testConcurrentMetastores (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testDBOwner (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testDBOwnerChange (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testDatabase (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testDatabaseLocation (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testDatabaseLocationWithPermissionProblems (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testDropTable (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testFilterLastPartition (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testFilterSinglePartition (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testFunctionWithResources (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testGetConfigValue (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testGetTableObjects (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testJDOPersistanceManagerCleanup (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testListPartitionNames (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testListPartitions (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testListPartitionsWihtLimitEnabled (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testNameMethods (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testPartition (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testPartitionFilter (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testRenamePartition (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testRetriableClientWithConnLifetime (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testSimpleFunction (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testSimpleTable (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testSimpleTypeApi (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testStatsFastTrivial (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testSynchronized (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testTableDatabase (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testTableFilter (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testTransactionalValidation (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testValidateTableCols (batchId=198)
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.testIpAddress (batchId=191)
org.apache.hadoop.hive.metastore.TestRemoteUGIHiveMetaStoreIpAddress.testIpAddress (batchId=202)
org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler (batchId=200)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testAlterPartition (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testAlterTable (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testAlterViewParititon (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testColumnStatistics (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testComplexTable (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testComplexTypeApi (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testConcurrentMetastores (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testDBOwner (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testDBOwnerChange (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testDatabase (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testDatabaseLocation (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testDatabaseLocationWithPermissionProblems (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testDropTable (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testFilterLastPartition (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testFilterSinglePartition (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testFunctionWithResources (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testGetConfigValue (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testGetTableObjects (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testJDOPersistanceManagerCleanup (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testListPartitionNames (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testListPartitions (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testListPartitionsWihtLimitEnabled (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testNameMethods (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testPartition (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testPartitionFilter (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testRenamePartition (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testRetriableClientWithConnLifetime (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testSimpleFunction (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testSimpleTable (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testSimpleTypeApi (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testStatsFastTrivial (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testSynchronized (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testTableDatabase (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testTableFilter (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testTransactionalValidation (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.testValidateTableCols (batchId=194)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testAlterPartition (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testAlterTable (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testAlterViewParititon (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testColumnStatistics (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testComplexTable (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testComplexTypeApi (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testConcurrentMetastores (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testDBOwner (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testDBOwnerChange (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testDatabase (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testDatabaseLocation (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testDatabaseLocationWithPermissionProblems (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testDropTable (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testFilterLastPartition (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testFilterSinglePartition (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testFunctionWithResources (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testGetConfigValue (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testGetTableObjects (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testJDOPersistanceManagerCleanup (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testListPartitionNames (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testListPartitions (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testListPartitionsWihtLimitEnabled (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testNameMethods (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testPartition (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testPartitionFilter (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testRenamePartition (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testRetriableClientWithConnLifetime (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testSimpleFunction (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testSimpleTable (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testSimpleTypeApi (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testStatsFastTrivial (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testSynchronized (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testTableDatabase (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testTableFilter (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testTransactionalValidation (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.testValidateTableCols (batchId=192)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testAlterPartition (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testAlterTable (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testAlterViewParititon (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testColumnStatistics (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testComplexTable (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testComplexTypeApi (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testConcurrentMetastores (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testDBOwner (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testDBOwnerChange (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testDatabase (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testDatabaseLocation (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testDatabaseLocationWithPermissionProblems (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testDropTable (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testFilterLastPartition (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testFilterSinglePartition (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testFunctionWithResources (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testGetConfigValue (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testGetTableObjects (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testJDOPersistanceManagerCleanup (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testListPartitionNames (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testListPartitions (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testListPartitionsWihtLimitEnabled (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testNameMethods (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testPartition (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testPartitionFilter (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testRenamePartition (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testRetriableClientWithConnLifetime (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testSimpleFunction (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testSimpleTable (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testSimpleTypeApi (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testStatsFastTrivial (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testSynchronized (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testTableDatabase (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testTableFilter (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testTransactionalValidation (batchId=203)
org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.testValidateTableCols (batchId=203)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreMetrics.testMetaDataCounts (batchId=200)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.alterRename (batchId=196)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.alterRenamePartitioned (batchId=196)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.database (batchId=196)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.describeNonpartitionedTable (batchId=196)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.grant (batchId=196)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.insertIntoPartitionTable (batchId=196)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.insertIntoTable (batchId=196)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.partitionedTable (batchId=196)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.role (batchId=196)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table (batchId=196)
org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.testCreateDatabaseWithTableNonDefaultNameNode (batchId=209)
org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.testCreateTableWithIndexAndPartitionsNonDefaultNameNode (batchId=209)
org.apache.hadoop.hive.ql.TestReplicationScenarios.org.apache.hadoop.hive.ql.TestReplicationScenarios (batchId=209)
org.apache.hadoop.hive.ql.TestTxnCommands.testMergeCardinalityViolation (batchId=272)
org.apache.hadoop.hive.ql.TestTxnCommands2.testDynamicPartitionsMerge (batchId=258)
org.apache.hadoop.hive.ql.TestTxnCommands2.testDynamicPartitionsMerge2 (batchId=258)
org.apache.hadoop.hive.ql.TestTxnCommands2.updateDeletePartitioned (batchId=258)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.testDynamicPartitionsMerge (batchId=270)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.testDynamicPartitionsMerge2 (batchId=270)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.updateDeletePartitioned (batchId=270)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testDynamicPartitionsMerge (batchId=267)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testDynamicPartitionsMerge2 (batchId=267)
org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.updateDeletePartitioned (batchId=267)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.checkExpectedLocks2 (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testDynamicPartitionInsert (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testMerge3Way01 (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testMerge3Way02 (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testMergePartitioned01 (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testMergePartitioned02 (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testMultiInsert (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testWriteSetTracking10 (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testWriteSetTracking11 (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testWriteSetTracking7 (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testWriteSetTracking8 (batchId=271)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testWriteSetTracking9 (batchId=271)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testAutoPurgeTablesAndPartitions (batchId=257)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testDropPartitionsWithPurge (batchId=257)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testGetAndDropTables (batchId=257)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testHiveRefreshOnConfChange (batchId=257)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testIndex (batchId=257)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testMetaStoreApiTiming (batchId=257)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testPartition (batchId=257)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testTable (batchId=257)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testThriftTable (batchId=257)
org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.testListener (batchId=210)
org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.testSimplePrivileges (batchId=211)
org.apache.hadoop.hive.ql.security.TestExtendedAcls.org.apache.hadoop.hive.ql.security.TestExtendedAcls (batchId=221)
org.apache.hadoop.hive.ql.security.TestFolderPermissions.org.apache.hadoop.hive.ql.security.TestFolderPermissions (batchId=209)
org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.testSimplePrivileges (batchId=211)
org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.testSimplePrivileges (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropDatabase (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropPartition (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropTable (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropView (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.testSimplePrivileges (batchId=209)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.testSimplePrivileges (batchId=221)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadDbFailure (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadDbSuccess (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadTableFailure (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadTableSuccess (batchId=211)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadTableSuccessWithReadOnly (batchId=211)
org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.dynamicPartitioningDelete (batchId=206)
org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.dynamicPartitioningInsert (batchId=206)
org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.dynamicPartitioningUpdate (batchId=206)
org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.schemaEvolutionAddColDynamicPartitioningInsert (batchId=206)
org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.schemaEvolutionAddColDynamicPartitioningUpdate (batchId=206)
org.apache.hadoop.hive.thrift.TestDBTokenStore.testDBTokenStore (batchId=193)
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testDelegationTokenSharedStore (batchId=221)
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testMetastoreProxyUser (batchId=221)
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testSaslWithHiveMetaStore (batchId=221)
org.apache.hive.beeline.TestBeeLineWithArgs.org.apache.hive.beeline.TestBeeLineWithArgs (batchId=212)
org.apache.hive.beeline.TestBeelinePasswordOption.org.apache.hive.beeline.TestBeelinePasswordOption (batchId=212)
org.apache.hive.beeline.cli.TestHiveCli.testCmd (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testDatabaseOptions (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testErrOutput (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testHelp (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testInValidCmd (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testInvalidDatabaseOptions (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testInvalidOptions (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testInvalidOptions2 (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testNoErrorDB (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testSetHeaderValue (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testSetPromptValue (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testSourceCmd (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testSourceCmd2 (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testSourceCmd3 (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testSqlFromCmd (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testSqlFromCmdWithDBName (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testUseCurrentDB1 (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testUseCurrentDB2 (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testUseCurrentDB3 (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testUseInvalidDB (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testVariables (batchId=168)
org.apache.hive.beeline.cli.TestHiveCli.testVariablesForSource (batchId=168)
org.apache.hive.beeline.hs2connection.TestBeelineConnectionUsingHiveSite.testBeelineConnectionHttp (batchId=212)
org.apache.hive.beeline.hs2connection.TestBeelineConnectionUsingHiveSite.testBeelineConnectionNoAuth (batchId=212)
org.apache.hive.beeline.hs2connection.TestBeelineConnectionUsingHiveSite.testBeelineConnectionSSL (batchId=212)
org.apache.hive.beeline.hs2connection.TestBeelineConnectionUsingHiveSite.testBeelineUsingArgs (batchId=212)
org.apache.hive.beeline.hs2connection.TestBeelineWithUserHs2ConnectionFile.testBeelineConnectionHttp (batchId=212)
org.apache.hive.beeline.hs2connection.TestBeelineWithUserHs2ConnectionFile.testBeelineConnectionNoAuth (batchId=212)
org.apache.hive.beeline.hs2connection.TestBeelineWithUserHs2ConnectionFile.testBeelineConnectionSSL (batchId=212)
org.apache.hive.hcatalog.api.TestHCatClient.org.apache.hive.hcatalog.api.TestHCatClient (batchId=170)
org.apache.hive.hcatalog.api.repl.commands.TestCommands.org.apache.hive.hcatalog.api.repl.commands.TestCommands (batchId=170)
org.apache.hive.hcatalog.cli.TestPermsGrp.testCustomPerms (batchId=175)
org.apache.hive.hcatalog.listener.TestDbNotificationListener.sqlInsertPartition (batchId=222)
org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat (batchId=180)
org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish (batchId=175)
org.apache.hive.hcatalog.streaming.TestStreaming.testConcurrentTransactionBatchCommits (batchId=182)
org.apache.hive.jdbc.TestJdbcDriver2.org.apache.hive.jdbc.TestJdbcDriver2 (batchId=216)
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark (batchId=218)
org.apache.hive.jdbc.TestJdbcWithMiniHA.org.apache.hive.jdbc.TestJdbcWithMiniHA (batchId=217)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.org.apache.hive.jdbc.TestJdbcWithMiniHS2 (batchId=218)
org.apache.hive.jdbc.TestJdbcWithMiniLlap.testEscapedStrings (batchId=218)
org.apache.hive.jdbc.TestJdbcWithMiniLlap.testLlapInputFormatEndToEnd (batchId=218)
org.apache.hive.jdbc.TestJdbcWithMiniLlap.testNonAsciiStrings (batchId=218)
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark (batchId=218)
org.apache.hive.jdbc.TestSSL.testMetastoreWithSSL (batchId=215)
org.apache.hive.jdbc.TestSSL.testSSLFetch (batchId=215)
org.apache.hive.jdbc.TestSSL.testSSLFetchHttp (batchId=215)
org.apache.hive.jdbc.TestServiceDiscoveryWithMiniHS2.testConnectionWithConfigsPublished (batchId=219)
org.apache.hive.jdbc.TestServiceDiscoveryWithMiniHS2.testConnectionWithoutConfigsPublished (batchId=219)
org.apache.hive.jdbc.TestXSRFFilter.testFilterDisabledNoInjection (batchId=218)
org.apache.hive.jdbc.TestXSRFFilter.testFilterDisabledWithInjection (batchId=218)
org.apache.hive.jdbc.TestXSRFFilter.testFilterEnabledWithInjection (batchId=218)
org.apache.hive.jdbc.authorization.TestHS2AuthzContext.testAuthzContextContentsDriverCmd (batchId=219)
org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth (batchId=219)
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthUDFBlacklist.testBlackListedUdfUsage (batchId=218)
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testAuthorization1 (batchId=219)
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testBlackListedUdfUsage (batchId=219)
org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.testHookContexts (batchId=230)
org.apache.hive.minikdc.TestJdbcNonKrbSASLWithMiniKdc.org.apache.hive.minikdc.TestJdbcNonKrbSASLWithMiniKdc (batchId=230)
org.apache.hive.minikdc.TestJdbcWithDBTokenStore.org.apache.hive.minikdc.TestJdbcWithDBTokenStore (batchId=230)
org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.testCookie (batchId=230)
org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthBinary.testAuthorization1 (batchId=230)
org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthHttp.testAuthorization1 (batchId=230)
org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.org.apache.hive.service.TestHS2ImpersonationWithRemoteMS (batchId=214)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3900/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3900/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3900/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 503 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12855668 - PreCommit-HIVE-Build;;;","03/Mar/17 07:13;thejas; * Looks like getMS can use getMSForConf as well
 * The test failures seems to have happened only with this patch, it could be related. Can you please check ?
 * I will do some additional reading to see if there is no reason to pass a thread local and non-thread local config to  RawStoreProxy.getProxy method
 * Can you please create a github pull request or review-board link . It makes it easier to review changes in a new patch;;;","17/Mar/17 17:45;vgumashta;Addressed all comments from [~thejas] and [~mohitsabharwal]. 
Test results here:  https://builds.apache.org/job/PreCommit-HIVE-Build/4207/testReport/. Failures not related.;;;","17/Mar/17 17:51;vgumashta;bq. I will do some additional reading to see if there is no reason to pass a thread local and non-thread local config to RawStoreProxy.getProxy method

I verified and this and looks fine to me. In the original code, if threadlocal conf is null, it is created from the supplied hiveconf.;;;","17/Mar/17 18:37;daijy;Patch LGTM now. [~mohitsabharwal], do you have any additional comments?;;;","20/Mar/17 23:27;vgumashta;Committed. Thanks [~mohitsabharwal] [~thejas] [~daijy] for the review.;;;","21/Mar/17 17:04;mohitsabharwal;Thanks, [~vgumashta], latest patch LGTM. Sorry about the late response.;;;",,,,,,,,,,,,,,,,,,,,,,
NullPointerException might occur when create table,HIVE-15329,13024796,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,winningalong,winningalong,winningalong,01/Dec/16 14:52,21/Jul/17 18:26,28/Nov/24 15:58,09/Dec/16 22:58,2.0.0,2.1.0,,,,,2.3.0,,,Metastore,,,0,metastore,,"NullPointerException might occur if table.getParameters() returns null when method isNonNativeTable is invoked in class MetaStoreUtils.
{code}
public static boolean isNonNativeTable(Table table) {
    if (table == null) {
      return false;
    }
    return (table.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE) != null);
  }
{code}
This will cause a stack trace without any suggestive information at client:
{code}
org.apache.hadoop.hive.metastore.api.MetaException: 
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/16 08:25;winningalong;HIVE-15329.1.patch;https://issues.apache.org/jira/secure/attachment/12841433/HIVE-15329.1.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 09 22:58:15 UTC 2016,,,,,,,,,,"0|i370yn:",9223372036854775807,,,,,,,,,,,,,,2.2.0,,,,,,,,,"02/Dec/16 08:40;winningalong;patch uploaded.;;;","05/Dec/16 12:23;winningalong;Anyone can help review? [~prasanth_j]
;;;","05/Dec/16 22:36;prasanth_j;+1, pending tests;;;","08/Dec/16 12:08;winningalong;Why hasn't the Hive QA come yet?
;;;","09/Dec/16 08:07;prasanth_j;Manually triggered Hive QA. Look for #2515 in https://builds.apache.org/view/H-L/view/Hive/job/PreCommit-HIVE-Build/ ;;;","09/Dec/16 09:47;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12841433/HIVE-15329.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 10792 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=44)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[columnstats_part_coltype] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=92)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2515/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2515/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2515/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12841433 - PreCommit-HIVE-Build;;;","09/Dec/16 22:54;prasanth_j;The test failures are not related to this patch and are already failing in master. Failures are tracked in HIVE-15058;;;","09/Dec/16 22:58;prasanth_j;Committed to master! Thanks [~winningalong] for the contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hplsql registerUDF conflicts with pom.xml,HIVE-15096,13016317,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,ferhui,ferhui,ferhui,29/Oct/16 03:00,21/Jul/17 18:26,28/Nov/24 15:58,13/Nov/16 03:17,2.0.0,2.0.1,2.1.0,,,,2.3.0,,,hpl/sql,,,0,,,"in hplsql code, registerUDF code is

    sql.add(""ADD JAR "" + dir + ""hplsql.jar"");
    sql.add(""ADD JAR "" + dir + ""antlr-runtime-4.5.jar"");
    sql.add(""ADD FILE "" + dir + Conf.SITE_XML);

but pom configufation is

  <parent>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive</artifactId>
    <version>2.2.0-SNAPSHOT</version>
    <relativePath>../pom.xml</relativePath>
  </parent>

  <artifactId>hive-hplsql</artifactId>
  <packaging>jar</packaging>
  <name>Hive HPL/SQL</name>

    <dependency>
       <groupId>org.antlr</groupId>
       <artifactId>antlr4-runtime</artifactId>
       <version>4.5</version>
    </dependency>

when run hplsql , errors occur as below

 Error while processing statement: /opt/apps/apache-hive-2.0.0-bin/lib/hplsql.jar does not exist",,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/16 05:32;ferhui;HIVE-15096.1.patch;https://issues.apache.org/jira/secure/attachment/12836285/HIVE-15096.1.patch","31/Oct/16 04:02;ferhui;HIVE-15096.patch;https://issues.apache.org/jira/secure/attachment/12836110/HIVE-15096.patch",,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 13 03:17:54 UTC 2016,,,,,,,,,,"0|i35knb:",9223372036854775807,,,,,,,,,,,,,,2.2.0,,,,,,,,,"29/Oct/16 03:02;ferhui;diff --git a/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java b/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java
index 6da4f5b..1e14361 100644
--- a/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java
+++ b/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java
@@ -615,9 +615,13 @@ public void registerUdf() {
     }
     ArrayList<String> sql = new ArrayList<String>();
     String dir = Utils.getExecDir();
-    sql.add(""ADD JAR "" + dir + ""hplsql.jar"");
-    sql.add(""ADD JAR "" + dir + ""antlr-runtime-4.5.jar"");
-    sql.add(""ADD FILE "" + dir + Conf.SITE_XML);
+    sql.add(""ADD JAR "" + dir + ""hive-hplsql-2.2.0-SNAPSHOT.jar"");
+    sql.add(""ADD JAR "" + dir + ""antlr4-runtime-4.5.jar"");
+    if(!conf.getLocation().equals("""")) {
+      sql.add(""ADD FILE "" + conf.getLocation());
+    } else {
+      sql.add(""ADD FILE "" + dir + Conf.SITE_XML);
+    }
     if (dotHplsqlrcExists) {
       sql.add(""ADD FILE "" + dir + Conf.DOT_HPLSQLRC);
     }
;;;","29/Oct/16 03:05;ferhui;anyone review it？;;;","31/Oct/16 04:02;ferhui;fix hpl/sql registerUDF,consistent with pom.xml;;;","31/Oct/16 06:21;Ferd;LGTM +1 pending to the test.;;;","31/Oct/16 08:39;dmtolpeko;-1 why to hardcode the version???? hive-hplsql-2.2.0-SNAPSHOT.jar 
you will need to change this file with every Hive realease.;;;","01/Nov/16 02:29;ferhui;there is no hplsql.jar with hive release. users don't know how to  change this file if they have no source code;;;","01/Nov/16 05:41;ferhui;update the patch, remove the hardcode version 2.2.0-SNAPSHOT.
can you review it again? give suggestions;;;","01/Nov/16 08:43;dmtolpeko;+1 looks good, thanks.;;;","12/Nov/16 04:07;ferhui;hi, [~dmtolpeko] . could you please commit this patch to master branch?;;;","13/Nov/16 03:17;Ferd;Committed to the master. Thanks [~ferhui] for the contribution and [~dmtolpeko] for the review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive insertion query execution fails on Hive on Spark,HIVE-15054,13015122,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,aihuaxu,aihuaxu,aihuaxu,25/Oct/16 15:53,21/Jul/17 18:26,28/Nov/24 15:58,07/Nov/16 03:27,2.0.0,,,,,,2.3.0,,,Spark,,,0,,,"The query of {{insert overwrite table tbl1}} sometimes will fail with the following errors. Seems we are constructing taskAttemptId with partitionId which is not unique if there are multiple attempts.

{noformat}
ava.lang.IllegalStateException: Hit error while closing operators - failing tree: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://table1/.hive-staging_hive_2016-06-14_01-53-17_386_3231646810118049146-9/_task_tmp.-ext-10002/_tmp.002148_0 to: hdfs://table1/.hive-staging_hive_2016-06-14_01-53-17_386_3231646810118049146-9/_tmp.-ext-10002/002148_0
at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.close(SparkMapRecordHandler.java:202)
at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.closeRecordProcessor(HiveMapFunctionResultList.java:58)
at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:106)
at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$15.apply(AsyncRDDActions.scala:120)
{noformat}

",,,,,,,,,,,,,HIVE-13066,,,,,,,,,,,,,,"26/Oct/16 13:00;aihuaxu;HIVE-15054.1.patch;https://issues.apache.org/jira/secure/attachment/12835320/HIVE-15054.1.patch","26/Oct/16 19:13;aihuaxu;HIVE-15054.2.patch;https://issues.apache.org/jira/secure/attachment/12835401/HIVE-15054.2.patch","28/Oct/16 20:00;aihuaxu;HIVE-15054.3.patch;https://issues.apache.org/jira/secure/attachment/12835872/HIVE-15054.3.patch","04/Nov/16 15:50;aihuaxu;HIVE-15054.4.patch;https://issues.apache.org/jira/secure/attachment/12837165/HIVE-15054.4.patch",,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 14:15:48 UTC 2016,,,,,,,,,,"0|i35d9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/16 16:04;aihuaxu;patch-1: initial patch. Change the id from the partitionId to taskAttemptId. It would cause some test failures per the comment. Will see the test result and I will fix it in the following patch.

So the problem is: if we use partitionId as part of {{mapred.task.id}} and the taskId is used as the filename in FileSinkOp, then there will be a conflict if there is a retry on the same task. Switch to taskAttemptId which should be unique.;;;","25/Oct/16 16:05;aihuaxu;+ [~xuefuz] [~csun] [~jxiang] and [~szehon] ;;;","25/Oct/16 16:06;xuefuz;[~aihuaxu] thanks for looking at this.

[~lirui]/[~chengxiang li], I think we attempted to fix this before. Can you review the patch?;;;","26/Oct/16 01:49;lirui;Yeah I tried something similar in HIVE-13066. I'll mark it as dup and let's fix it here.
[~aihuaxu], what I don't understand is why the error doesn't always happen. I wasn't able to reproduce HIVE-13066, and I guess not all tasks with multiple attempts will trigger the issue here. Could you do some more investigation to find out? Thanks.;;;","26/Oct/16 12:49;aihuaxu;[~lirui] Thanks for taking a look. It would be hard to repro. It depends on which state the first executor is when it's aborted or dies. You will see such issue when the task is done with the writing the data to a tmp file and renaming to the file tmp file while at that time spark kills such task in your case or the executor loses the connection. The case I have seen is,  the connection to the executor times out but the executor is almost done with its work (the result is finished writing and renamed to the final tmp file and only thing left is to report to the driver that the task is done).  

 If the rename doesn't happen, then you won't see such issue. ;;;","26/Oct/16 13:51;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12835320/HIVE-15054.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 10621 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[auto_sortmerge_join_16] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[root_dir_external_table] (batchId=157)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_sortmerge_join_16] (batchId=114)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[parquet_join] (batchId=100)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[sample10] (batchId=112)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[temp_table_gb1] (batchId=106)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_between_in] (batchId=116)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0] (batchId=164)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0] (batchId=164)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1] (batchId=164)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1820/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1820/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1820/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12835320 - PreCommit-HIVE-Build;;;","26/Oct/16 14:19;lirui;Thanks for the explanations Aihua.
I'm not sure which format is better: {{partitionId_attemptNumber}}, or just {{taskAttemptId}}. Does hive rely on the attemptNumber to identify the multiple outputs for the same task/partition?;;;","26/Oct/16 15:49;aihuaxu;Hive doesn't use attemptNumber. I feel taskAttemptId is better since it matches with what is expected and other engines like tez and MR (expecting taskId and attemptId here). That info helps in the diagnostics. 

How do you think?;;;","26/Oct/16 18:35;aihuaxu;From the test failures, actually hive in some cases are expecting taskId_attemptNumber format although attemptNumber is not used. I will change to {{taskAttemptId_0}}.;;;","26/Oct/16 22:34;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12835401/HIVE-15054.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10621 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[auto_sortmerge_join_16] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[root_dir_external_table] (batchId=157)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_sortmerge_join_16] (batchId=114)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[sample10] (batchId=112)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[temp_table_gb1] (batchId=106)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_between_in] (batchId=116)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0] (batchId=164)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0] (batchId=164)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1] (batchId=164)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1829/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1829/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1829/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12835401 - PreCommit-HIVE-Build;;;","27/Oct/16 01:24;lirui;I could be wrong, but from what I remember, partitionId_attemptNumber is closer to MR's taskId_attemptNumber. If task 0 has two attempts, we'll have 0_0 and 0_1. If we use taskAttemptId, that'll be two unique numbers. Not sure if hive can understand the two attempts are for the same task/partition.;;;","27/Oct/16 01:54;aihuaxu;I'm still trying understand the difference between them. Seems both should work, but I'm seeing some test problems.



;;;","27/Oct/16 20:18;aihuaxu;[~lirui] It's strange that with taskAttemptId_0, it will cause sample10.q e.g. to produce different result while partitionId_attemptNumber is fine. Can't figure out why. Do you have any insights?

I can see that some of our tests could fail if somehow the first attempt fails and generates the file like 000000_1 since some test output list the file names, but I guess MR would have the same problem as well.;;;","28/Oct/16 06:12;lirui;Hi [~aihuaxu], I don't know for sure either.
My hunch is that since taskAttemptId are all unique numbers, how can hive know whether two attempts are for the same task or not?;;;","28/Oct/16 06:36;lirui;This is how MR generates the attempt Id: https://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapreduce/TaskAttemptID.html

We'll be very close to the format (e.g. {{attempt_200707121733_0003_m_000005_0}}) if we use partitionId_attemptNumber. Except we're using stage Id instead of job Id.;;;","28/Oct/16 12:57;aihuaxu;Yeah. Seems hive is using 000005 part to track if they are the same task or not somewhere.  Originally I thought that info was only used for the final filename. I will take another look how that was used in hive.;;;","28/Oct/16 20:04;aihuaxu;patch-3: switch to partitionId_attemptNumber. Spark will have a different taskId for the different attempts of the same task while hive needs to use the same id to figure out if the data are duplicate. So seems we have to use partitionId_attemptNumber here. 
;;;","28/Oct/16 20:10;aihuaxu;[~lirui] You are right that hive needs to use the same id to figure out it's the same task. Spark has different taskId for different task attempt. Seems partitionId is the closest choice. ;;;","29/Oct/16 02:16;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12835872/HIVE-15054.3.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 10626 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=91)
org.apache.hive.spark.client.TestSparkClient.testJobSubmission (batchId=272)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1873/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1873/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1873/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12835872 - PreCommit-HIVE-Build;;;","31/Oct/16 07:14;lirui;Thanks [~aihuaxu] for the investigation and update!
The patch looks good. But I find the comments a little bit confusing. How about something like this
{code}
// Hive requires this TaskAttemptId to be unique. MR's TaskAttemptId is composed of ""attempt_timestamp_jobNum_m/r_taskNum_attemptNum"". The counterpart for Spark should be ""attempt_timestamp_stageNum_m/r_partitionId_attemptNum"". When there're multiple attempts for a task, Hive will rely on the partitionId to figure out if the data are duplicate or not (see org.apache.hadoop.hive.ql.exec.Utils.removeTempOrDuplicateFiles)  when collecting the final outputs
{code};;;","31/Oct/16 15:46;aihuaxu;patch-4: update the comment.;;;","31/Oct/16 15:47;aihuaxu;[~lirui] I think what you provided is better. Just updated the comments in the patch.;;;","02/Nov/16 03:39;lirui;Thanks for updating [~aihuaxu]. +1;;;","03/Nov/16 15:24;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12836843/HIVE-15054.4.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1945/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1945/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1945/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2016-11-03 15:23:59.713
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ MAVEN_OPTS='-Xmx1g -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-1945/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2016-11-03 15:23:59.716
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 345353c HIVE-15039: A better job monitor console output for HoS (Rui reviewed by Xuefu and Ferdinand)
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 345353c HIVE-15039: A better job monitor console output for HoS (Rui reviewed by Xuefu and Ferdinand)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2016-11-03 15:24:00.603
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
Going to apply patch with: patch -p1
patching file ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HivePairFlatMapFunction.java
+ [[ maven == \m\a\v\e\n ]]
+ rm -rf /data/hiveptest/working/maven/org/apache/hive
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
ANTLR Parser Generator  Version 3.4
org/apache/hadoop/hive/metastore/parser/Filter.g
DataNucleus Enhancer (version 4.1.6) for API ""JDO""
DataNucleus Enhancer : Classpath
>>  /usr/share/maven/boot/plexus-classworlds-2.x.jar
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDatabase
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFieldSchema
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MType
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MConstraint
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MOrder
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStringList
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartition
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MIndex
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRole
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRoleMap
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMasterKey
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDelegationToken
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MVersionTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MResourceUri
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFunction
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationLog
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationNextId
DataNucleus Enhancer completed with success for 30 classes. Timings : input=183 ms, enhance=242 ms, total=425 ms. Consult the log for full details
Generating vector expression code
Generating vector expression test code
ANTLR Parser Generator  Version 3.4
org/apache/hadoop/hive/ql/parse/HiveLexer.g
org/apache/hadoop/hive/ql/parse/HiveParser.g
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-exec: Error resolving project artifact: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to datanucleus (http://www.datanucleus.org/downloads/maven2): Connect to localhost:3128 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused for project org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12836843 - PreCommit-HIVE-Build;;;","04/Nov/16 18:50;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12837165/HIVE-15054.4.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10628 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_4] (batchId=91)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1968/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1968/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1968/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12837165 - PreCommit-HIVE-Build;;;","07/Nov/16 03:27;lirui;Committed to master. Thanks [~aihuaxu] for the fix.;;;","07/Nov/16 14:15;aihuaxu;Thanks Rui.;;;",,,,,,,,,,,,,,,,,,
Add config to block queries that scan > N number of partitions ,HIVE-12603,12919477,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,,lskuff,lskuff,06/Dec/15 00:07,28/Nov/16 17:20,28/Nov/24 15:58,08/Dec/15 08:20,2.0.0,,,,,,,,,Metastore,Query Planning,,0,,,"Strict mode is useful for blocking queries that load all partitions, but it's still possible to put significant load on the HMS for queries that scan a large number of partitions. It would be useful to add a config provide a hard limit to the number of partitions scanned by a query.",,,,,,,,,,,,,,HIVE-6492,,HIVE-9499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 11 16:32:59 UTC 2015,,,,,,,,,,"0|i2phxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/15 19:06;sershe;hive.limit.query.max.table.partition?;;;","08/Dec/15 08:19;lskuff;Thanks [~sershe], missed that config. That's exactly what we want. ;;;","11/Dec/15 16:32;thejas;Note that HIVE-9499 prevents this from being used as a general config.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC standalone jar is missing classes from hadoop-commons jar.,HIVE-15110,13017092,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,,ggregory,ggregory,02/Nov/16 04:50,02/Nov/16 05:58,28/Nov/24 15:58,02/Nov/16 05:58,2.0.0,2.1.0,,,,,,,,JDBC,,,0,,,"- Create a Generic JDBC Driver in Eclipse DTP or JBoss Dev Studio.
- Connect to a Hive server.
- You get the error: 
{noformat}
java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration
	at org.apache.hive.jdbc.HiveConnection.createUnderlyingTransport(HiveConnection.java:432)
	at org.apache.hive.jdbc.HiveConnection.createBinaryTransport(HiveConnection.java:452)
	at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:193)
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:157)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107)
	at org.eclipse.datatools.connectivity.drivers.jdbc.JDBCConnection.createConnection(JDBCConnection.java:328)
	at org.eclipse.datatools.connectivity.DriverConnectionBase.internalCreateConnection(DriverConnectionBase.java:105)
	at org.eclipse.datatools.connectivity.DriverConnectionBase.open(DriverConnectionBase.java:54)
	at org.eclipse.datatools.connectivity.drivers.jdbc.JDBCConnection.open(JDBCConnection.java:96)
	at org.eclipse.datatools.connectivity.drivers.jdbc.JDBCConnectionFactory.createConnection(JDBCConnectionFactory.java:53)
	at org.eclipse.datatools.connectivity.internal.ConnectionFactoryProvider.createConnection(ConnectionFactoryProvider.java:83)
	at org.eclipse.datatools.connectivity.internal.ConnectionProfile.createConnection(ConnectionProfile.java:359)
	at org.eclipse.datatools.connectivity.internal.ManagedConnection.createConnection(ManagedConnection.java:166)
	at org.eclipse.datatools.connectivity.internal.CreateConnectionJob.run(CreateConnectionJob.java:56)
	at org.eclipse.core.internal.jobs.Worker.run(Worker.java:55)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:814)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 15 more
{noformat}

For 2.0.0, I exploded the standalone jar, added the contents of hadoop-common-2.7.2.jar, and repacked the jar. That works.

Please fix this as I do not want to do this for all new versions. I have to do this nonesense because one of the runtime containers I deal with only deals with JDBC drivers that are all in one jar.","JBoss Developer Studio

Version: 9.1.0.GA
Build id: GA-v20160414-0124-B497
Build date: 20160414-0124

Oracle jdk1.8.0_91",,,,,,,,,,,,HIVE-14837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2016-11-02 04:50:35.0,,,,,,,,,,"0|i35pev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive on spark throws NPE exception for union all query ,HIVE-14742,13004595,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,,aihuaxu,aihuaxu,13/Sep/16 13:52,13/Sep/16 15:09,28/Nov/24 15:58,13/Sep/16 15:09,2.0.0,,,,,,,,,Spark,,,0,,,"{noformat}
create table foo (fooId string, fooData string) partitioned by (fooPartition string) stored as parquet;
insert into foo partition (fooPartition = '1') values ('1', '1'), ('2', '2');
set hive.execution.engine=spark;
select * from ( 
select 
fooId as myId, 
fooData as myData 
from foo where fooPartition = '1' 
union all 
select 
fooId as myId, 
fooData as myData 
from foo where fooPartition = '3' 
) allData;
{noformat}

Error while compiling statement: FAILED: NullPointerException null",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 15:09:50 UTC 2016,,,,,,,,,,"0|i33kin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/16 15:09;aihuaxu;Actually it has been fixed by HIVE-9570.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
metrics errors because of conflicts with hadoops older metrics jar,HIVE-14620,12999755,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Invalid,thejas,thejas,thejas,24/Aug/16 19:05,24/Aug/16 19:43,28/Nov/24 15:58,24/Aug/16 19:11,2.0.0,2.1.0,,,,,,,,HiveServer2,Metastore,,0,,,"Hadoop has older 3.0.1 jar while hive uses newer 3.1.0 jar.
This causes metrics to throw the following error -

{noformat}
016-08-24 13:08:24,427 ERROR [HiveServer2-Handler-Pool: Thread-55]: metastore.HiveMetaStore (HiveMetaStore.java:init(516)) - error in Metrics init: java.lang.reflect.InvocationTargetException null
java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedConstructorAccessor92.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at org.apache.hadoop.hive.common.metrics.common.MetricsFactory.init(MetricsFactory.java:42)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:513)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:77)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:83)
        at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5982)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:203)
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
        at sun.reflect.GeneratedConstructorAccessor95.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1549)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:89)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:135)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:107)
        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3227)
        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3246)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:504)
        at org.apache.hive.service.cli.session.HiveSessionImpl.open(HiveSessionImpl.java:144)
        at sun.reflect.GeneratedMethodAccessor122.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
        at com.sun.proxy.$Proxy22.open(Unknown Source)
        at org.apache.hive.service.cli.session.SessionManager.openSession(SessionManager.java:281)
        at org.apache.hive.service.cli.CLIService.openSessionWithImpersonation(CLIService.java:204)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.getSessionHandle(ThriftCLIService.java:421)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:316)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1257)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1242)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:562)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoSuchMethodError: com.codahale.metrics.JmxAttributeGauge.<init>(Ljavax/management/MBeanServerConnection;Ljavax/management/ObjectName;Ljava/lang/String;)V
        at com.codahale.metrics.jvm.BufferPoolMetricSet.getMetrics(BufferPoolMetricSet.java:45)
        at org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.registerAll(CodahaleMetrics.java:324)
        at org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.<init>(CodahaleMetrics.java:179)
        ... 45 more

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 24 19:11:04 UTC 2016,,,,,,,,,,"0|i32qof:",9223372036854775807,,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,"24/Aug/16 19:11;thejas;This is not valid issue. Hive 2.0.0 already has the change to put hive jars first via HIVE-12739 .
This was seen only in an non-apache build based on hive 1.x where metrics changes were backported.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't fail config validation for removed configs,HIVE-14133,12985259,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,ashutoshc,ashutoshc,ashutoshc,29/Jun/16 22:50,07/Jul/16 22:36,28/Nov/24 15:58,07/Jul/16 22:36,2.0.0,2.1.0,,,,,,,,Configuration,,,0,,,Users may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.,,,,,,,,,,,,,,HIVE-14132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2016-06-29 22:50:00.0,,,,,,,,,,"0|i30c8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Property ""hive.mapjoin.optimized.keys"" does not exist",HIVE-13829,12972303,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Not A Problem,sladymon,BigDataOrange,BigDataOrange,24/May/16 07:22,02/Jun/16 17:02,28/Nov/24 15:58,02/Jun/16 17:02,2.0.0,,,,,,,,,Configuration,,,0,,,"Refering to the documentation (https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties), it is possible to set the following property ""hive.mapjoin.optimized.keys"". Unfortunately, this property seems to be unknown to Hive.
Here is an extract of the hive-site.xml which includes the property:
{noformat}
  <property>
      <name>hive.mapjoin.optimized.hashtable</name>
      <value>true</value>
      <description>Whether Hive should use a memory-optimized hash table for MapJoin. Only works on Tez, because memory-optimized hash table cannot be serialized.</description>
  </property>
{noformat}
In the logs I have:
{noformat}
May 24 09:09:02 hiveserver2.bigdata.fr HiveConf of name hive.mapjoin.optimized.keys does not exist
{noformat}","Hadoop 2.7.2, Hive 2.0.0, Spark 1.6.1, Kerberos",,,,,,,,,,,,,,,,,HIVE-9331,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 31 23:07:26 UTC 2016,,,,,,,,,,"0|i2yerr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/16 03:09;leftyl;HIVE-9331 removed *hive.mapjoin.optimized.keys* and *hive.mapjoin.lazy.hashtable* in release 1.1.0 but the documentation hasn't been updated yet.;;;","27/May/16 08:57;BigDataOrange;Hi [~leftylev], thanks for the feedback. In that case, I think this JIRA can be closed. ;;;","31/May/16 23:07;sladymon;HIVE-9331 has now been documented, so this JIRA will be closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle exceptions during SARG creation,HIVE-12639,12920561,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,prasanth_j,prasanth_j,prasanth_j,09/Dec/15 21:48,24/May/16 15:55,28/Nov/24 15:58,09/Dec/15 21:49,2.0.0,2.1.0,,,,,,,,,,,0,,,"Bad predicates can cause SearchArgument creation to throw exception.  For example, filters like where ts = '2014-15-16 17:18:19.20' can throw IllegalArgumentException during SARG creation as timestamp is of wrong format (month is invalid). If SARG creation fails, it should return YES_NO_NULL TruthValue instead of throwing exception. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 09 21:49:11 UTC 2015,,,,,,,,,,"0|i2polr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/15 21:49;prasanth_j;Duplicate of HIVE-12596;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distinct functions don't work properly after hive.map.groupby.sorted is default to true,HIVE-13768,12970094,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Not A Problem,,aihuaxu,aihuaxu,16/May/16 18:13,17/May/16 21:19,28/Nov/24 15:58,17/May/16 21:19,2.0.0,,,,,,,,,Query Planning,,,0,,,"HIVE-12325 changes hive.map.groupby.sorted default value to true. The following {{select count(distinct) from t1;}} will return incorrect result now. In the older version, if you set hive.map.groupby.sorted to true, seems it will also return incorrect result.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 17 21:19:45 UTC 2016,,,,,,,,,,"0|i2y15j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/May/16 21:19;aihuaxu;Seems I misunderstood the problem. It assumes that table is sorted by group by key. So it works as expected.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexArrayOutOfBoundsException during vectorized map join,HIVE-12896,12932895,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,gopalv,jdere,jdere,20/Jan/16 22:30,12/May/16 10:58,28/Nov/24 15:58,12/May/16 10:58,2.0.0,,,,,,,,,Vectorization,,,0,,,"Trying a simple join on a couple of the TPCDS tables. Query works with vectorization disabled.
{noformat}
 select c_customer_sk, c_customer_id from tpcds_bin_partitioned_orc_10.customer, tpcds_bin_partitioned_orc_10.customer_demographics where c_current_cdemo_sk = cd_demo_sk limit 20
{noformat}

{noformat}
], TaskAttempt 3 failed, info=[Error: Failure while running task: attempt_1448429572030_8225_4_01_000003_3:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:195)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:160)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:351)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:59)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:59)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:36)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:354)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:172)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:852)
	at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:115)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:852)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:114)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:168)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:152)
	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow$StringReaderByValue.apply(VectorDeserializeRow.java:345)
	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserializeByValue(VectorDeserializeRow.java:684)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultSingleValue(VectorMapJoinGenerateResultOperator.java:183)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.finishInner(VectorMapJoinInnerGenerateResultOperator.java:180)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:373)
	... 24 more
{noformat}",,,,,,,,,,,,,,,,HIVE-13245,,,,,,,,,,,"27/Jan/16 12:16;gopalv;HIVE-12896.tar.gz;https://issues.apache.org/jira/secure/attachment/12784625/HIVE-12896.tar.gz","20/Jan/16 22:30;jdere;query.explain.txt;https://issues.apache.org/jira/secure/attachment/12783442/query.explain.txt",,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 27 12:16:54 UTC 2016,,,,,,,,,,"0|i2rrnz:",9223372036854775807,HIVE-13682,,,,,,,,,,,,,,,,,,,,,,"20/Jan/16 22:30;jdere;Attaching query explain plan;;;","21/Jan/16 03:27;gopalv;This bug looks familiar - ref: HIVE-12463
;;;","21/Jan/16 03:33;jdere;Hmm, the branch I'm on (derivative of master) has the fix for HIVE-12463;;;","21/Jan/16 03:51;gopalv;I ran this with the same logging in BytesColumnVector

{code}
+    if (sourceBuf.length < start + length || length < 0) {
+       throw new RuntimeException(String.format(""[%d] %d > %d + %d"", elementNum, sourceBuf.length, start, length));
+    }
{code}

{code}
Caused by: java.lang.RuntimeException: [205] 8388608 > 3935989 + -2
        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:153)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow$StringReaderByValue.apply(VectorDeserializeRow.java:350)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserializeByValue(VectorDeserializeRow.java:690)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultSingleValue(VectorMapJoinGenerateResultOperator.java:183)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.finishInner(VectorMapJoinInnerGenerateResultOperator.java:180)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:373)
        ... 26 more
{code};;;","22/Jan/16 23:52;gopalv;Assigned to me, while I investigate deeper.;;;","27/Jan/16 10:04;gopalv;The issue is most likely an off-by-1 error somewhere in the hashtable reader.

I confirmed that we're not getting the null byte for the data correctly & is reading part of the 1st value as the isNull byte.;;;","27/Jan/16 10:32;gopalv;Digging deeper, the bug disappears if this is turned into a primary key only join.

{code}
create temporary table cr stored as orc as select distinct * from (select *, row_number() over (partition by c_current_cdemo_sk) as r from tpcds_bin_partitioned_orc_10.customer) x where r = 1;

-- works because the hashtable has no >1 values for each key
select c_customer_sk, c_customer_id from cr, tpcds_bin_partitioned_orc_10.customer_demographics where c_current_cdemo_sk = cd_demo_sk limit 20; 

select c_customer_sk, c_customer_id from tpcds_bin_partitioned_orc_10.customer, tpcds_bin_partitioned_orc_10.customer_demographics where c_current_cdemo_sk = cd_demo_sk limit 20;
{code};;;","27/Jan/16 12:16;gopalv;Minimal test-case for the bug.

When run with the native vectorized hashtable, it hands out two rows which are incorrect.

{code}
154500  AAAAAAAAEILFCAAA
6       NULL
NULL    �AAAAAAAAMNMFAAAA�=AAAAAAAANDHAGAAA��AAAAAAAANBDMEAAA[��AAAAAAAALLPOEAAA�[�
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert from select generates incorrect result when hive.optimize.constant.propagation is on,HIVE-13235,12948164,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,aihuaxu,aihuaxu,aihuaxu,08/Mar/16 21:26,03/May/16 13:58,28/Nov/24 15:58,03/May/16 13:58,2.0.0,,,,,,,,,Query Planning,,,0,,,"The following query returns incorrect result when constant optimization is turned on. The subquery happens to have an alias p1 to be the same as the input partition name. Constant optimizer will optimize it incorrectly as the constant.

When constant optimizer is turned off, we will get the correct result.
{noformat}
set hive.cbo.enable=false;
set hive.optimize.constant.propagation = true;
create table t1(c1 string, c2 double) partitioned by (p1 string, p2 string);
create table t2(p1 double, c2 string);
insert into table t1 partition(p1='40', p2='p2') values('c1', 0.0);
INSERT OVERWRITE TABLE t2  select if((c2 = 0.0), c2, '0') as p1, 2 as p2 from t1 where c1 = 'c1' and p1 = '40';
select * from t2;

40   2
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/16 20:28;aihuaxu;HIVE-13235.1.patch;https://issues.apache.org/jira/secure/attachment/12792580/HIVE-13235.1.patch","14/Mar/16 21:47;aihuaxu;HIVE-13235.2.patch;https://issues.apache.org/jira/secure/attachment/12793406/HIVE-13235.2.patch","16/Mar/16 21:01;aihuaxu;HIVE-13235.3.patch;https://issues.apache.org/jira/secure/attachment/12793853/HIVE-13235.3.patch","02/May/16 20:10;aihuaxu;HIVE-13235.4.patch;https://issues.apache.org/jira/secure/attachment/12801810/HIVE-13235.4.patch",,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 03 13:58:52 UTC 2016,,,,,,,,,,"0|i2uczr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/16 20:32;aihuaxu;Attached the patch-1: seems we are trying to resolve the column info from the parent OP to child OP by alias. But alias should be visible to the children and the internalName is visible to the parents.;;;","14/Mar/16 21:50;aihuaxu;Attached patch-2: for the cases when the column has both name and alias, we will use NamedColumnInfo which will match against column name during comparison rather than alias since alias is not visible yet for such cases.;;;","16/Mar/16 08:02;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12793406/HIVE-13235.2.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 15 failed/errored test(s), 9829 tests executed
*Failed tests:*
{noformat}
TestSparkCliDriver-groupby3_map.q-sample2.q-auto_join14.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-groupby_map_ppr_multi_distinct.q-table_access_keys_stats.q-groupby4_noskew.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-join_rc.q-insert1.q-vectorized_rcfile_columnar.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-ppd_join4.q-join9.q-ppd_join3.q-and-12-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input25
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input26
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_part10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_into5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_25
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_constprog_semijoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_load_dyn_part14
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_25
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7280/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7280/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7280/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 15 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12793406 - PreCommit-HIVE-TRUNK-Build;;;","16/Mar/16 21:01;aihuaxu;Attached patch-3: to address test failures. Set the column name when the select column is a table column, otherwise, set it to alias. ;;;","18/Mar/16 21:15;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12793853/HIVE-13235.3.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 9836 tests executed
*Failed tests:*
{noformat}
TestSparkCliDriver-groupby3_map.q-sample2.q-auto_join14.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-groupby_map_ppr_multi_distinct.q-table_access_keys_stats.q-groupby4_noskew.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-join_rc.q-insert1.q-vectorized_rcfile_columnar.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-ppd_join4.q-join9.q-ppd_join3.q-and-12-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_constantPropagateForInsertSelect
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_tez_join_hash
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_join_hash
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7305/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7305/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7305/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12793853 - PreCommit-HIVE-TRUNK-Build;;;","07/Apr/16 01:14;ashutoshc;[~aihuaxu] Can you create a RB for this ?;;;","07/Apr/16 14:10;aihuaxu;[~ashutoshc] I haven't had a final solution yet. Seems my solutions would fix the issue but also break valid constant propagation. I think it's on the right direction: for select operators, an alias and internal name are not enough. We should have another columnName if it's mapped to table column (e.g., select col1 as alias). The parent ops would only see col1 but child ops would only see alias. Right now, we ignore col1 but use alias always.

I'm working on it but seems to need bigger changes. Will create RB when it's ready.  ;;;","02/May/16 20:17;aihuaxu;Attached patch-4: for non-cbo case, we will keep track of the select column's original expression and use that rather than using the alias to match against another column info. We will not do that for cbo case since cbo has optimized AST tree and may not have the original expression. ;;;","02/May/16 20:34;ashutoshc;[~pxiong] Is this same as HIVE-13602 ? ;;;","03/May/16 00:21;pxiong;[~ashutoshc], i just checked the problem that [~aihuaxu] mentioned in this jira. It seems that it is quite related to HIVE-13602. I also test the problem in this jira and it disappears with the patch in HIVE-13602.;;;","03/May/16 00:39;ashutoshc;Thanks [~pxiong] for testing this out. So, it seems we only need one patch to solve these 2 problems. I haven't looked at either patch yet but seems like we can commit either of these. [~aihuaxu] What do you think?;;;","03/May/16 00:50;pxiong;[~ashutoshc], thanks for your comments. I totally agree with you. I just briefly reviewed [~aihuaxu]'s patch and i think the main difference is that his patch is improving the tableAlias/colAlias matching and my patch is completely dropping the tableAlias/colAlias matching method.;;;","03/May/16 13:19;aihuaxu;That's great news. I will take a look at HIVE-13602 to see the implementation. It's possible that HIVE-13602 is a better approach since I'm not familiar with CBO and had bypassed CBO to just get noncbo to work. Let me take a look. Thanks for the info.;;;","03/May/16 13:58;aihuaxu;I checked the patch HIVE-13602 and verified multiple scenarios for cbo and non-cbo. All worked.  HIVE-13602 seems to be a better fix. I will dup this to HIVE-13602.

I didn't verify for other affected operators like union though.;;;","03/May/16 13:58;aihuaxu;HIVE-13602 seems is a better way to fix this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue appending HIVE_QUERY_ID without checking if the prefix already exists,HIVE-13408,12955447,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Invalid,vikram.dixit,vikram.dixit,vikram.dixit,01/Apr/16 23:58,26/Apr/16 22:44,28/Nov/24 15:58,26/Apr/16 22:43,2.0.0,,,,,,,,,Shims,,,0,,,"{code}
We are resetting the hadoop caller context to HIVE_QUERY_ID:HIVE_QUERY_ID:
{code}",,,,,,,,,,,,,,,,,,,,HIVE-12254,,,,,,,"02/Apr/16 00:01;vikram.dixit;HIVE-13408.1.patch;https://issues.apache.org/jira/secure/attachment/12796628/HIVE-13408.1.patch","05/Apr/16 22:28;vikram.dixit;HIVE-13408.2.patch;https://issues.apache.org/jira/secure/attachment/12797185/HIVE-13408.2.patch",,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 26 22:43:20 UTC 2016,,,,,,,,,,"0|i2vjin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/16 00:01;vikram.dixit;Depends on HIVE-12254.;;;","02/Apr/16 00:03;vikram.dixit;Ping [~jdere].;;;","05/Apr/16 22:27;vikram.dixit;Addressed [~jdere]'s review comments (given offline).;;;","05/Apr/16 22:28;vikram.dixit;The right file.;;;","05/Apr/16 22:35;jdere;+1;;;","10/Apr/16 06:48;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12797185/HIVE-13408.2.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7533/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7533/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7533/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-7533/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 0ebd4d1 HIVE-13434 : BaseSemanticAnalyzer.unescapeSQLString doesn't unescape \u0000 style character literals. (Kousuke Saruta via Ashutosh Chauhan)
+ git clean -f -d
+ git checkout master
Already on 'master'
+ git reset --hard origin/master
HEAD is now at 0ebd4d1 HIVE-13434 : BaseSemanticAnalyzer.unescapeSQLString doesn't unescape \u0000 style character literals. (Kousuke Saruta via Ashutosh Chauhan)
+ git merge --ff-only origin/master
Already up-to-date.
+ git gc
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12797185 - PreCommit-HIVE-TRUNK-Build;;;","22/Apr/16 17:47;sershe;Should this be rebased and committed?;;;","22/Apr/16 17:51;vikram.dixit;[~sershe] Unfortunately not. This one depends on updating the hadoop version used in hive and committing of other patches (HIVE-12254) before we can commit this to master.;;;","22/Apr/16 17:58;sershe;Should there be a separate fix for 2.0.1 then? If this causes the infinite log lines, we should just remove the addition for 2.0.1.;;;","26/Apr/16 22:43;sershe;This is actually broken by HIVE-12254 (that is not committed yet), according to [~vikram.dixit]; should be included there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Excessive console message from SparkClientImpl [Spark Branch],HIVE-12569,12917677,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,xuefuz,xuefuz,xuefuz,02/Dec/15 19:05,31/Mar/16 02:38,28/Nov/24 15:58,31/Mar/16 02:38,2.0.0,,,,,,,,,Spark,,,0,,,"{code}
15/12/02 11:00:46 INFO client.SparkClientImpl: 15/12/02 11:00:46 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)
15/12/02 11:00:47 INFO client.SparkClientImpl: 15/12/02 11:00:47 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)
15/12/02 11:00:48 INFO client.SparkClientImpl: 15/12/02 11:00:48 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)
15/12/02 11:00:49 INFO client.SparkClientImpl: 15/12/02 11:00:49 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)
15/12/02 11:00:50 INFO client.SparkClientImpl: 15/12/02 11:00:50 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)
{code}
I see this using Hive CLI after a spark job is launched and it goes non-stopping even if the job is finished.",,,,,,,,,,,,,HIVE-13376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 01 21:48:53 UTC 2016,,,,,,,,,,"0|i2p6tr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/15 23:52;xuefuz;Actually we do like to get this fixed in 2.0.;;;","03/Dec/15 01:10;xuefuz;[~lirui], [~chengxiang li], any idea what causes this and how to fix? We didn't seem to have the problem previously. I saw some discussion on the web, but it seems related but not completely applicable. http://stackoverflow.com/questions/30828879/application-report-for-application-state-accepted-never-ends-for-spark-submit. Also, https://github.com/bigdatagenomics/adam/issues/561;;;","03/Dec/15 01:27;nemon;I noticed this log, too.
And here is my finds:
The log messages come from a spark class org.apache.spark.deploy.yarn.Client.scala
{code}
 logInfo(s""Application report for $appId (state: $state)"")
{code}
And printed out by a Redirector in Hive's SparkClientImpl.java.
{code}
   while ((line = in.readLine()) != null) {
      LOG.info(line);
   }
{code}
The direct way to fix it is only logging message when state changing (Need to change spark code).
And another way is to add a log4j.properties file to spark-submit's class path,changing org.apache.spark.deploy.yarn.Client.scala's log level to WARN.
May be there are other solutions.
;;;","03/Dec/15 02:28;xuefuz;Thanks, [~nemon].

[~vanzin], any insight here? Thanks.;;;","03/Dec/15 03:35;chengxiang li;As [~nemon] analyzed, the message comes from Spark side, it should be spark get stuck in {{org.apache.spark.deploy.yarn.Client::monitorApplication}} as it never get end state. Looks like a spark issue.;;;","03/Dec/15 05:46;chengxiang li;Actually, it's not a exactly issue, although the spark job finished, the spark application indeed still alive, so the reported state is right, we just do not want to print it on CLI console. change the log level should be the simplest solution.;;;","03/Dec/15 07:37;lirui;I also hit the issue before. Changing log4j conf to make the logs go to RFA instead of console fixed my problem.;;;","03/Dec/15 16:51;vanzin;You can set {{spark.yarn.report.interval}} to a really large number (default is 1000).;;;","06/Jan/16 19:12;sershe;This is a blocker with no real activity for a while. Is this really a blocker?;;;","06/Jan/16 19:17;xuefuz;I have just downgraded it.;;;","01/Feb/16 21:48;sershe;Removing 2.0 from target versions as RC0 has been cut. Please -1 the RC, reinstate the version and make this a blocker if that is not acceptable :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Impossible cast from LongColumnVector to TimestampColumnVector in VectorColumnAssignFactory.buildObjectAssign method,HIVE-13071,12939775,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,mmccline,Jayer,Jayer,17/Feb/16 10:25,29/Mar/16 08:14,28/Nov/24 15:58,29/Mar/16 08:14,2.0.0,,,,,,,,,,,29/Feb/16 00:00,0,,,"In the method of VectorColumnAssignFactory.buildObjectAssign, it is impossible to cast from LongColumnVector to TimestampColumnVector. This cast will always throw a ClassCastException.",,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/16 10:27;Jayer;HIVE-13071.patch;https://issues.apache.org/jira/secure/attachment/12788221/HIVE-13071.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 29 08:14:43 UTC 2016,,,,,,,,,,"0|i2sxs7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/16 08:14;mmccline;Fixed with HIVE-13111.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LLAP: Stripe metadata cache holds unreachable keys ,HIVE-13225,12947893,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Invalid,gopalv,gopalv,gopalv,08/Mar/16 03:28,08/Mar/16 20:16,28/Nov/24 15:58,08/Mar/16 04:30,2.0.0,2.1.0,,,,,,,,llap,,,0,,,"The Hash inspection reported that there were some unreachable keys in the metadata hashmap.

{code}
for (...) {
      if (hasFileId && metadataCache != null) {
        stripeKey.stripeIx = stripeIx;
        value = metadataCache.getStripeMetadata(stripeKey);
      }
      if (value == null || !value.hasAllIndexes(globalInc)) {
          if (hasFileId && metadataCache != null) {
            value = metadataCache.putStripeMetadata(value);
...
}
{code}

Means that the hashCode of the key changes after the put if there are > 1 stripes in the file.

{code}
  public OrcStripeMetadata putStripeMetadata(OrcStripeMetadata metaData) {
....
    OrcStripeMetadata val = stripeMetadata.putIfAbsent(metaData.getKey(), metaData);
{code}

needs to make a copy of the Key, if it needs to preserve hash consistency.",,,,,,,,,,,,,,,,,,HIVE-12995,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 08 20:16:33 UTC 2016,,,,,,,,,,"0|i2ubbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/16 04:30;gopalv;Cache issue seems to be entirely related to the linked patch - back to inspecting that patch for cache hit-rate metrics.;;;","08/Mar/16 20:16;sershe;Probably some bug there, I'll take a look. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comparison bug in HiveSplitGenerator.InputSplitComparator#compare(),HIVE-12405,12912738,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Fixed,Aleksei,Aleksei,Aleksei,13/Nov/15 07:11,19/Nov/15 22:17,28/Nov/24 15:58,19/Nov/15 22:16,2.0.0,,,,,,,,,Tez,,,0,,,"""compare()"" method in HiveSplitGenerator.InputSplitComparator has the following condition on line 281 which is always false and is most likely a typo:
{code}
if (startPos1 > startPos1) {
{code}
As a result, in certain conditions splits might be sorted in incorrect order.",,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/15 07:16;Aleksei;HIVE-12405.patch;https://issues.apache.org/jira/secure/attachment/12772134/HIVE-12405.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 19 22:17:00 UTC 2015,,,,,,,,,,"0|i2ocen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/15 07:16;Aleksei;Fix the bug, add a unit test.;;;","13/Nov/15 07:25;gopalv;Good catch - the patch looks good - +1.

This might need a backport to branch-1 for consistency.;;;","13/Nov/15 17:48;Aleksei;I already checked. In branch-1 splits are not sorted, so there's no comparator and no such bug as a result.;;;","13/Nov/15 21:31;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12772134/HIVE-12405.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 9781 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_rp_annotate_stats_groupby
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_import
org.apache.hadoop.hive.hwi.TestHWISessionManager.testHiveDriver
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6030/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6030/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-6030/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12772134 - PreCommit-HIVE-TRUNK-Build;;;","16/Nov/15 02:39;Aleksei;Test failures don't seem to be related.;;;","19/Nov/15 22:17;ashutoshc;Pushed to master. Thanks, Aleksei!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
orc_merge9.q hangs when writing orc metadata section,HIVE-12452,12914010,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Implemented,prasanth_j,prasanth_j,prasanth_j,18/Nov/15 05:55,18/Nov/15 19:44,28/Nov/24 15:58,18/Nov/15 19:44,2.0.0,,,,,,,,,ORC,,,0,,,"When running tests for HIVE-12450 orc_merge9.q hung without completing.
See attached screenshot for the thread that hung.",,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/15 05:55;prasanth_j;orc-writer-hang.png;https://issues.apache.org/jira/secure/attachment/12772923/orc-writer-hang.png",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 18 19:44:40 UTC 2015,,,,,,,,,,"0|i2ok7z:",9223372036854775807,,,,,,,,,,,,,,2.0.0,,,,,,,,,"18/Nov/15 19:44;prasanth_j;Fix for this is implemented in .2 patch of HIVE-12450;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0 is added when you insert decimal value which number of fractional digits is less than defined scale,HIVE-12394,12912554,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Not A Problem,,taksaito,taksaito,12/Nov/15 18:16,12/Nov/15 20:47,28/Nov/24 15:58,12/Nov/15 18:45,2.0.0,,,,,,,,,,,,0,,,"{noformat}
hive> create table test_decimal (d decimal(3,2));
hive> insert into table test_decimal values (0.1);
hive> select * from test_decimal;
0.10
{noformat}

Extra 0 is added to the end of the result.",,,,,,,,,,,,,,,,,,HIVE-12063,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 12 20:47:38 UTC 2015,,,,,,,,,,"0|i2obav:",9223372036854775807,,,,,,,,,,,,,,2.0.0,,,,,,,,,"12/Nov/15 18:45;jdere;Padding of decimal values on display is expected now, due to HIVE-12063;;;","12/Nov/15 20:47;leftyl;I had thought HIVE-12063 didn't need documentation, but apparently some doc would be helpful.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate remote_script.q result,HIVE-12068,12903397,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Cannot Reproduce,,aihuaxu,aihuaxu,08/Oct/15 15:29,20/Oct/15 14:48,28/Nov/24 15:58,20/Oct/15 14:48,2.0.0,,,,,,,,,Test,,,0,,,"Investigate the result of remote_script.q. The result seems incorrect. We may have some issues with the code.

{noformat}
1       2       NULL
1       2       NULL
1       NULL
2       NULL
1       NULL
2       NULL
1       NULL
2       NULL
1       NULL
2       NULL
{noformat}",,,,,,,,,,,,,,,,,,HIVE-11785,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 20 14:48:29 UTC 2015,,,,,,,,,,"0|i2mrbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/15 14:46;aihuaxu;The issue seems to be fixed by HIVE-11785.
;;;","20/Oct/15 14:48;aihuaxu;The second column value 'null' actually is correct since we escape newline, carriage return and tab so the text from the script is considered to be the first column.

The output is as following without sorting.
{noformat}
1
2       NULL
2       NULL
1       2       NULL
1
2       NULL
2       NULL
1       2       NULL
{noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Select distinct"" always gives OOM error in the local run",HIVE-11446,12851209,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Cannot Reproduce,,aihuaxu,aihuaxu,03/Aug/15 21:07,04/Aug/15 14:13,28/Nov/24 15:58,04/Aug/15 14:13,2.0.0,,,,,,,,,Hive,,,0,,,"{noformat}
create table src (key string);

select distinct t1.key from src t1, src t2 where t1.key=t2.key;
{noformat}

The command fails with OOM error.

{noformat}
java.lang.Exception: java.lang.OutOfMemoryError: Java heap space
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:406)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:826)
        at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:376)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:406)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:268)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 04 14:13:17 UTC 2015,,,,,,,,,,"0|i2ia9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/15 14:13;aihuaxu;I saw this issue and then tried to debug. But it works during the debug, then after that, it also works without debug. Probably it's due to my own hadoop setup. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Unable to drop a default partition with ""int"" type",HIVE-11237,12844693,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,aihuaxu,aihuaxu,aihuaxu,13/Jul/15 16:30,13/Jul/15 17:10,28/Nov/24 15:58,13/Jul/15 17:10,2.0.0,,,,,,,,,Query Processor,,,0,,,"{noformat}
CREATE TABLE test (col1 string) PARTITIONED BY (p1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' STORED AS TEXTFILE;
INSERT OVERWRITE TABLE test PARTITION (p1) SELECT code, IF(salary > 60000, 100, null) as p1 FROM default.sample_07;
hive> SHOW PARTITIONS test;
OK
p1=100
p1=__HIVE_DEFAULT_PARTITION__
Time taken: 0.124 seconds, Fetched: 2 row(s)

hive> ALTER TABLE test DROP partition (p1 = '__HIVE_DEFAULT_PARTITION__');
FAILED: SemanticException Unexpected unknown partitions for (p1 = null)
{noformat}

The default partition name '__HIVE_DEFAULT_PARTITION__' cannot be deleted.
",,,,,,,,,,,,,HIVE-11208,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 13 17:10:16 UTC 2015,,,,,,,,,,"0|i2h6tb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/15 17:10;aihuaxu;Similar one HIVE-11208 is already created. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ObjectStore should call closeAll() on JDO query object to release the resources,HIVE-11021,12838180,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,Duplicate,aihuaxu,aihuaxu,aihuaxu,16/Jun/15 14:20,16/Jun/15 14:33,28/Nov/24 15:58,16/Jun/15 14:32,2.0.0,,,,,,,,,Metastore,,,0,,,"In ObjectStore class, in getMDatabase() and getMTable(), after retrieving the database and table info from the database, we should call closeAll() on JDO query to release the resource. It would cause the cursor leaking on the database otherwise.",,,,,,,,,,,,,HIVE-10895,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 16 14:32:55 UTC 2015,,,,,,,,,,"0|i2g3tr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/15 14:32;aihuaxu;Actually already have one for the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slf4j warning again from 2.X.X,HIVE-21157,13211425,Bug,Resolved,HIVE,Hive,software,ayushtkn,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,Won't Fix,ryu_kobayashi,ryu_kobayashi,ryu_kobayashi,24/Jan/19 01:22,25/Jan/19 02:35,28/Nov/24 15:58,25/Jan/19 02:35,2.0.0,,,,,,,,,CLI,,,0,,,"The warning of Slf4j has occurred again since 2.0.0:
{code}
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
{code}",,,,,,,,,,,,,,,,,,HIVE-9496,HIVE-6162,,,,,,,,"24/Jan/19 01:29;ryu_kobayashi;HIVE-21157.01.patch;https://issues.apache.org/jira/secure/attachment/12956069/HIVE-21157.01.patch",,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 25 02:35:37 UTC 2019,,,,,,,,,,"0|yi08uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/19 05:34;hiveqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 31s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 11s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 11s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}  8m 38s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  xml  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-15771/dev-support/hive-personality.sh |
| git revision | master / a7e704c |
| Default Java | 1.8.0_111 |
| modules | C: packaging U: packaging |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-15771/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

;;;","24/Jan/19 06:37;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12956069/HIVE-21157.01.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 15707 tests executed
*Failed tests:*
{noformat}
TestReplicationScenariosIncrementalLoadAcidTables - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/15771/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/15771/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-15771/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12956069 - PreCommit-HIVE-Build;;;","24/Jan/19 22:24;ashutoshc;This will exclude log4j-slf4j-impl from Hive's lib and instead will make hadoop's version of that jar to be picked up. I am not sure that what we wants. Infact hadoop's version is already behind, so dont think we want that. Instead, hive's version should be picked up.;;;","25/Jan/19 02:35;ryu_kobayashi;[~ashutoshc] I see. I wanted to exclude this slf4j's warning but I will invalidate this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,